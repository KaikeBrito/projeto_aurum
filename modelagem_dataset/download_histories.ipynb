{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e47310e-3b74-4d20-8806-195412f64c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "download_histories.py\n",
    "\n",
    "Uso:\n",
    "- coloque a lista de tickers (formato ex: 'ABEV3.SA') em `tickers`.\n",
    "- execute este script: python download_histories.py\n",
    "- os arquivos serão salvos em ./data/historical/<TICKER>.parquet e ./data/historical/<TICKER>.csv\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from tqdm import tqdm\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d667dd83-abb4-4d22-8956-19ac2f52935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging simples\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "# diretório onde os históricos serão salvos (compatível com seu script anterior)\n",
    "HIST_DIR = os.path.join(\"data\", \"historical\")\n",
    "os.makedirs(HIST_DIR, exist_ok=True)\n",
    "\n",
    "# parâmetros de download\n",
    "DEFAULT_START = \"1998-01-01\"\n",
    "BATCH_SIZE = 15            # número de tickers por chamada yfinance.download (ajuste se achar necessário)\n",
    "MAX_ATTEMPTS = 4           # tentativas por batch\n",
    "SLEEP_BETWEEN_BATCHES = 1  # segundos entre batches para reduzir risco de throttling\n",
    "SLEEP_BETWEEN_TICKERS = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4118df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticker_exists_local(ticker: str) -> bool:\n",
    "    \"\"\"Verifica se já existe parquet salvo para ticker (usa para pular downloads)\"\"\"\n",
    "    path = os.path.join(HIST_DIR, f\"{ticker}.parquet\")\n",
    "    return os.path.isfile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dae925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history_df(ticker: str, df: pd.DataFrame, save_csv: bool = True):\n",
    "    \"\"\"Salva DataFrame em parquet e opcionalmente em CSV. Garante coluna 'date' se índice for DatetimeIndex.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        raise ValueError(\"DataFrame nulo ou vazio\")\n",
    "    df = df.copy()\n",
    "    # garantir que a coluna de data exista como coluna\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index.name = \"date\"\n",
    "        df = df.reset_index()\n",
    "    # converter coluna date para string ISO ao salvar CSV (mantém compatibilidade)\n",
    "    out_parquet = os.path.join(HIST_DIR, f\"{ticker}.parquet\")\n",
    "    out_csv = os.path.join(HIST_DIR, f\"{ticker}.csv\")\n",
    "    try:\n",
    "        df.to_parquet(out_parquet, index=False)\n",
    "        logging.info(\"Saved %s rows for %s -> %s\", len(df), ticker, out_parquet)\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Erro salvando parquet para %s: %s\", ticker, e)\n",
    "        raise\n",
    "    if save_csv:\n",
    "        try:\n",
    "            # padronizar data para ISO antes de salvar CSV (se existir)\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')\n",
    "            df.to_csv(out_csv, index=False)\n",
    "            logging.info(\"Saved CSV for %s -> %s\", ticker, out_csv)\n",
    "        except Exception as e:\n",
    "            logging.exception(\"Erro salvando CSV para %s: %s\", ticker, e)\n",
    "            # não raise — parquet já salvo, apenas logamos o problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a892b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_batch(batch: List[str], start: str = DEFAULT_START, threads: bool = True) -> Dict[str, Optional[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Tenta baixar um batch de tickers via yfinance.download.\n",
    "    Retorna dict ticker -> DataFrame or None (se falhou).\n",
    "    \"\"\"\n",
    "    joined = \" \".join(batch)\n",
    "    attempt = 0\n",
    "    last_exc = None\n",
    "    while attempt < MAX_ATTEMPTS:\n",
    "        try:\n",
    "            logging.info(\"yfinance.download attempt %d for batch size %d\", attempt+1, len(batch))\n",
    "            data = yf.download(tickers=joined, start=start, progress=False, threads=threads, group_by='ticker', auto_adjust=False, actions=True)\n",
    "            result = {}\n",
    "            if isinstance(data, pd.DataFrame) and isinstance(data.columns, pd.MultiIndex):\n",
    "                for ticker in batch:\n",
    "                    if ticker in data.columns.get_level_values(0):\n",
    "                        df_t = data[ticker].copy()\n",
    "                        result[ticker] = df_t\n",
    "                    else:\n",
    "                        try:\n",
    "                            single = yf.download(ticker, start=start, progress=False, actions=True)\n",
    "                            result[ticker] = single if not single.empty else None\n",
    "                        except Exception:\n",
    "                            result[ticker] = None\n",
    "            else:\n",
    "                for ticker in batch:\n",
    "                    try:\n",
    "                        df_t = yf.download(ticker, start=start, progress=False, actions=True)\n",
    "                        result[ticker] = df_t if not df_t.empty else None\n",
    "                    except Exception:\n",
    "                        result[ticker] = None\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "            logging.warning(\"Erro no yfinance.download (attempt %d): %s\", attempt+1, str(e))\n",
    "            attempt += 1\n",
    "            time.sleep(2 ** attempt)  # backoff exponencial\n",
    "    logging.error(\"Todas tentativas falharam para batch (%s). Último erro: %s\", joined, last_exc)\n",
    "    # fallback: tentar baixar ticker a ticker\n",
    "    result = {}\n",
    "    for ticker in batch:\n",
    "        try:\n",
    "            df_t = yf.download(ticker, start=start, progress=False, actions=True)\n",
    "            result[ticker] = df_t if not df_t.empty else None\n",
    "        except Exception as e:\n",
    "            logging.warning(\"Fallback individual falhou para %s: %s\", ticker, e)\n",
    "            result[ticker] = None\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb7f708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_all_histories(tickers: List[str], start: str = DEFAULT_START, force: bool = False, save_summary: bool = True, save_csv_per_ticker: bool = True):\n",
    "    \"\"\"\n",
    "    Processo principal: recebe lista de tickers (strings), baixa históricos e salva parquet + csv por ticker.\n",
    "    - force: se True, re-baixa mesmo que arquivo exista.\n",
    "    - save_csv_per_ticker: se True salva um CSV para cada ticker (além do parquet).\n",
    "    - retorna um DataFrame resumo com status por ticker.\n",
    "    \"\"\"\n",
    "    os.makedirs(HIST_DIR, exist_ok=True)\n",
    "    tickers = [t for t in tickers if isinstance(t, str) and t.strip()]\n",
    "    tickers = list(dict.fromkeys(tickers))\n",
    "    summary = []\n",
    "    for i in range(0, len(tickers), BATCH_SIZE):\n",
    "        batch = tickers[i:i+BATCH_SIZE]\n",
    "        to_download = [t for t in batch if force or not ticker_exists_local(t)]\n",
    "        if not to_download:\n",
    "            logging.info(\"Batch %d: todos já existem localmente — pulando.\", i//BATCH_SIZE+1)\n",
    "            for t in batch:\n",
    "                summary.append({\n",
    "                    \"ticker\": t,\n",
    "                    \"status\": \"skipped_local\",\n",
    "                    \"rows\": None,\n",
    "                    \"saved_parquet\": os.path.join(HIST_DIR, f\"{t}.parquet\") if ticker_exists_local(t) else None,\n",
    "                    \"saved_csv\": os.path.join(HIST_DIR, f\"{t}.csv\") if os.path.exists(os.path.join(HIST_DIR, f\"{t}.csv\")) else None\n",
    "                })\n",
    "            continue\n",
    "\n",
    "        logging.info(\"Processando batch %d/%d (download %d/%d)\", i//BATCH_SIZE+1, (len(tickers)+BATCH_SIZE-1)//BATCH_SIZE, len(to_download), len(batch))\n",
    "        results = download_batch(to_download, start=start)\n",
    "        for t in batch:\n",
    "            df_t = results.get(t) if t in results else None\n",
    "            if df_t is None or (isinstance(df_t, pd.DataFrame) and df_t.empty):\n",
    "                logging.warning(\"Nenhum dado para %s em batch; tentativa isolada...\", t)\n",
    "                try:\n",
    "                    single = yf.download(t, start=start, progress=False, actions=True)\n",
    "                    df_t = single if not single.empty else None\n",
    "                except Exception:\n",
    "                    df_t = None\n",
    "            if df_t is None or df_t.empty:\n",
    "                logging.error(\"Falha obtendo dados para %s\", t)\n",
    "                summary.append({\"ticker\": t, \"status\": \"failed\", \"rows\": 0, \"saved_parquet\": None, \"saved_csv\": None})\n",
    "            else:\n",
    "                try:\n",
    "                    save_history_df(t, df_t, save_csv=save_csv_per_ticker)\n",
    "                    summary.append({\n",
    "                        \"ticker\": t,\n",
    "                        \"status\": \"ok\",\n",
    "                        \"rows\": len(df_t),\n",
    "                        \"saved_parquet\": os.path.join(HIST_DIR, f\"{t}.parquet\"),\n",
    "                        \"saved_csv\": os.path.join(HIST_DIR, f\"{t}.csv\") if save_csv_per_ticker else None\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    logging.exception(\"Erro salvando para %s: %s\", t, e)\n",
    "                    summary.append({\"ticker\": t, \"status\": \"save_error\", \"rows\": len(df_t) if isinstance(df_t, pd.DataFrame) else None, \"saved_parquet\": None, \"saved_csv\": None})\n",
    "            time.sleep(SLEEP_BETWEEN_TICKERS)\n",
    "        time.sleep(SLEEP_BETWEEN_BATCHES)\n",
    "\n",
    "    df_summary = pd.DataFrame(summary)\n",
    "    if save_summary:\n",
    "        ts = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "        summary_path = os.path.join(HIST_DIR, f\"download_summary_{ts}.csv\")\n",
    "        df_summary.to_csv(summary_path, index=False)\n",
    "        logging.info(\"Resumo salvo em %s\", summary_path)\n",
    "    return df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8175894c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:33:22,428 INFO Batch 1: todos já existem localmente — pulando.\n",
      "2025-10-16 14:33:22,435 INFO Batch 2: todos já existem localmente — pulando.\n",
      "2025-10-16 14:33:22,439 INFO Batch 3: todos já existem localmente — pulando.\n",
      "2025-10-16 14:33:22,447 INFO Batch 4: todos já existem localmente — pulando.\n",
      "2025-10-16 14:33:22,453 INFO Batch 5: todos já existem localmente — pulando.\n",
      "2025-10-16 14:33:22,459 INFO Batch 6: todos já existem localmente — pulando.\n",
      "2025-10-16 14:33:22,464 INFO Batch 7: todos já existem localmente — pulando.\n",
      "2025-10-16 14:33:22,539 INFO Resumo salvo em data\\historical\\download_summary_20251016T173322Z.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tickers a baixar: 97 ['ALOS3.SA', 'ABEV3.SA', 'ANIM3.SA', 'ASAI3.SA', 'AURE3.SA', 'AZZA3.SA', 'BBSE3.SA', 'BBDC3.SA', 'BBDC4.SA', 'BRAP4.SA']\n",
      "       ticker         status  rows                      saved_parquet  \\\n",
      "0    ALOS3.SA  skipped_local  None   data\\historical\\ALOS3.SA.parquet   \n",
      "1    ABEV3.SA  skipped_local  None   data\\historical\\ABEV3.SA.parquet   \n",
      "2    ANIM3.SA  skipped_local  None   data\\historical\\ANIM3.SA.parquet   \n",
      "3    ASAI3.SA  skipped_local  None   data\\historical\\ASAI3.SA.parquet   \n",
      "4    AURE3.SA  skipped_local  None   data\\historical\\AURE3.SA.parquet   \n",
      "5    AZZA3.SA  skipped_local  None   data\\historical\\AZZA3.SA.parquet   \n",
      "6    BBSE3.SA  skipped_local  None   data\\historical\\BBSE3.SA.parquet   \n",
      "7    BBDC3.SA  skipped_local  None   data\\historical\\BBDC3.SA.parquet   \n",
      "8    BBDC4.SA  skipped_local  None   data\\historical\\BBDC4.SA.parquet   \n",
      "9    BRAP4.SA  skipped_local  None   data\\historical\\BRAP4.SA.parquet   \n",
      "10   BBAS3.SA  skipped_local  None   data\\historical\\BBAS3.SA.parquet   \n",
      "11   BRKM5.SA  skipped_local  None   data\\historical\\BRKM5.SA.parquet   \n",
      "12   BRAV3.SA  skipped_local  None   data\\historical\\BRAV3.SA.parquet   \n",
      "13  BPAC11.SA  skipped_local  None  data\\historical\\BPAC11.SA.parquet   \n",
      "14   CXSE3.SA  skipped_local  None   data\\historical\\CXSE3.SA.parquet   \n",
      "15   CEAB3.SA  skipped_local  None   data\\historical\\CEAB3.SA.parquet   \n",
      "16   CMIG4.SA  skipped_local  None   data\\historical\\CMIG4.SA.parquet   \n",
      "17   COGN3.SA  skipped_local  None   data\\historical\\COGN3.SA.parquet   \n",
      "18   CSMG3.SA  skipped_local  None   data\\historical\\CSMG3.SA.parquet   \n",
      "19   CPLE3.SA  skipped_local  None   data\\historical\\CPLE3.SA.parquet   \n",
      "20   CPLE6.SA  skipped_local  None   data\\historical\\CPLE6.SA.parquet   \n",
      "21   CSAN3.SA  skipped_local  None   data\\historical\\CSAN3.SA.parquet   \n",
      "22   CPFE3.SA  skipped_local  None   data\\historical\\CPFE3.SA.parquet   \n",
      "23   CMIN3.SA  skipped_local  None   data\\historical\\CMIN3.SA.parquet   \n",
      "24   CURY3.SA  skipped_local  None   data\\historical\\CURY3.SA.parquet   \n",
      "25   CVCB3.SA  skipped_local  None   data\\historical\\CVCB3.SA.parquet   \n",
      "26   CYRE3.SA  skipped_local  None   data\\historical\\CYRE3.SA.parquet   \n",
      "27   DIRR3.SA  skipped_local  None   data\\historical\\DIRR3.SA.parquet   \n",
      "28   ECOR3.SA  skipped_local  None   data\\historical\\ECOR3.SA.parquet   \n",
      "29   ELET3.SA  skipped_local  None   data\\historical\\ELET3.SA.parquet   \n",
      "30   ELET6.SA  skipped_local  None   data\\historical\\ELET6.SA.parquet   \n",
      "31   EMBR3.SA  skipped_local  None   data\\historical\\EMBR3.SA.parquet   \n",
      "32  ENGI11.SA  skipped_local  None  data\\historical\\ENGI11.SA.parquet   \n",
      "33   ENEV3.SA  skipped_local  None   data\\historical\\ENEV3.SA.parquet   \n",
      "34   EGIE3.SA  skipped_local  None   data\\historical\\EGIE3.SA.parquet   \n",
      "35   EQTL3.SA  skipped_local  None   data\\historical\\EQTL3.SA.parquet   \n",
      "36   EZTC3.SA  skipped_local  None   data\\historical\\EZTC3.SA.parquet   \n",
      "37   FLRY3.SA  skipped_local  None   data\\historical\\FLRY3.SA.parquet   \n",
      "38   GGBR4.SA  skipped_local  None   data\\historical\\GGBR4.SA.parquet   \n",
      "39   GOAU4.SA  skipped_local  None   data\\historical\\GOAU4.SA.parquet   \n",
      "40   GGPS3.SA  skipped_local  None   data\\historical\\GGPS3.SA.parquet   \n",
      "41   GMAT3.SA  skipped_local  None   data\\historical\\GMAT3.SA.parquet   \n",
      "42   HAPV3.SA  skipped_local  None   data\\historical\\HAPV3.SA.parquet   \n",
      "43   HYPE3.SA  skipped_local  None   data\\historical\\HYPE3.SA.parquet   \n",
      "44  IGTI11.SA  skipped_local  None  data\\historical\\IGTI11.SA.parquet   \n",
      "45   INTB3.SA  skipped_local  None   data\\historical\\INTB3.SA.parquet   \n",
      "46   IRBR3.SA  skipped_local  None   data\\historical\\IRBR3.SA.parquet   \n",
      "47   ISAE4.SA  skipped_local  None   data\\historical\\ISAE4.SA.parquet   \n",
      "48   ITSA4.SA  skipped_local  None   data\\historical\\ITSA4.SA.parquet   \n",
      "49   ITUB4.SA  skipped_local  None   data\\historical\\ITUB4.SA.parquet   \n",
      "\n",
      "                        saved_csv  \n",
      "0    data\\historical\\ALOS3.SA.csv  \n",
      "1    data\\historical\\ABEV3.SA.csv  \n",
      "2    data\\historical\\ANIM3.SA.csv  \n",
      "3    data\\historical\\ASAI3.SA.csv  \n",
      "4    data\\historical\\AURE3.SA.csv  \n",
      "5    data\\historical\\AZZA3.SA.csv  \n",
      "6    data\\historical\\BBSE3.SA.csv  \n",
      "7    data\\historical\\BBDC3.SA.csv  \n",
      "8    data\\historical\\BBDC4.SA.csv  \n",
      "9    data\\historical\\BRAP4.SA.csv  \n",
      "10   data\\historical\\BBAS3.SA.csv  \n",
      "11   data\\historical\\BRKM5.SA.csv  \n",
      "12   data\\historical\\BRAV3.SA.csv  \n",
      "13  data\\historical\\BPAC11.SA.csv  \n",
      "14   data\\historical\\CXSE3.SA.csv  \n",
      "15   data\\historical\\CEAB3.SA.csv  \n",
      "16   data\\historical\\CMIG4.SA.csv  \n",
      "17   data\\historical\\COGN3.SA.csv  \n",
      "18   data\\historical\\CSMG3.SA.csv  \n",
      "19   data\\historical\\CPLE3.SA.csv  \n",
      "20   data\\historical\\CPLE6.SA.csv  \n",
      "21   data\\historical\\CSAN3.SA.csv  \n",
      "22   data\\historical\\CPFE3.SA.csv  \n",
      "23   data\\historical\\CMIN3.SA.csv  \n",
      "24   data\\historical\\CURY3.SA.csv  \n",
      "25   data\\historical\\CVCB3.SA.csv  \n",
      "26   data\\historical\\CYRE3.SA.csv  \n",
      "27   data\\historical\\DIRR3.SA.csv  \n",
      "28   data\\historical\\ECOR3.SA.csv  \n",
      "29   data\\historical\\ELET3.SA.csv  \n",
      "30   data\\historical\\ELET6.SA.csv  \n",
      "31   data\\historical\\EMBR3.SA.csv  \n",
      "32  data\\historical\\ENGI11.SA.csv  \n",
      "33   data\\historical\\ENEV3.SA.csv  \n",
      "34   data\\historical\\EGIE3.SA.csv  \n",
      "35   data\\historical\\EQTL3.SA.csv  \n",
      "36   data\\historical\\EZTC3.SA.csv  \n",
      "37   data\\historical\\FLRY3.SA.csv  \n",
      "38   data\\historical\\GGBR4.SA.csv  \n",
      "39   data\\historical\\GOAU4.SA.csv  \n",
      "40   data\\historical\\GGPS3.SA.csv  \n",
      "41   data\\historical\\GMAT3.SA.csv  \n",
      "42   data\\historical\\HAPV3.SA.csv  \n",
      "43   data\\historical\\HYPE3.SA.csv  \n",
      "44  data\\historical\\IGTI11.SA.csv  \n",
      "45   data\\historical\\INTB3.SA.csv  \n",
      "46   data\\historical\\IRBR3.SA.csv  \n",
      "47   data\\historical\\ISAE4.SA.csv  \n",
      "48   data\\historical\\ITSA4.SA.csv  \n",
      "49   data\\historical\\ITUB4.SA.csv  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 14:33:25,833 INFO Combined saved to data\\historical\\all_histories.parquet (rows=1256344)\n",
      "2025-10-16 14:33:37,326 INFO Combined CSV saved to data\\historical\\all_histories.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined rows: 1256344\n"
     ]
    }
   ],
   "source": [
    "# utilitário para recombinar todos os parquets em um único arquivo (long format)\n",
    "def combine_all_to_single_parquet(out_path: str = os.path.join(HIST_DIR, \"all_histories.parquet\"), out_csv: Optional[str] = os.path.join(HIST_DIR, \"all_histories.csv\"), tickers: Optional[List[str]] = None):\n",
    "    \"\"\"\n",
    "    Lê todos os parquets em HIST_DIR (ou tickers list) e concatena em formato long:\n",
    "    columns: ['ticker','date', 'Open','High','Low','Close','Adj Close','Volume', 'Dividends','Stock Splits']\n",
    "    Salva em parquet e opcionalmente em csv.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    if tickers:\n",
    "        files = [os.path.join(HIST_DIR, f\"{t}.parquet\") for t in tickers if os.path.exists(os.path.join(HIST_DIR, f\"{t}.parquet\"))]\n",
    "    else:\n",
    "        files = [os.path.join(HIST_DIR, f) for f in os.listdir(HIST_DIR) if f.endswith(\".parquet\")]\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = pd.read_parquet(f)\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "            fname = os.path.basename(f).replace(\".parquet\",\"\")\n",
    "            if 'ticker' not in df.columns:\n",
    "                df.insert(0, 'ticker', fname)\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            logging.warning(\"Erro lendo %s: %s\", f, e)\n",
    "    if not dfs:\n",
    "        raise RuntimeError(\"Nenhum parquet encontrado para combinar.\")\n",
    "    big = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "    big.to_parquet(out_path, index=False)\n",
    "    logging.info(\"Combined saved to %s (rows=%d)\", out_path, len(big))\n",
    "    if out_csv:\n",
    "        try:\n",
    "            # converter date para formato iso ao salvar CSV\n",
    "            if 'date' in big.columns:\n",
    "                big['date'] = pd.to_datetime(big['date']).dt.strftime('%Y-%m-%d')\n",
    "            big.to_csv(out_csv, index=False)\n",
    "            logging.info(\"Combined CSV saved to %s\", out_csv)\n",
    "        except Exception as e:\n",
    "            logging.exception(\"Erro salvando combined CSV: %s\", e)\n",
    "    return big\n",
    "\n",
    "# ------------------------------\n",
    "# Exemplo de uso\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) carregue a lista de tickers a partir do arquivo que você já salvou\n",
    "    tickers_file = os.path.join(\"tickers_ibrx100_full.csv\")\n",
    "    if os.path.exists(tickers_file):\n",
    "        df = pd.read_csv(tickers_file)\n",
    "        if 'Ticker' in df.columns:\n",
    "            tickers = df['Ticker'].dropna().astype(str).tolist()\n",
    "        else:\n",
    "            tickers = df.iloc[:,0].dropna().astype(str).tolist()\n",
    "    else:\n",
    "        raise RuntimeError(f\"Não encontrou {tickers_file}. Coloque seu CSV de tickers no mesmo diretório ou edite este script.\")\n",
    "\n",
    "    # 2) opção: validar/normalizar tickers (garantir sufixo .SA)\n",
    "    def normalize(t):\n",
    "        t = str(t).strip().upper()\n",
    "        if not t.endswith(\".SA\"):\n",
    "            t = t.replace(\".SA\",\"\") + \".SA\"\n",
    "        return t\n",
    "    tickers = [normalize(t) for t in tickers]\n",
    "    print(\"Tickers a baixar:\", len(tickers), tickers[:10])\n",
    "\n",
    "    # 3) executar (force=True re-baixa mesmo se já existir)\n",
    "    summary_df = download_all_histories(tickers, start=DEFAULT_START, force=False, save_summary=True, save_csv_per_ticker=True)\n",
    "    print(summary_df.head(50))\n",
    "\n",
    "    # 4) opcional: combinar tudo em um único parquet e CSV (pode ser grande)\n",
    "    combined = combine_all_to_single_parquet()\n",
    "    print(\"Combined rows:\", len(combined))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_aurum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
