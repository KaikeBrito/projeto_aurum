{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e47310e-3b74-4d20-8806-195412f64c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "download_histories.py\n",
    "\n",
    "Uso:\n",
    "- coloque a lista de tickers (formato ex: 'ABEV3.SA') em `tickers`.\n",
    "- execute este script: python download_histories.py\n",
    "- os arquivos serão salvos em ./data/historical/<TICKER>.parquet e ./data/historical/<TICKER>.csv\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from tqdm import tqdm\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d667dd83-abb4-4d22-8956-19ac2f52935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging simples\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "# diretório onde os históricos serão salvos (compatível com seu script anterior)\n",
    "HIST_DIR = os.path.join(\"data\", \"historical\")\n",
    "os.makedirs(HIST_DIR, exist_ok=True)\n",
    "\n",
    "# parâmetros de download\n",
    "DEFAULT_START = \"1998-01-01\"\n",
    "BATCH_SIZE = 15            # número de tickers por chamada yfinance.download (ajuste se achar necessário)\n",
    "MAX_ATTEMPTS = 4           # tentativas por batch\n",
    "SLEEP_BETWEEN_BATCHES = 1  # segundos entre batches para reduzir risco de throttling\n",
    "SLEEP_BETWEEN_TICKERS = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4118df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticker_exists_local(ticker: str) -> bool:\n",
    "    \"\"\"Verifica se já existe parquet salvo para ticker (usa para pular downloads)\"\"\"\n",
    "    path = os.path.join(HIST_DIR, f\"{ticker}.parquet\")\n",
    "    return os.path.isfile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_history_df(ticker: str, df: pd.DataFrame, save_csv: bool = True):\n",
    "    \"\"\"Salva DataFrame em parquet e opcionalmente em CSV. Garante coluna 'date' se índice for DatetimeIndex.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        raise ValueError(\"DataFrame nulo ou vazio\")\n",
    "    df = df.copy()\n",
    "    # garantir que a coluna de data exista como coluna\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index.name = \"date\"\n",
    "        df = df.reset_index()\n",
    "    # converter coluna date para string ISO ao salvar CSV (mantém compatibilidade)\n",
    "    out_parquet = os.path.join(HIST_DIR, f\"{ticker}.parquet\")\n",
    "    out_csv = os.path.join(HIST_DIR, f\"{ticker}.csv\")\n",
    "    try:\n",
    "        df.to_parquet(out_parquet, index=False)\n",
    "        logging.info(\"Saved %s rows for %s -> %s\", len(df), ticker, out_parquet)\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Erro salvando parquet para %s: %s\", ticker, e)\n",
    "        raise\n",
    "    if save_csv:\n",
    "        try:\n",
    "            # padronizar data para ISO antes de salvar CSV (se existir)\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')\n",
    "            df.to_csv(out_csv, index=False)\n",
    "            logging.info(\"Saved CSV for %s -> %s\", ticker, out_csv)\n",
    "        except Exception as e:\n",
    "            logging.exception(\"Erro salvando CSV para %s: %s\", ticker, e)\n",
    "            # não raise — parquet já salvo, apenas logamos o problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a892b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_batch(batch: List[str], start: str = DEFAULT_START, threads: bool = True) -> Dict[str, Optional[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Tenta baixar um batch de tickers via yfinance.download.\n",
    "    Retorna dict ticker -> DataFrame or None (se falhou).\n",
    "    \"\"\"\n",
    "    joined = \" \".join(batch)\n",
    "    attempt = 0\n",
    "    last_exc = None\n",
    "    while attempt < MAX_ATTEMPTS:\n",
    "        try:\n",
    "            logging.info(\"yfinance.download attempt %d for batch size %d\", attempt+1, len(batch))\n",
    "            data = yf.download(tickers=joined, start=start, progress=False, threads=threads, group_by='ticker', auto_adjust=False, actions=True)\n",
    "            result = {}\n",
    "            if isinstance(data, pd.DataFrame) and isinstance(data.columns, pd.MultiIndex):\n",
    "                for ticker in batch:\n",
    "                    if ticker in data.columns.get_level_values(0):\n",
    "                        df_t = data[ticker].copy()\n",
    "                        result[ticker] = df_t\n",
    "                    else:\n",
    "                        try:\n",
    "                            single = yf.download(ticker, start=start, progress=False, actions=True)\n",
    "                            result[ticker] = single if not single.empty else None\n",
    "                        except Exception:\n",
    "                            result[ticker] = None\n",
    "            else:\n",
    "                for ticker in batch:\n",
    "                    try:\n",
    "                        df_t = yf.download(ticker, start=start, progress=False, actions=True)\n",
    "                        result[ticker] = df_t if not df_t.empty else None\n",
    "                    except Exception:\n",
    "                        result[ticker] = None\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "            logging.warning(\"Erro no yfinance.download (attempt %d): %s\", attempt+1, str(e))\n",
    "            attempt += 1\n",
    "            time.sleep(2 ** attempt)  # backoff exponencial\n",
    "    logging.error(\"Todas tentativas falharam para batch (%s). Último erro: %s\", joined, last_exc)\n",
    "    # fallback: tentar baixar ticker a ticker\n",
    "    result = {}\n",
    "    for ticker in batch:\n",
    "        try:\n",
    "            df_t = yf.download(ticker, start=start, progress=False, actions=True)\n",
    "            result[ticker] = df_t if not df_t.empty else None\n",
    "        except Exception as e:\n",
    "            logging.warning(\"Fallback individual falhou para %s: %s\", ticker, e)\n",
    "            result[ticker] = None\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7f708e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_all_histories(tickers: List[str], start: str = DEFAULT_START, force: bool = False, save_summary: bool = True, save_csv_per_ticker: bool = True):\n",
    "    \"\"\"\n",
    "    Processo principal: recebe lista de tickers (strings), baixa históricos e salva parquet + csv por ticker.\n",
    "    - force: se True, re-baixa mesmo que arquivo exista.\n",
    "    - save_csv_per_ticker: se True salva um CSV para cada ticker (além do parquet).\n",
    "    - retorna um DataFrame resumo com status por ticker.\n",
    "    \"\"\"\n",
    "    os.makedirs(HIST_DIR, exist_ok=True)\n",
    "    tickers = [t for t in tickers if isinstance(t, str) and t.strip()]\n",
    "    tickers = list(dict.fromkeys(tickers))\n",
    "    summary = []\n",
    "    for i in range(0, len(tickers), BATCH_SIZE):\n",
    "        batch = tickers[i:i+BATCH_SIZE]\n",
    "        to_download = [t for t in batch if force or not ticker_exists_local(t)]\n",
    "        if not to_download:\n",
    "            logging.info(\"Batch %d: todos já existem localmente — pulando.\", i//BATCH_SIZE+1)\n",
    "            for t in batch:\n",
    "                summary.append({\n",
    "                    \"ticker\": t,\n",
    "                    \"status\": \"skipped_local\",\n",
    "                    \"rows\": None,\n",
    "                    \"saved_parquet\": os.path.join(HIST_DIR, f\"{t}.parquet\") if ticker_exists_local(t) else None,\n",
    "                    \"saved_csv\": os.path.join(HIST_DIR, f\"{t}.csv\") if os.path.exists(os.path.join(HIST_DIR, f\"{t}.csv\")) else None\n",
    "                })\n",
    "            continue\n",
    "\n",
    "        logging.info(\"Processando batch %d/%d (download %d/%d)\", i//BATCH_SIZE+1, (len(tickers)+BATCH_SIZE-1)//BATCH_SIZE, len(to_download), len(batch))\n",
    "        results = download_batch(to_download, start=start)\n",
    "        for t in batch:\n",
    "            df_t = results.get(t) if t in results else None\n",
    "            if df_t is None or (isinstance(df_t, pd.DataFrame) and df_t.empty):\n",
    "                logging.warning(\"Nenhum dado para %s em batch; tentativa isolada...\", t)\n",
    "                try:\n",
    "                    single = yf.download(t, start=start, progress=False, actions=True)\n",
    "                    df_t = single if not single.empty else None\n",
    "                except Exception:\n",
    "                    df_t = None\n",
    "            if df_t is None or df_t.empty:\n",
    "                logging.error(\"Falha obtendo dados para %s\", t)\n",
    "                summary.append({\"ticker\": t, \"status\": \"failed\", \"rows\": 0, \"saved_parquet\": None, \"saved_csv\": None})\n",
    "            else:\n",
    "                try:\n",
    "                    save_history_df(t, df_t, save_csv=save_csv_per_ticker)\n",
    "                    summary.append({\n",
    "                        \"ticker\": t,\n",
    "                        \"status\": \"ok\",\n",
    "                        \"rows\": len(df_t),\n",
    "                        \"saved_parquet\": os.path.join(HIST_DIR, f\"{t}.parquet\"),\n",
    "                        \"saved_csv\": os.path.join(HIST_DIR, f\"{t}.csv\") if save_csv_per_ticker else None\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    logging.exception(\"Erro salvando para %s: %s\", t, e)\n",
    "                    summary.append({\"ticker\": t, \"status\": \"save_error\", \"rows\": len(df_t) if isinstance(df_t, pd.DataFrame) else None, \"saved_parquet\": None, \"saved_csv\": None})\n",
    "            time.sleep(SLEEP_BETWEEN_TICKERS)\n",
    "        time.sleep(SLEEP_BETWEEN_BATCHES)\n",
    "\n",
    "    df_summary = pd.DataFrame(summary)\n",
    "    if save_summary:\n",
    "        ts = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "        summary_path = os.path.join(HIST_DIR, f\"download_summary_{ts}.csv\")\n",
    "        df_summary.to_csv(summary_path, index=False)\n",
    "        logging.info(\"Resumo salvo em %s\", summary_path)\n",
    "    return df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8175894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilitário para recombinar todos os parquets em um único arquivo (long format)\n",
    "def combine_all_to_single_parquet(out_path: str = os.path.join(HIST_DIR, \"all_histories.parquet\"), out_csv: Optional[str] = os.path.join(HIST_DIR, \"all_histories.csv\"), tickers: Optional[List[str]] = None):\n",
    "    \"\"\"\n",
    "    Lê todos os parquets em HIST_DIR (ou tickers list) e concatena em formato long:\n",
    "    columns: ['ticker','date', 'Open','High','Low','Close','Adj Close','Volume', 'Dividends','Stock Splits']\n",
    "    Salva em parquet e opcionalmente em csv.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    if tickers:\n",
    "        files = [os.path.join(HIST_DIR, f\"{t}.parquet\") for t in tickers if os.path.exists(os.path.join(HIST_DIR, f\"{t}.parquet\"))]\n",
    "    else:\n",
    "        files = [os.path.join(HIST_DIR, f) for f in os.listdir(HIST_DIR) if f.endswith(\".parquet\")]\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = pd.read_parquet(f)\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "            fname = os.path.basename(f).replace(\".parquet\",\"\")\n",
    "            if 'ticker' not in df.columns:\n",
    "                df.insert(0, 'ticker', fname)\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            logging.warning(\"Erro lendo %s: %s\", f, e)\n",
    "    if not dfs:\n",
    "        raise RuntimeError(\"Nenhum parquet encontrado para combinar.\")\n",
    "    big = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "    big.to_parquet(out_path, index=False)\n",
    "    logging.info(\"Combined saved to %s (rows=%d)\", out_path, len(big))\n",
    "    if out_csv:\n",
    "        try:\n",
    "            # converter date para formato iso ao salvar CSV\n",
    "            if 'date' in big.columns:\n",
    "                big['date'] = pd.to_datetime(big['date']).dt.strftime('%Y-%m-%d')\n",
    "            big.to_csv(out_csv, index=False)\n",
    "            logging.info(\"Combined CSV saved to %s\", out_csv)\n",
    "        except Exception as e:\n",
    "            logging.exception(\"Erro salvando combined CSV: %s\", e)\n",
    "    return big\n",
    "\n",
    "# ------------------------------\n",
    "# Exemplo de uso\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) carregue a lista de tickers a partir do arquivo que você já salvou\n",
    "    tickers_file = os.path.join(\"tickers_ibrx100_full.csv\")\n",
    "    if os.path.exists(tickers_file):\n",
    "        df = pd.read_csv(tickers_file)\n",
    "        if 'Ticker' in df.columns:\n",
    "            tickers = df['Ticker'].dropna().astype(str).tolist()\n",
    "        else:\n",
    "            tickers = df.iloc[:,0].dropna().astype(str).tolist()\n",
    "    else:\n",
    "        raise RuntimeError(f\"Não encontrou {tickers_file}. Coloque seu CSV de tickers no mesmo diretório ou edite este script.\")\n",
    "\n",
    "    # 2) opção: validar/normalizar tickers (garantir sufixo .SA)\n",
    "    def normalize(t):\n",
    "        t = str(t).strip().upper()\n",
    "        if not t.endswith(\".SA\"):\n",
    "            t = t.replace(\".SA\",\"\") + \".SA\"\n",
    "        return t\n",
    "    tickers = [normalize(t) for t in tickers]\n",
    "    print(\"Tickers a baixar:\", len(tickers), tickers[:10])\n",
    "\n",
    "    # 3) executar (force=True re-baixa mesmo se já existir)\n",
    "    summary_df = download_all_histories(tickers, start=DEFAULT_START, force=False, save_summary=True, save_csv_per_ticker=True)\n",
    "    print(summary_df.head(50))\n",
    "\n",
    "    # 4) opcional: combinar tudo em um único parquet e CSV (pode ser grande)\n",
    "    combined = combine_all_to_single_parquet()\n",
    "    print(\"Combined rows:\", len(combined))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_aurum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
