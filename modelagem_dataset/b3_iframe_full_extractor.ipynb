{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f68fae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b3_iframe_full_extractor.py\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "from io import StringIO\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Selenium imports\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import (\n",
    "    StaleElementReferenceException,\n",
    "    ElementClickInterceptedException,\n",
    "    TimeoutException,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "410d0d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "TICKER_REGEX = re.compile(r'^[A-Z]{2,6}\\d{1,2}$', re.I)\n",
    "HEADER_KEYWORDS = {\"code\", \"stock\", \"ativo\", \"código\", \"codigo\", \"symbol\", \"ticker\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "043fc294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_token(tok: str):\n",
    "    if not isinstance(tok, str):\n",
    "        return None\n",
    "    s = tok.strip().upper().split()[0]\n",
    "    s = re.sub(r'[^A-Z0-9]', '', s)\n",
    "    if TICKER_REGEX.match(s):\n",
    "        return s + '.SA'\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07efb82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def try_download_csv_from_iframe(iframe_url, headers=None, timeout=10):\n",
    "    \"\"\"\n",
    "    Tenta baixar CSV diretamente do conteúdo do iframe.\n",
    "    Também procura na página por links que apontem para arquivos .csv (download).\n",
    "    Retorna list de tickers se achou, senão None.\n",
    "    \"\"\"\n",
    "    headers = headers or {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    try:\n",
    "        r = requests.get(iframe_url, headers=headers, timeout=timeout)\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(\"requests ao iframe falhou:\", e)\n",
    "        return None\n",
    "\n",
    "    # Se a resposta já for CSV (começa com \"Code,\"...), parse direto\n",
    "    text = r.text\n",
    "    if text.lstrip().startswith(\"Code,\") or text.lstrip().splitlines()[0].lower().startswith(\"code,\"):\n",
    "        print(\"Encontrado CSV diretamente no iframe (conteúdo).\")\n",
    "        df = pd.read_csv(StringIO(text))\n",
    "        if \"Code\" in df.columns or \"code\" in [c.lower() for c in df.columns]:\n",
    "            col = next(c for c in df.columns if c.lower() == \"code\")\n",
    "            tickers = [normalize_token(x) for x in df[col].astype(str)]\n",
    "            tickers = [t for t in tickers if t]\n",
    "            return list(dict.fromkeys(tickers))\n",
    "\n",
    "    # Se HTML, procurar links <a> com href terminando em .csv ou com \"composition\"\n",
    "    soup = BeautifulSoup(text, \"lxml\")\n",
    "    links = soup.find_all(\"a\", href=True)\n",
    "    for a in links:\n",
    "        href = a[\"href\"]\n",
    "        href_low = href.lower()\n",
    "        # link relativo? montar absoluto\n",
    "        if href_low.endswith(\".csv\") or \"composition\" in href_low and (\".csv\" in href_low or \"download\" in href_low or \"composition\" in href_low):\n",
    "            # montar url absoluto se necessário\n",
    "            if href.startswith(\"//\"):\n",
    "                href = \"https:\" + href\n",
    "            elif href.startswith(\"/\"):\n",
    "                base = re.match(r'^(https?://[^/]+)', iframe_url)\n",
    "                if base:\n",
    "                    href = base.group(1) + href\n",
    "            # tentar baixar\n",
    "            try:\n",
    "                rr = requests.get(href, headers=headers, timeout=timeout)\n",
    "                rr.raise_for_status()\n",
    "                txt = rr.text\n",
    "                # tentar parsear CSV\n",
    "                try:\n",
    "                    df = pd.read_csv(StringIO(txt))\n",
    "                except Exception:\n",
    "                    # às vezes tem BOM ou encoding diferente\n",
    "                    df = pd.read_csv(StringIO(txt.encode('utf-8').decode('utf-8', errors='ignore')))\n",
    "                # detectar coluna de códigos\n",
    "                col = None\n",
    "                for c in df.columns:\n",
    "                    if str(c).lower() in (\"code\",\"codigo\",\"codigo ativo\",\"codigo ativo\",\"code \"):\n",
    "                        col = c; break\n",
    "                if col is None:\n",
    "                    # fallback: tentar primeira coluna\n",
    "                    col = df.columns[0]\n",
    "                tickers = [normalize_token(x) for x in df[col].astype(str)]\n",
    "                tickers = [t for t in tickers if t]\n",
    "                print(f\"Download CSV via link bem-sucedido: {href}\")\n",
    "                return list(dict.fromkeys(tickers))\n",
    "            except Exception as e:\n",
    "                # continuar procurando outros links\n",
    "                print(\"Falha ao baixar/parsar link csv:\", href, e)\n",
    "                continue\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ce6fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Selenium robust pagination fallback ----------------\n",
    "def selenium_extract_all_from_iframe(iframe_url, headless=True, wait_seconds=10, debug=False):\n",
    "    options = Options()\n",
    "    if headless:\n",
    "        options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(\"--window-size=1920,1200\")\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    try:\n",
    "        driver.get(iframe_url)\n",
    "        # esperar pela tabela (ou pelo conteúdo)\n",
    "        try:\n",
    "            WebDriverWait(driver, wait_seconds).until(EC.presence_of_element_located((By.TAG_NAME, \"table\")))\n",
    "        except TimeoutException:\n",
    "            time.sleep(1.5)\n",
    "\n",
    "        # Scroll para baixo para expor selects/pagination\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(0.8)\n",
    "\n",
    "        # 1) tentar selecionar a maior page-size usando o texto da opção (não o value)\n",
    "        selects = driver.find_elements(By.TAG_NAME, \"select\")\n",
    "        if debug: print(f\"[debug] selects encontrados: {len(selects)}\")\n",
    "        made_change = False\n",
    "        for sel in selects:\n",
    "            try:\n",
    "                opts = sel.find_elements(By.TAG_NAME, \"option\")\n",
    "                numeric = []\n",
    "                for o in opts:\n",
    "                    txt = (o.text or \"\").strip()\n",
    "                    # extrair número do texto (ex: \"20\", \"20 per page\", \"Show 100\")\n",
    "                    m = re.search(r'(\\d+)', txt)\n",
    "                    if m:\n",
    "                        n = int(m.group(1))\n",
    "                        numeric.append((n, o))\n",
    "                if numeric:\n",
    "                    numeric_sorted = sorted(numeric, key=lambda x: x[0], reverse=True)\n",
    "                    max_n, option_elem = numeric_sorted[0]\n",
    "                    # clique com JS (mais robusto) ou normal click\n",
    "                    try:\n",
    "                        option_elem.click()\n",
    "                    except Exception:\n",
    "                        driver.execute_script(\"arguments[0].selected = true; arguments[0].dispatchEvent(new Event('change'))\", option_elem)\n",
    "                    if debug: print(f\"[debug] ajustei page-size para {max_n}\")\n",
    "                    made_change = True\n",
    "                    time.sleep(1.2)\n",
    "                    break\n",
    "            except StaleElementReferenceException:\n",
    "                continue\n",
    "\n",
    "        # função para extrair tickers da tabela visível\n",
    "        def extract_current():\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, \"lxml\")\n",
    "            tables = soup.find_all(\"table\")\n",
    "            for table in tables:\n",
    "                # identificar por cabeçalho\n",
    "                ths = [th.get_text(\" \", strip=True).lower() for th in table.find_all(\"th\")]\n",
    "                if not ths:\n",
    "                    first_tr = table.find(\"tr\")\n",
    "                    if first_tr:\n",
    "                        ths = [td.get_text(\" \", strip=True).lower() for td in first_tr.find_all([\"td\",\"th\"])]\n",
    "                joined = \" \".join(ths)\n",
    "                if any(k in joined for k in HEADER_KEYWORDS):\n",
    "                    # usar pandas para garantir consistência\n",
    "                    try:\n",
    "                        df = pd.read_html(StringIO(str(table)))[0]\n",
    "                        # localizar melhor coluna de código\n",
    "                        code_col = None\n",
    "                        for c in df.columns:\n",
    "                            if any(k in str(c).lower() for k in HEADER_KEYWORDS):\n",
    "                                code_col = c; break\n",
    "                        if code_col is None:\n",
    "                            # heurística: coluna com mais matches\n",
    "                            best_col = None; best_count = 0\n",
    "                            for c in df.columns:\n",
    "                                vals = df[c].astype(str).fillna('').tolist()\n",
    "                                cnt = sum(1 for v in vals if normalize_token(v))\n",
    "                                if cnt > best_count:\n",
    "                                    best_count = cnt; best_col = c\n",
    "                            code_col = best_col\n",
    "                        if code_col is None:\n",
    "                            continue\n",
    "                        vals = [normalize_token(x) for x in df[code_col].astype(str)]\n",
    "                        return [v for v in vals if v]\n",
    "                    except Exception:\n",
    "                        continue\n",
    "            if tables:\n",
    "                try:\n",
    "                    df = pd.read_html(StringIO(str(tables[0])))[0]\n",
    "                    best_col = None; best_count = 0\n",
    "                    for c in df.columns:\n",
    "                        vals = df[c].astype(str).fillna('').tolist()\n",
    "                        cnt = sum(1 for v in vals if normalize_token(v))\n",
    "                        if cnt > best_count:\n",
    "                            best_count = cnt; best_col = c\n",
    "                    if best_col is not None:\n",
    "                        vals = [normalize_token(x) for x in df[best_col].astype(str)]\n",
    "                        return [v for v in vals if v]\n",
    "                except Exception:\n",
    "                    pass\n",
    "            return []\n",
    "\n",
    "        collected = []\n",
    "        seen = set()\n",
    "\n",
    "        # extrair página inicial\n",
    "        page0 = extract_current()\n",
    "        if debug: print(f\"[debug] page0 count={len(page0)}\")\n",
    "        for t in page0:\n",
    "            if t not in seen:\n",
    "                seen.add(t); collected.append(t)\n",
    "\n",
    "        # 2) tentar localizar paginação numerada\n",
    "        # procurar por elementos comuns (li.paginate_button, a.page-link, ul.pagination li a)\n",
    "        page_numbers = set()\n",
    "        try:\n",
    "            els = driver.find_elements(By.CSS_SELECTOR, \"ul.pagination li, div.pagination li, nav.pagination li\")\n",
    "            for li in els:\n",
    "                try:\n",
    "                    a = li.find_element(By.TAG_NAME, \"a\")\n",
    "                    txt = (a.text or \"\").strip()\n",
    "                    if txt.isdigit():\n",
    "                        page_numbers.add(int(txt))\n",
    "                except Exception:\n",
    "                    continue\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            anchors = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "            for a in anchors:\n",
    "                try:\n",
    "                    txt = (a.text or \"\").strip()\n",
    "                    if txt.isdigit():\n",
    "                        page_numbers.add(int(txt))\n",
    "                except Exception:\n",
    "                    continue\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        page_numbers = sorted(page_numbers)\n",
    "        if debug: print(f\"[debug] detected page numbers: {page_numbers}\")\n",
    "\n",
    "        # se achou números, iterar por eles\n",
    "        if page_numbers:\n",
    "            for p in page_numbers:\n",
    "                if p == 1:\n",
    "                    continue\n",
    "                anchors = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                clicked = False\n",
    "                for a in anchors:\n",
    "                    try:\n",
    "                        txt = (a.text or \"\").strip()\n",
    "                        if txt == str(p):\n",
    "                            try:\n",
    "                                a.click()\n",
    "                            except (ElementClickInterceptedException, StaleElementReferenceException):\n",
    "                                driver.execute_script(\"arguments[0].click();\", a)\n",
    "                            # aguardar nova tabela\n",
    "                            try:\n",
    "                                WebDriverWait(driver, wait_seconds).until(EC.presence_of_element_located((By.TAG_NAME, \"table\")))\n",
    "                            except Exception:\n",
    "                                time.sleep(0.8)\n",
    "                            time.sleep(0.5)\n",
    "                            new = extract_current()\n",
    "                            if debug: print(f\"[debug] page {p} got {len(new)}\")\n",
    "                            for t in new:\n",
    "                                if t not in seen:\n",
    "                                    seen.add(t); collected.append(t)\n",
    "                            clicked = True\n",
    "                            break\n",
    "                    except StaleElementReferenceException:\n",
    "                        continue\n",
    "                if not clicked and debug:\n",
    "                    print(f\"[debug] could not click page {p}\")\n",
    "\n",
    "        else:\n",
    "            # se não há botões numerados, tentar clicar repetidamente em \"next\"\n",
    "            if debug: print(\"[debug] no numeric pages; trying next loop\")\n",
    "            while True:\n",
    "                found_next = None\n",
    "                anchors = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for a in anchors:\n",
    "                    try:\n",
    "                        txt = (a.text or \"\").strip().lower()\n",
    "                        if txt in (\">\", \"»\", \"next\", \"próximo\", \"proximo\"):\n",
    "                            found_next = a\n",
    "                            break\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                if not found_next:\n",
    "                    break\n",
    "                try:\n",
    "                    found_next.click()\n",
    "                except (ElementClickInterceptedException, StaleElementReferenceException):\n",
    "                    driver.execute_script(\"arguments[0].click();\", found_next)\n",
    "                try:\n",
    "                    WebDriverWait(driver, wait_seconds).until(EC.presence_of_element_located((By.TAG_NAME, \"table\")))\n",
    "                except Exception:\n",
    "                    time.sleep(0.8)\n",
    "                time.sleep(0.4)\n",
    "                new = extract_current()\n",
    "                added = 0\n",
    "                for t in new:\n",
    "                    if t not in seen:\n",
    "                        seen.add(t); collected.append(t); added += 1\n",
    "                if debug: print(f\"[debug] next loop added {added}\")\n",
    "                if added == 0:\n",
    "                    break\n",
    "\n",
    "        # salvar CSV e retornar\n",
    "        if collected:\n",
    "            with open(\"tickers_ibrx100_full.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\"Ticker\"])\n",
    "                for t in collected:\n",
    "                    writer.writerow([t])\n",
    "            if debug: print(f\"[debug] salvou {len(collected)} tickers\")\n",
    "            return collected\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "295377ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[debug] tentando URL: https://sistemaswebb3-listados.b3.com.br/indexPage/day/IBXX?language=en-us\n",
      "[debug] tentando URL: https://sistemaswebb3-listados.b3.com.br/indexPage/day/IBRX100?language=en-us\n",
      "[debug] tentando URL: https://sistemaswebb3-listados.b3.com.br/indexPage/day/IBRX-100?language=en-us\n",
      "[debug] Selenium tentando: https://sistemaswebb3-listados.b3.com.br/indexPage/day/IBXX?language=en-us\n",
      "[debug] selects encontrados: 2\n",
      "[debug] ajustei page-size para 120\n",
      "[debug] page0 count=97\n",
      "[debug] detected page numbers: []\n",
      "[debug] no numeric pages; trying next loop\n",
      "[debug] salvou 97 tickers\n",
      "✅ Extraído via Selenium. Tickers: 97\n",
      "TOTAL TICKERS: 97\n",
      "['ALOS3.SA', 'ABEV3.SA', 'ANIM3.SA', 'ASAI3.SA', 'AURE3.SA', 'AZZA3.SA', 'BBSE3.SA', 'BBDC3.SA', 'BBDC4.SA', 'BRAP4.SA', 'BBAS3.SA', 'BRKM5.SA', 'BRAV3.SA', 'BPAC11.SA', 'CXSE3.SA', 'CEAB3.SA', 'CMIG4.SA', 'COGN3.SA', 'CSMG3.SA', 'CPLE3.SA', 'CPLE6.SA', 'CSAN3.SA', 'CPFE3.SA', 'CMIN3.SA', 'CURY3.SA', 'CVCB3.SA', 'CYRE3.SA', 'DIRR3.SA', 'ECOR3.SA', 'ELET3.SA', 'ELET6.SA', 'EMBR3.SA', 'ENGI11.SA', 'ENEV3.SA', 'EGIE3.SA', 'EQTL3.SA', 'EZTC3.SA', 'FLRY3.SA', 'GGBR4.SA', 'GOAU4.SA', 'GGPS3.SA', 'GMAT3.SA', 'HAPV3.SA', 'HYPE3.SA', 'IGTI11.SA', 'INTB3.SA', 'IRBR3.SA', 'ISAE4.SA', 'ITSA4.SA', 'ITUB4.SA', 'KLBN11.SA', 'RENT3.SA', 'LREN3.SA', 'LWSA3.SA', 'MGLU3.SA', 'POMO4.SA', 'MBRF3.SA', 'BEEF3.SA', 'MOTV3.SA', 'MOVI3.SA', 'MRVE3.SA', 'MULT3.SA', 'NATU3.SA', 'PCAR3.SA', 'PETR3.SA', 'PETR4.SA', 'RECV3.SA', 'PRIO3.SA', 'PETZ3.SA', 'PSSA3.SA', 'RADL3.SA', 'RAIZ4.SA', 'RAPT4.SA', 'RDOR3.SA', 'RAIL3.SA', 'SBSP3.SA', 'SAPR11.SA', 'SANB11.SA', 'SMTO3.SA', 'SRNA3.SA', 'CSNA3.SA', 'SLCE3.SA', 'SMFT3.SA', 'SUZB3.SA', 'TAEE11.SA', 'VIVT3.SA', 'TEND3.SA', 'TIMS3.SA', 'TOTS3.SA', 'UGPA3.SA', 'USIM5.SA', 'VALE3.SA', 'VAMO3.SA', 'VBBR3.SA', 'VIVA3.SA', 'WEGE3.SA', 'YDUQ3.SA']\n"
     ]
    }
   ],
   "source": [
    "# -------------- Orquestração --------------\n",
    "def get_all_ibrx100_tickers(iframe_src=\"https://sistemaswebb3-listados.b3.com.br/indexPage/day/IBXX?language=en-us\", try_index_codes=None, use_selenium_fallback=True, debug=False):\n",
    "    \"\"\"\n",
    "    Tenta baixar CSV direto; se não, tenta requests/parsing; se não, usa Selenium para paginação.\n",
    "    try_index_codes: lista de strings para substituir 'IBXX' placeholder (ex: ['IBRX100'])\n",
    "    \"\"\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    urls_to_try = [iframe_src]\n",
    "    if try_index_codes:\n",
    "        for code in try_index_codes:\n",
    "            urls_to_try.append(iframe_src.replace(\"IBXX\", code))\n",
    "    for url in urls_to_try:\n",
    "        if debug: print(\"[debug] tentando URL:\", url)\n",
    "        try:\n",
    "            tickers = try_download_csv_from_iframe(url, headers=headers)\n",
    "            if tickers:\n",
    "                print(\"✅ Extraído via download/parsing direto. Tickers:\", len(tickers))\n",
    "                return tickers\n",
    "        except Exception as e:\n",
    "            if debug: print(\"requests parse falhou:\", e)\n",
    "            pass\n",
    "\n",
    "    # 2) fallback: Selenium\n",
    "    if use_selenium_fallback:\n",
    "        for url in urls_to_try:\n",
    "            try:\n",
    "                if debug: print(\"[debug] Selenium tentando:\", url)\n",
    "                res = selenium_extract_all_from_iframe(url, headless=not debug, wait_seconds=12, debug=debug)\n",
    "                if res:\n",
    "                    print(\"✅ Extraído via Selenium. Tickers:\", len(res))\n",
    "                    return res\n",
    "            except Exception as e:\n",
    "                if debug: print(\"Selenium tentativa falhou:\", e)\n",
    "                continue\n",
    "\n",
    "    print(\"⚠️ Não foi possível extrair tickers por nenhum método.\")\n",
    "    return []\n",
    "\n",
    "# USO\n",
    "if __name__ == \"__main__\":\n",
    "    # passe try_index_codes para substituir IBXX por IBRX100 se necessário\n",
    "    tickers = get_all_ibrx100_tickers(\n",
    "        iframe_src=\"https://sistemaswebb3-listados.b3.com.br/indexPage/day/IBXX?language=en-us\",\n",
    "        try_index_codes=[\"IBRX100\",\"IBRX-100\"],\n",
    "        use_selenium_fallback=True,\n",
    "        debug=True  # define False para headless e menos logs\n",
    "    )\n",
    "    print(\"TOTAL TICKERS:\", len(tickers))\n",
    "    print(tickers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_aurum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
