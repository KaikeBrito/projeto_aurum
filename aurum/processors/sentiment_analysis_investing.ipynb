{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59180b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Tentando conectar em: https://br.investing.com/equities/petrobras-pn-news/1\n",
      "üì° Status Code: 200\n",
      "üìÑ T√≠tulo da P√°gina: Petrobras PN Not√≠cias (PETR4) - Investing.com\n",
      "‚õî ALERTA: O Cloudflare capturou a requisi√ß√£o (Bloqueio Anti-Rob√¥).\n"
     ]
    }
   ],
   "source": [
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def teste_conexao():\n",
    "    url = \"https://br.investing.com/equities/petrobras-pn-news/1\"\n",
    "    print(f\"üîå Tentando conectar em: {url}\")\n",
    "    \n",
    "    # Cria o scraper (simula um navegador Chrome)\n",
    "    scraper = cloudscraper.create_scraper()\n",
    "    \n",
    "    try:\n",
    "        resp = scraper.get(url)\n",
    "        print(f\"üì° Status Code: {resp.status_code}\")\n",
    "        \n",
    "        if resp.status_code != 200:\n",
    "            print(\"‚ùå Bloqueio detectado pelo c√≥digo de status.\")\n",
    "            return\n",
    "\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        \n",
    "        # Teste 1: Verifica T√≠tulo da P√°gina\n",
    "        print(f\"üìÑ T√≠tulo da P√°gina: {soup.title.string if soup.title else 'Sem T√≠tulo'}\")\n",
    "        \n",
    "        # Teste 2: Verifica se √© p√°gina de bloqueio\n",
    "        texto_pagina = resp.text.lower()\n",
    "        if \"just a moment\" in texto_pagina or \"access denied\" in texto_pagina or \"cloudflare\" in texto_pagina:\n",
    "            print(\"‚õî ALERTA: O Cloudflare capturou a requisi√ß√£o (Bloqueio Anti-Rob√¥).\")\n",
    "            return\n",
    "            \n",
    "        # Teste 3: Testa Seletores de Not√≠cia\n",
    "        print(\"\\nüïµÔ∏è Testando Seletores CSS:\")\n",
    "        \n",
    "        selectors = [\n",
    "            'article.articleItem',       # Antigo padr√£o\n",
    "            'div.textDiv',               # Alternativo antigo\n",
    "            'ul[data-test=\"news-list\"]', # Novo layout poss√≠vel\n",
    "            'div.news-analysis-v2_content__z0_23', # Layout 2024\n",
    "            'li.border-b'                # Layout gen√©rico lista\n",
    "        ]\n",
    "        \n",
    "        encontrou_algo = False\n",
    "        for sel in selectors:\n",
    "            items = soup.select(sel)\n",
    "            print(f\"   -> Seletor '{sel}': Encontrou {len(items)} elementos.\")\n",
    "            if len(items) > 0:\n",
    "                encontrou_algo = True\n",
    "                print(f\"      Exemplo: {items[0].text[:50].strip()}...\")\n",
    "        \n",
    "        if not encontrou_algo:\n",
    "            print(\"\\n‚ùå NENHUM SELETOR FUNCIONOU. O site mudou o HTML ou n√£o carregou as not√≠cias.\")\n",
    "            # Salva o HTML para voc√™ ver o que veio\n",
    "            with open(\"debug_pagina_investing.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(resp.text)\n",
    "            print(\"üíæ Salvei o HTML recebido em 'debug_pagina_investing.html'. Abra no navegador para ver o que o rob√¥ est√° vendo.\")\n",
    "        else:\n",
    "            print(\"\\n‚úÖ SUCESSO! Encontramos not√≠cias. O problema era apenas o seletor.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"üî• Erro fatal na conex√£o: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    teste_conexao()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65f88f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïµÔ∏è Investigando layout de: https://br.investing.com/equities/petrobras-pn-news/1\n",
      "üìÑ T√≠tulo confirmado: Petrobras PN Not√≠cias (PETR4) - Investing.com\n",
      "\n",
      "--- üîç TENTANDO ENCONTRAR NOT√çCIAS PELO LINK ---\n",
      "Encontrados 11 links potenciais de not√≠cia.\n",
      "\n",
      "‚úÖ ESTRUTURA DETECTADA (Primeiro item):\n",
      "   Manchete: Recomenda√ß√µes de analistas\n",
      "   Link: https://br.investing.com/news/analyst-ratings\n",
      "   Tag Pai: <li class='['flex', 'items-center', 'navbar_multi_list_list__BidbT']'>\n",
      "   Tag Av√¥: <ul class='SEM_CLASSE'>\n",
      "   Tag Bisav√¥: <div class='SEM_CLASSE'>\n",
      "\n",
      "üí° DICA PARA MIM (IA): Use as classes acima para consertar o scraper.\n"
     ]
    }
   ],
   "source": [
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def investigar_layout():\n",
    "    url = \"https://br.investing.com/equities/petrobras-pn-news/1\"\n",
    "    print(f\"üïµÔ∏è Investigando layout de: {url}\")\n",
    "    \n",
    "    scraper = cloudscraper.create_scraper()\n",
    "    \n",
    "    try:\n",
    "        resp = scraper.get(url)\n",
    "        if resp.status_code != 200:\n",
    "            print(f\"‚ùå Erro de status: {resp.status_code}\")\n",
    "            return\n",
    "\n",
    "        soup = BeautifulSoup(resp.text, 'html.parser')\n",
    "        print(f\"üìÑ T√≠tulo confirmado: {soup.title.string.strip()}\")\n",
    "        \n",
    "        # ESTRAT√âGIA: Encontrar qualquer link que pare√ßa uma not√≠cia\n",
    "        # Links de not√≠cia no investing geralmente come√ßam com /news/\n",
    "        print(\"\\n--- üîç TENTANDO ENCONTRAR NOT√çCIAS PELO LINK ---\")\n",
    "        \n",
    "        links_candidatos = []\n",
    "        all_links = soup.find_all('a', href=True)\n",
    "        \n",
    "        for link in all_links:\n",
    "            href = link['href']\n",
    "            text = link.text.strip()\n",
    "            \n",
    "            # Filtros para achar not√≠cias reais e ignorar menus\n",
    "            if '/news/' in href and len(text) > 20 and 'coment√°rios' not in text.lower():\n",
    "                links_candidatos.append(link)\n",
    "\n",
    "        print(f\"Encontrados {len(links_candidatos)} links potenciais de not√≠cia.\")\n",
    "        \n",
    "        if len(links_candidatos) > 0:\n",
    "            print(\"\\n‚úÖ ESTRUTURA DETECTADA (Primeiro item):\")\n",
    "            exemplo = links_candidatos[0]\n",
    "            print(f\"   Manchete: {exemplo.text}\")\n",
    "            print(f\"   Link: {exemplo['href']}\")\n",
    "            \n",
    "            # Descobrir a classe do PAI (o container da not√≠cia)\n",
    "            pai = exemplo.parent\n",
    "            print(f\"   Tag Pai: <{pai.name} class='{pai.get('class') if pai.get('class') else 'SEM_CLASSE'}'>\")\n",
    "            \n",
    "            avo = pai.parent\n",
    "            if avo:\n",
    "                print(f\"   Tag Av√¥: <{avo.name} class='{avo.get('class') if avo.get('class') else 'SEM_CLASSE'}'>\")\n",
    "                \n",
    "            bisavo = avo.parent\n",
    "            if bisavo:\n",
    "                print(f\"   Tag Bisav√¥: <{bisavo.name} class='{bisavo.get('class') if bisavo.get('class') else 'SEM_CLASSE'}'>\")\n",
    "                \n",
    "            print(\"\\nüí° DICA PARA MIM (IA): Use as classes acima para consertar o scraper.\")\n",
    "        else:\n",
    "            print(\"‚ùå Ainda n√£o achei nada. O HTML pode estar sendo renderizado via Javascript.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Erro: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    investigar_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73d522a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5fa1021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- CONFIGURA√á√ÉO ---\n",
    "INPUT_DIR = \"../data/news/investing\"\n",
    "OUTPUT_DIR = \"../data/news/processed/investing\"\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a548fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_data_json_sujo(raw_date):\n",
    "    \"\"\"\n",
    "    Limpa datas que vieram sujas do JSON Mode.\n",
    "    Entrada: '2025-12-20 01:31:03\",\"writerId\":0...'\n",
    "    Sa√≠da: datetime(2025, 12, 20)\n",
    "    \"\"\"\n",
    "    if not isinstance(raw_date, str): return pd.NaT\n",
    "    \n",
    "    texto = raw_date.strip()\n",
    "    \n",
    "    try:\n",
    "        # Padr√£o ISO (YYYY-MM-DD) - Comum no JSON\n",
    "        # Procura 4 digitos - 2 digitos - 2 digitos logo no come√ßo ou meio\n",
    "        match_iso = re.search(r'(\\d{4})-(\\d{2})-(\\d{2})', texto)\n",
    "        if match_iso:\n",
    "            return datetime.strptime(match_iso.group(0), '%Y-%m-%d')\n",
    "\n",
    "        # Padr√£o Brasileiro com Ponto (DD.MM.YYYY)\n",
    "        match_pontos = re.search(r'(\\d{2})\\.(\\d{2})\\.(\\d{4})', texto)\n",
    "        if match_pontos:\n",
    "            return datetime.strptime(match_pontos.group(0), '%d.%m.%Y')\n",
    "            \n",
    "        # Padr√£o Brasileiro com Barra (DD/MM/YYYY)\n",
    "        match_barras = re.search(r'(\\d{2})/(\\d{2})/(\\d{4})', texto)\n",
    "        if match_barras:\n",
    "            return datetime.strptime(match_barras.group(0), '%d/%m/%Y')\n",
    "            \n",
    "        # Relativos (\"5 horas atr√°s\") -> Assume Hoje\n",
    "        texto_lower = texto.lower()\n",
    "        if 'atr√°s' in texto_lower or 'ago' in texto_lower or 'min' in texto_lower:\n",
    "            return datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return pd.NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb33e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_vader(texto):\n",
    "    if not isinstance(texto, str): return 0.0\n",
    "    return analyzer.polarity_scores(texto)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fa006fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üßπ INICIANDO SANEAMENTO INVESTING (V4 - FIX JSON SUJO) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Limpando: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23/23 [00:01<00:00, 18.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "üìä RESUMO FINAL:\n",
      "‚úÖ Arquivos salvos em 'processed': 23\n",
      "‚ö†Ô∏è Arquivos ignorados (vazios): 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def processar_investing_v4():\n",
    "    print(\"--- üßπ INICIANDO SANEAMENTO INVESTING (V4 - FIX JSON SUJO) ---\")\n",
    "    arquivos = glob.glob(os.path.join(INPUT_DIR, \"*.parquet\"))\n",
    "    \n",
    "    count_sucesso = 0\n",
    "    count_vazios = 0\n",
    "    \n",
    "    for arquivo in tqdm(arquivos, desc=\"Limpando\"):\n",
    "        nome_arq = os.path.basename(arquivo)\n",
    "        try:\n",
    "            df = pd.read_parquet(arquivo)\n",
    "            \n",
    "            if df.empty or 'raw_date' not in df.columns:\n",
    "                count_vazios += 1\n",
    "                continue\n",
    "\n",
    "            # 1. Limpeza Cir√∫rgica da Data\n",
    "            df['date'] = df['raw_date'].apply(limpar_data_json_sujo)\n",
    "            \n",
    "            # Remove quem ficou sem data\n",
    "            df_limpo = df.dropna(subset=['date']).copy()\n",
    "            \n",
    "            if df_limpo.empty:\n",
    "                # print(f\"‚ö†Ô∏è {nome_arq}: Todas as datas falharam. Ex: {df['raw_date'].iloc[0][:50]}...\")\n",
    "                count_vazios += 1\n",
    "                continue\n",
    "\n",
    "            # 2. Sentimento\n",
    "            df_limpo['sentiment'] = df_limpo['title'].apply(calcular_vader)\n",
    "\n",
    "            # 3. Sele√ß√£o de Colunas\n",
    "            cols_finais = ['ticker', 'date', 'title', 'sentiment', 'link']\n",
    "            # Cria colunas faltantes se precisar\n",
    "            for col in cols_finais:\n",
    "                if col not in df_limpo.columns: df_limpo[col] = None\n",
    "            \n",
    "            df_limpo = df_limpo[cols_finais]\n",
    "\n",
    "            # 4. Salva (Agora vai funcionar!)\n",
    "            df_limpo.to_parquet(os.path.join(OUTPUT_DIR, nome_arq), index=False)\n",
    "            count_sucesso += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro em {nome_arq}: {e}\")\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"üìä RESUMO FINAL:\")\n",
    "    print(f\"‚úÖ Arquivos salvos em 'processed': {count_sucesso}\")\n",
    "    print(f\"‚ö†Ô∏è Arquivos ignorados (vazios): {count_vazios}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processar_investing_v4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d16dfbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Total de arquivos brutos: 23\n",
      "\n",
      "--- üìÑ news_investing_ABEV3.parquet ---\n",
      "Linhas: 1\n",
      "Colunas: ['title', 'link', 'raw_date', 'source', 'ticker', 'slug']\n",
      "üîé Amostra de Datas Brutas:\n",
      "['2025-12-20 01:31:03\",\"writerId\":0,\"front_writer_name\":\"\",\"front_member_name\":\"\",\"front_writer_link\":\"\",\"writerName\":\"\",\"writerLink\":\"\",\"writerImage\":\"\",\"userFirstName\":\"\",\"userLastName\":\"\",\"writerArticleCount\":0,\"company_name\":\"\",\"company_name_prepared\":\"\",\"company_href\":\"\",\"company_url\":\"\",\"user_type\":\"\",\"authorLink\":\"\",\"editorId\":0,\"editorFirstName\":\"\",\"editorLastName\":\"\",\"editorLink\":\"\",\"image_copyright\":\"\\\\u0026copy; REUTERS\",\"related_image_big\":\"TheNewYorkStockExchange_800x533_L_1646233225.jpg\",\"images_serialized\":\"a:1:{i:0;a:2:{s:3:\\\\\"src\\\\\";s:48:\\\\\"TheNewYorkStockExchange_800x533_L_1646233225.jpg\\\\\";s:5:\\\\\"width\\\\\";i:200;}}\",\"imageHref\":\"https://i-invdn-com.investing.com/news/TheNewYorkStockExchange_800x533_L_1646233225.jpg']\n",
      "\n",
      "--- üìÑ news_investing_AZZA3.parquet ---\n",
      "Linhas: 1\n",
      "Colunas: ['title', 'link', 'raw_date', 'source', 'ticker', 'slug']\n",
      "üîé Amostra de Datas Brutas:\n",
      "['2025-12-17 15:53:13\",\"image\":\"\",\"user\":{\"userID\":\"234996476\",\"userFirstName\":\"Maria\",\"userLastName\":\"Niza\",\"userImage\":\"https://i-invdn-com.investing.com/defaultUserMaleTmp.png\",\"memberProfileHref\":\"/members/234996476\",\"shownName\":\"Maria Niza\",\"nickName\":\"']\n",
      "\n",
      "--- üìÑ news_investing_B3SA3.parquet ---\n",
      "Linhas: 1\n",
      "Colunas: ['title', 'link', 'raw_date', 'source', 'ticker', 'slug']\n",
      "üîé Amostra de Datas Brutas:\n",
      "['2025-12-05 18:06:44\",\"image\":\"\",\"user\":{\"userID\":\"246924869\",\"userFirstName\":\"Giulius\",\"userLastName\":\"Barganha\",\"userImage\":\"https://d30-invdn-com.investing.com/company_logo/39ecc76c10ee06600ff2067db22cd3d3.jpg\",\"memberProfileHref\":\"/members/246924869\",\"shownName\":\"Giulius Barganha\",\"nickName\":\"Xdaquestao']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "PATH_RAW = \"../data/news/investing\"\n",
    "\n",
    "def diagnosticar():\n",
    "    arquivos = glob.glob(os.path.join(PATH_RAW, \"*.parquet\"))\n",
    "    \n",
    "    if not arquivos:\n",
    "        print(\"‚ùå Nenhum arquivo encontrado na pasta raw.\")\n",
    "        return\n",
    "\n",
    "    print(f\"üìÇ Total de arquivos brutos: {len(arquivos)}\")\n",
    "    \n",
    "    # Pega os 3 primeiros arquivos n√£o vazios para garantir\n",
    "    count = 0\n",
    "    for arq in arquivos:\n",
    "        if count >= 3: break\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_parquet(arq)\n",
    "            if df.empty:\n",
    "                print(f\"‚ö†Ô∏è {os.path.basename(arq)} est√° VAZIO.\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\n--- üìÑ {os.path.basename(arq)} ---\")\n",
    "            print(f\"Linhas: {len(df)}\")\n",
    "            print(\"Colunas:\", list(df.columns))\n",
    "            \n",
    "            if 'raw_date' in df.columns:\n",
    "                print(\"üîé Amostra de Datas Brutas:\")\n",
    "                print(df['raw_date'].head(10).tolist())\n",
    "            else:\n",
    "                print(\"‚ùå ERRO CR√çTICO: Coluna 'raw_date' n√£o existe!\")\n",
    "            \n",
    "            count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler {arq}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    diagnosticar()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
