{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed82eddd",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Documentação**: Script de Coleta de Notícias (Google News RSS)\n",
    "\n",
    "#### **1. Objetivo**\n",
    "\n",
    "Este script implementa parte do Pilar 2 (Qualidade da Comunicação) do projeto Aurum. Sua responsabilidade é automatizar a coleta de notícias financeiras recentes para cada empresa (ticker) listada no índice IBRX-100.\n",
    "\n",
    "O script utiliza o *feed RSS do Google News* como fonte de dados, buscando menções a cada ticker nos últimos 30 dias. Os dados brutos coletados são a base para a futura análise de sentimento (NLP).\n",
    "\n",
    "#### **2. Configuração (Input)**\n",
    "\n",
    "O script depende de um único arquivo de entrada:\n",
    "\n",
    "* `tickers_ibrx100_full.csv`: Um arquivo CSV que deve conter a lista completa de tickers do IBRX-100.\n",
    "    * Formato esperado: O script lê a **primeira coluna** deste arquivo. Os tickers podem estar no formato `PETR4.SA` ou `PETR4`. A função `load_tickers_from_csv` remove automaticamente o sufixo `.SA` para otimizar a busca no Google News.\n",
    "\n",
    "#### **3. Saída (Output)**\n",
    "\n",
    "O script gera dois arquivos idênticos em conteúdo, localizados em `data/news/`:\n",
    "\n",
    "1.  `raw_news_data.parquet`\n",
    "2.  `raw_news_data.csv`\n",
    "\n",
    "O schema (colunas) do DataFrame salvo é:\n",
    "\n",
    "| Coluna | Tipo | Descrição |\n",
    "| :--- | :--- | :--- |\n",
    "| `ticker_query` | string | O ticker usado na busca (ex: `PETR4`). |\n",
    "| `title` | string | O título da notícia. |\n",
    "| `link` | string | O link original da notícia. |\n",
    "| `published_date` | datetime | A data e hora da publicação (já convertida). |\n",
    "| `source` | string | O nome do veículo de mídia (ex: \"InfoMoney\"). |\n",
    "| `summary` | string | Um pequeno resumo ou *snippet* da notícia (HTML). |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ff0cee",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport time\nimport urllib.parse\n\nimport feedparser\nimport pandas as pd\nimport tqdm\n\n# Diretório para salvar os dados (pasta data no nível do aurum, não data_sources)\nDATA_DIR = os.path.join(\"..\", \"data\")\nNEWS_DIR = os.path.join(DATA_DIR, \"news\")\nos.makedirs(NEWS_DIR, exist_ok=True)\n\ndef load_tickers_from_csv(file_path: str) -> list:\n    \"\"\"Carrega a lista de tickers a partir de um arquivo CSV.\"\"\"\n    df = pd.read_csv(file_path)\n    tickers = df.iloc[:, 0].dropna().astype(str).tolist()\n    # Remove o sufixo '.SA' para usar na busca de notícias\n    return [t.replace('.SA', '') for t in tickers]\n\ndef fetch_news_for_ticker(ticker: str):\n    \"\"\"Busca notícias para um ticker específico usando o RSS do Google News.\"\"\"\n    raw_query = f'\"{ticker}\" when:30d'\n    search_query = urllib.parse.quote(raw_query)\n    url = f\"https://news.google.com/rss/search?q={search_query}&hl=pt-BR&gl=BR&ceid=BR:pt-419\"\n    \n    feed = feedparser.parse(url)\n    \n    news_items = []\n    for entry in feed.entries:\n        news_items.append({\n            'ticker_query': ticker,\n            'title': entry.title,\n            'link': entry.link,\n            'published_date': entry.published,\n            'source': entry.source.title if hasattr(entry, 'source') else 'Unknown',\n            'summary': entry.summary if hasattr(entry, 'summary') else ''\n        })\n    return news_items\n\n\nif __name__ == \"__main__\":\n    # Caminho para o arquivo de tickers na pasta data\n    tickers_csv_path = os.path.join(DATA_DIR, \"tickers_ibrx100_full.csv\")\n    tickers = load_tickers_from_csv(tickers_csv_path)\n    \n    all_news = []\n    \n    print(\"Iniciando a coleta de notícias via Google News RSS...\")\n    for ticker in tqdm.tqdm(tickers, desc=\"Buscando notícias\"):\n        try:\n            news = fetch_news_for_ticker(ticker)\n            if news:\n                all_news.extend(news)\n            time.sleep(0.5)\n        except Exception as e:\n            print(f\"Erro ao buscar notícias para {ticker}: {e}\")\n\n    if not all_news:\n        print(\"\\nNenhuma notícia foi coletada. Verifique a conexão ou a consulta de busca. Encerrando.\")\n    else:\n        df_news = pd.DataFrame(all_news)\n        df_news.drop_duplicates(subset=['title', 'link'], inplace=True)\n        df_news['published_date'] = pd.to_datetime(df_news['published_date'], errors='coerce')\n        \n        # --- SALVANDO ARQUIVOS ---\n        output_path_parquet = os.path.join(NEWS_DIR, \"raw_news_data.parquet\")\n        output_path_csv = os.path.join(NEWS_DIR, \"raw_news_data.csv\")\n\n        # Salva em Parquet\n        df_news.to_parquet(output_path_parquet, index=False)\n        \n        # Salva em CSV\n        df_news.to_csv(output_path_csv, index=False)\n        \n        print(f\"\\nColeta concluída. {len(df_news)} notícias únicas salvas.\")\n        print(f\"-> {output_path_parquet}\")\n        print(f\"-> {output_path_csv}\")\n        print(\"\\nAmostra das notícias coletadas:\")\n        print(df_news.head())\n"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}