{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a68349e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### üóÇÔ∏è Hist√≥rico ‚Äî downloader & combinador (b3_hist_downloader.py)\n",
    "\n",
    "**Resumo r√°pido**  \n",
    "Script que baixa hist√≥ricos de pre√ßos (via `yfinance`), salva por ticker em **parquet** (e opcionalmente CSV), e permite combinar todos os parquets em um √∫nico arquivo *long-format*. Feito para rodar em batch com retry/backoff, pular tickers j√° salvos e gerar um resumo final.\n",
    "\n",
    "\n",
    "#### ‚ú® Principais responsabilidades\n",
    "- Baixar hist√≥ricos de pre√ßo por lotes (batch) com `yfinance`.  \n",
    "- Salvar cada ticker em `data/historical/{TICKER}.parquet` e `{TICKER}.csv`.  \n",
    "- Gerenciar retries (exponencial), fallback ticker-a-ticker e logs.  \n",
    "- Gerar CSV de resumo por execu√ß√£o (`download_summary_YYYYMMDDTHHMMSSZ.csv`).  \n",
    "- Recombinar todos os parquets em um √∫nico `all_histories.parquet` / `all_histories.csv`.\n",
    "\n",
    "\n",
    "#### üß© Fun√ß√µes principais (one-liners)\n",
    "- `ticker_exists_local(ticker)` ‚Üí verifica exist√™ncia de `{ticker}.parquet`.  \n",
    "- `save_history_df(ticker, df, save_csv=True)` ‚Üí salva parquet e CSV; garante coluna `date`.  \n",
    "- `download_batch(batch, start, threads)` ‚Üí tenta baixar um batch via `yfinance.download` com retries e fallback.  \n",
    "- `download_all_histories(tickers, start, force, save_summary, save_csv_per_ticker)` ‚Üí orquestra o download em batches, salva e retorna um `DataFrame` resumo.  \n",
    "- `combine_all_to_single_parquet(out_path, out_csv, tickers)` ‚Üí concatena todos os parquets em formato long e salva.\n",
    "\n",
    "\n",
    "#### ‚öôÔ∏è Par√¢metros principais (valores padr√£o)\n",
    "| Par√¢metro | Valor padr√£o |\n",
    "|---:|:---|\n",
    "| `HIST_DIR` | `data/historical/` |\n",
    "| `DEFAULT_START` | `\"2011-01-01\"` |\n",
    "| `BATCH_SIZE` | `15` |\n",
    "| `MAX_ATTEMPTS` | `4` |\n",
    "| `SLEEP_BETWEEN_BATCHES` | `1` (seg) |\n",
    "| `SLEEP_BETWEEN_TICKERS` | `0.2` (seg) |\n",
    "\n",
    "\n",
    "#### üìÇ Sa√≠das geradas\n",
    "- `data/historical/{TICKER}.parquet` ‚Äî parquet por ticker.  \n",
    "- `data/historical/{TICKER}.csv` ‚Äî  CSV por ticker.  \n",
    "- `data/historical/download_summary_{ts}.csv` ‚Äî resumo da execu√ß√£o.  \n",
    "- `data/historical/all_histories.parquet` & `all_histories.csv` ‚Äî concat final (long format).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffeaf9f",
   "metadata": {},
   "outputs": [],
   "source": "# logging simples\nimport datetime\nimport logging\nimport os\nimport time\n\nimport pandas as pd\n\nfrom typing import Dict, Iterable, List, Optional, Set\n\nimport yfinance as yf\n\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n\n# diret√≥rio onde os hist√≥ricos ser√£o salvos (pasta data no n√≠vel do aurum, n√£o data_sources)\nHIST_DIR = os.path.join(\"..\", \"data\", \"historical\")\nos.makedirs(HIST_DIR, exist_ok=True)\n\n# par√¢metros de download\nDEFAULT_START = \"2011-01-01\"\nBATCH_SIZE = 15\nMAX_ATTEMPTS = 4          \nSLEEP_BETWEEN_BATCHES = 1  \nSLEEP_BETWEEN_TICKERS = 0.2\n\ndef ticker_exists_local(ticker: str) -> bool:\n    \"\"\"Verifica se j√° existe parquet salvo para ticker (usa para pular downloads)\"\"\"\n    path = os.path.join(HIST_DIR, f\"{ticker}.parquet\")\n    return os.path.isfile(path)\n\ndef save_history_df(ticker: str, df: pd.DataFrame, save_csv: bool = True):\n    \"\"\"Salva DataFrame em parquet e opcionalmente em CSV. Garante coluna 'date' se √≠ndice for DatetimeIndex.\"\"\"\n    if df is None or df.empty:\n        raise ValueError(\"DataFrame nulo ou vazio\")\n    df = df.copy()\n    # garantir que a coluna de data exista como coluna\n    if isinstance(df.index, pd.DatetimeIndex):\n        df.index.name = \"date\"\n        df = df.reset_index()\n    # converter coluna date para string ISO ao salvar CSV (mant√©m compatibilidade)\n    out_parquet = os.path.join(HIST_DIR, f\"{ticker}.parquet\")\n    out_csv = os.path.join(HIST_DIR, f\"{ticker}.csv\")\n    try:\n        df.to_parquet(out_parquet, index=False)\n        logging.info(\"Saved %s rows for %s -> %s\", len(df), ticker, out_parquet)\n    except Exception as e:\n        logging.exception(\"Erro salvando parquet para %s: %s\", ticker, e)\n        raise\n    if save_csv:\n        try:\n            # padronizar data para ISO antes de salvar CSV (se existir)\n            if 'date' in df.columns:\n                df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')\n            df.to_csv(out_csv, index=False)\n            logging.info(\"Saved CSV for %s -> %s\", ticker, out_csv)\n        except Exception as e:\n            logging.exception(\"Erro salvando CSV para %s: %s\", ticker, e)\n            # n√£o raise ‚Äî parquet j√° salvo, apenas logamos o problema\n\ndef download_batch(batch: List[str], start: str = DEFAULT_START, threads: bool = True) -> Dict[str, Optional[pd.DataFrame]]:\n    \"\"\"\n    Tenta baixar um batch de tickers via yfinance.download.\n    Retorna dict ticker -> DataFrame or None (se falhou).\n    \"\"\"\n    joined = \" \".join(batch)\n    attempt = 0\n    last_exc = None\n    while attempt < MAX_ATTEMPTS:\n        try:\n            logging.info(\"yfinance.download attempt %d for batch size %d\", attempt+1, len(batch))\n            data = yf.download(tickers=joined, start=start, progress=False, threads=threads, group_by='ticker', auto_adjust=False, actions=True)\n            result = {}\n            if isinstance(data, pd.DataFrame) and isinstance(data.columns, pd.MultiIndex):\n                for ticker in batch:\n                    if ticker in data.columns.get_level_values(0):\n                        df_t = data[ticker].copy()\n                        result[ticker] = df_t\n                    else:\n                        try:\n                            single = yf.download(ticker, start=start, progress=False, actions=True)\n                            result[ticker] = single if not single.empty else None\n                        except Exception:\n                            result[ticker] = None\n            else:\n                for ticker in batch:\n                    try:\n                        df_t = yf.download(ticker, start=start, progress=False, actions=True)\n                        result[ticker] = df_t if not df_t.empty else None\n                    except Exception:\n                        result[ticker] = None\n            return result\n        except Exception as e:\n            last_exc = e\n            logging.warning(\"Erro no yfinance.download (attempt %d): %s\", attempt+1, str(e))\n            attempt += 1\n            time.sleep(2 ** attempt)  # backoff exponencial\n    logging.error(\"Todas tentativas falharam para batch (%s). √öltimo erro: %s\", joined, last_exc)\n    # fallback: tentar baixar ticker a ticker\n    result = {}\n    for ticker in batch:\n        try:\n            df_t = yf.download(ticker, start=start, progress=False, actions=True)\n            result[ticker] = df_t if not df_t.empty else None\n        except Exception as e:\n            logging.warning(\"Fallback individual falhou para %s: %s\", ticker, e)\n            result[ticker] = None\n    return result\n\ndef download_all_histories(tickers: List[str], start: str = DEFAULT_START, force: bool = False, save_summary: bool = True, save_csv_per_ticker: bool = True):\n    \"\"\"\n    Processo principal: recebe lista de tickers (strings), baixa hist√≥ricos e salva parquet + csv por ticker.\n    - force: se True, re-baixa mesmo que arquivo exista.\n    - save_csv_per_ticker: se True salva um CSV para cada ticker (al√©m do parquet).\n    - retorna um DataFrame resumo com status por ticker.\n    \"\"\"\n    os.makedirs(HIST_DIR, exist_ok=True)\n    tickers = [t for t in tickers if isinstance(t, str) and t.strip()]\n    tickers = list(dict.fromkeys(tickers))\n    summary = []\n    for i in range(0, len(tickers), BATCH_SIZE):\n        batch = tickers[i:i+BATCH_SIZE]\n        to_download = [t for t in batch if force or not ticker_exists_local(t)]\n        if not to_download:\n            logging.info(\"Batch %d: todos j√° existem localmente ‚Äî pulando.\", i//BATCH_SIZE+1)\n            for t in batch:\n                summary.append({\n                    \"ticker\": t,\n                    \"status\": \"skipped_local\",\n                    \"rows\": None,\n                    \"saved_parquet\": os.path.join(HIST_DIR, f\"{t}.parquet\") if ticker_exists_local(t) else None,\n                    \"saved_csv\": os.path.join(HIST_DIR, f\"{t}.csv\") if os.path.exists(os.path.join(HIST_DIR, f\"{t}.csv\")) else None\n                })\n            continue\n\n        logging.info(\"Processando batch %d/%d (download %d/%d)\", i//BATCH_SIZE+1, (len(tickers)+BATCH_SIZE-1)//BATCH_SIZE, len(to_download), len(batch))\n        results = download_batch(to_download, start=start)\n        for t in batch:\n            df_t = results.get(t) if t in results else None\n            if df_t is None or (isinstance(df_t, pd.DataFrame) and df_t.empty):\n                logging.warning(\"Nenhum dado para %s em batch; tentativa isolada...\", t)\n                try:\n                    single = yf.download(t, start=start, progress=False, actions=True)\n                    df_t = single if not single.empty else None\n                except Exception:\n                    df_t = None\n            if df_t is None or df_t.empty:\n                logging.error(\"Falha obtendo dados para %s\", t)\n                summary.append({\"ticker\": t, \"status\": \"failed\", \"rows\": 0, \"saved_parquet\": None, \"saved_csv\": None})\n            else:\n                try:\n                    save_history_df(t, df_t, save_csv=save_csv_per_ticker)\n                    summary.append({\n                        \"ticker\": t,\n                        \"status\": \"ok\",\n                        \"rows\": len(df_t),\n                        \"saved_parquet\": os.path.join(HIST_DIR, f\"{t}.parquet\"),\n                        \"saved_csv\": os.path.join(HIST_DIR, f\"{t}.csv\") if save_csv_per_ticker else None\n                    })\n                except Exception as e:\n                    logging.exception(\"Erro salvando para %s: %s\", t, e)\n                    summary.append({\"ticker\": t, \"status\": \"save_error\", \"rows\": len(df_t) if isinstance(df_t, pd.DataFrame) else None, \"saved_parquet\": None, \"saved_csv\": None})\n            time.sleep(SLEEP_BETWEEN_TICKERS)\n        time.sleep(SLEEP_BETWEEN_BATCHES)\n\n    df_summary = pd.DataFrame(summary)\n    if save_summary:\n        ts = datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n        summary_path = os.path.join(HIST_DIR, f\"download_summary_{ts}.csv\")\n        df_summary.to_csv(summary_path, index=False)\n        logging.info(\"Resumo salvo em %s\", summary_path)\n    return df_summary\n\ndef combine_all_to_single_parquet(out_path: str = os.path.join(HIST_DIR, \"all_histories.parquet\"), out_csv: Optional[str] = os.path.join(HIST_DIR, \"all_histories.csv\"), tickers: Optional[List[str]] = None):\n    \"\"\"\n    L√™ todos os parquets em HIST_DIR (ou tickers list) e concatena em formato long:\n    columns: ['ticker','date', 'Open','High','Low','Close','Adj Close','Volume', 'Dividends','Stock Splits']\n    Salva em parquet e opcionalmente em csv.\n    \"\"\"\n    files = []\n    if tickers:\n        files = [os.path.join(HIST_DIR, f\"{t}.parquet\") for t in tickers if os.path.exists(os.path.join(HIST_DIR, f\"{t}.parquet\"))]\n    else:\n        files = [os.path.join(HIST_DIR, f) for f in os.listdir(HIST_DIR) if f.endswith(\".parquet\")]\n    dfs = []\n    for f in files:\n        try:\n            df = pd.read_parquet(f)\n            if 'date' in df.columns:\n                df['date'] = pd.to_datetime(df['date'])\n            fname = os.path.basename(f).replace(\".parquet\",\"\")\n            if 'ticker' not in df.columns:\n                df.insert(0, 'ticker', fname)\n            dfs.append(df)\n        except Exception as e:\n            logging.warning(\"Erro lendo %s: %s\", f, e)\n    if not dfs:\n        raise RuntimeError(\"Nenhum parquet encontrado para combinar.\")\n    big = pd.concat(dfs, ignore_index=True, sort=False)\n    big.to_parquet(out_path, index=False)\n    logging.info(\"Combined saved to %s (rows=%d)\", out_path, len(big))\n    if out_csv:\n        try:\n            # converter date para formato iso ao salvar CSV\n            if 'date' in big.columns:\n                big['date'] = pd.to_datetime(big['date']).dt.strftime('%Y-%m-%d')\n            big.to_csv(out_csv, index=False)\n            logging.info(\"Combined CSV saved to %s\", out_csv)\n        except Exception as e:\n            logging.exception(\"Erro salvando combined CSV: %s\", e)\n    return big\n\nif __name__ == \"__main__\":\n    # 1) carregue a lista de tickers a partir do arquivo que voc√™ j√° salvou\n    # Usa caminho relativo para acessar a pasta data no mesmo n√≠vel de data_sources\n    tickers_file = os.path.join(\"..\", \"data\", \"tickers_ibrx100_full.csv\")\n    if os.path.exists(tickers_file):\n        df = pd.read_csv(tickers_file)\n        if 'Ticker' in df.columns:\n            tickers = df['Ticker'].dropna().astype(str).tolist()\n        else:\n            tickers = df.iloc[:,0].dropna().astype(str).tolist()\n    else:\n        raise RuntimeError(f\"N√£o encontrou {tickers_file}. Coloque seu CSV de tickers na pasta 'aurum/data/' ou edite este script.\")\n\n    # 2) op√ß√£o: validar/normalizar tickers (garantir sufixo .SA)\n    def normalize(t):\n        t = str(t).strip().upper()\n        if not t.endswith(\".SA\"):\n            t = t.replace(\".SA\",\"\") + \".SA\"\n        return t\n    tickers = [normalize(t) for t in tickers]\n    print(\"Tickers a baixar:\", len(tickers), tickers[:10])\n\n    # 3) executar (force=True re-baixa mesmo se j√° existir)\n    summary_df = download_all_histories(tickers, start=DEFAULT_START, force=False, save_summary=True, save_csv_per_ticker=True)\n    print(summary_df.head(50))\n\n    # 4) opcional: combinar tudo em um √∫nico parquet e CSV (pode ser grande)\n    combined = combine_all_to_single_parquet()\n    print(\"Combined rows:\", len(combined))\n"
  },
  {
   "cell_type": "markdown",
   "id": "a8a1e096",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5bc8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "\n",
    "def print_header(title):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o auxiliar para imprimir um cabe√ßalho formatado no log.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\" {title.upper()} \")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "def analyze_dataframe(file_path):\n",
    "    \"\"\"\n",
    "    Carrega e analisa um DataFrame de hist√≥rico de a√ß√µes.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"--- Iniciando An√°lise do Arquivo: {file_path} ---\")\n",
    "\n",
    "    # --- 1. Verifica√ß√£o e Carregamento dos Dados ---\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print_header(\"[ERRO] Arquivo n√£o encontrado\")\n",
    "        print(f\"O arquivo no caminho '{file_path}' n√£o foi localizado.\")\n",
    "        print(\"Por favor, verifique se o caminho est√° correto e a pasta 'historical' existe.\")\n",
    "        print(\"=\" * 70)\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Tenta carregar o CSV. \n",
    "        # A coluna 'date' √© convertida para datetime no carregamento.\n",
    "        df = pd.read_csv(file_path, parse_dates=['date'])\n",
    "        print(f\"\\n[SUCESSO] Arquivo carregado. Total de {len(df)} linhas e {len(df.columns)} colunas.\")\n",
    "    except Exception as e:\n",
    "        print_header(\"[ERRO] Falha ao carregar o arquivo\")\n",
    "        print(f\"Ocorreu um erro ao tentar ler o arquivo CSV: {e}\")\n",
    "        print(\"=\" * 70)\n",
    "        return\n",
    "\n",
    "    # --- 2. Amostra dos Dados (Head) ---\n",
    "    print_header(\"1. Amostra dos Dados (Primeiras 5 Linhas)\")\n",
    "    # .to_string() formata o DataFrame como uma tabela de texto leg√≠vel\n",
    "    print(df.head().to_string())\n",
    "\n",
    "    # --- 3. Informa√ß√µes do DataFrame (Info) ---\n",
    "    print_header(\"2. Informa√ß√µes do DataFrame (Tipos de Coluna e Nulos)\")\n",
    "    # O df.info() imprime diretamente. Para captur√°-lo e format√°-lo,\n",
    "    # usamos um buffer de string.\n",
    "    buffer = io.StringIO()\n",
    "    df.info(buf=buffer)\n",
    "    info_str = buffer.getvalue()\n",
    "    print(info_str)\n",
    "\n",
    "    # --- 4. Contagem de Valores Nulos ---\n",
    "    print_header(\"3. Resumo de Valores Nulos por Coluna\")\n",
    "    null_counts = df.isnull().sum()\n",
    "    \n",
    "    if null_counts.sum() == 0:\n",
    "        print(\"√ìtimo! N√£o h√° valores nulos em nenhuma coluna.\")\n",
    "    else:\n",
    "        # Filtra para mostrar apenas colunas que *possuem* valores nulos\n",
    "        print(null_counts[null_counts > 0].to_string())\n",
    "\n",
    "    # --- 5. Estat√≠sticas Descritivas ---\n",
    "    print_header(\"4. Estat√≠sticas Descritivas (Colunas Num√©ricas)\")\n",
    "    # O 'include='np.number' garante que s√≥ analisar√° colunas num√©ricas\n",
    "    # .to_string() formata a sa√≠da para melhor visualiza√ß√£o no log\n",
    "    try:\n",
    "        print(df.describe().to_string())\n",
    "    except Exception as e:\n",
    "        print(f\"N√£o foi poss√≠vel calcular estat√≠sticas descritivas: {e}\")\n",
    "\n",
    "    # --- 6. An√°lise de Tickers ---\n",
    "    print_header(\"5. An√°lise da Coluna 'ticker'\")\n",
    "    if 'ticker' in df.columns:\n",
    "        unique_tickers = df['ticker'].unique()\n",
    "        num_unique_tickers = len(unique_tickers)\n",
    "        print(f\"Total de tickers √∫nicos encontrados: {num_unique_tickers}\")\n",
    "        \n",
    "        # Mostra uma amostra se houver muitos tickers\n",
    "        if num_unique_tickers > 10:\n",
    "            print(f\"Amostra de tickers: {unique_tickers[:10]}...\")\n",
    "        else:\n",
    "            print(f\"Tickers presentes: {unique_tickers}\")\n",
    "    else:\n",
    "        print(\"Coluna 'ticker' n√£o encontrada.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"--- AN√ÅLISE CONCLU√çDA ---\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# --- Ponto de Execu√ß√£o Principal ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Define o caminho do arquivo\n",
    "    path_do_arquivo = 'data/historical/all_histories.csv'\n",
    "    \n",
    "    # Executa a fun√ß√£o de an√°lise\n",
    "    analyze_dataframe(path_do_arquivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e460a5d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85e9d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os # Garante que 'os' est√° importado\n",
    "\n",
    "# Configura√ß√£o de logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 1. Carregar os Dados ---\n",
    "try:\n",
    "    # Tenta carregar o Parquet primeiro, √© mais r√°pido\n",
    "    df_prices_raw = pd.read_parquet(\"data/historical/all_histories.parquet\")\n",
    "    logger.info(f\"Dados de pre√ßo brutos carregados do PARQUET: {df_prices_raw.shape[0]} linhas\")\n",
    "except FileNotFoundError:\n",
    "    logger.warning(\"Arquivo all_histories.parquet n√£o encontrado! Tentando carregar o CSV...\")\n",
    "    try:\n",
    "        # Carrega o CSV como fallback\n",
    "        df_prices_raw = pd.read_csv(\"data/historical/all_histories.csv\", parse_dates=['date'])\n",
    "        logger.info(f\"Dados de pre√ßo brutos carregados do CSV: {df_prices_raw.shape[0]} linhas\")\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(\"Nenhum arquivo de hist√≥rico (parquet ou csv) encontrado. Execute o download primeiro.\")\n",
    "        raise e\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erro ao carregar dados: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- 2. Limpeza Cr√≠tica (Remover NaN de pr√©-IPO) ---\n",
    "# Remove todas as linhas onde a a√ß√£o ainda n√£o existia (Adj Close ou Volume s√£o NaN)\n",
    "df_prices_clean = df_prices_raw.dropna(subset=['Adj Close', 'Volume'])\n",
    "\n",
    "# --- 3. Limpeza Opcional (Remover dias sem negocia√ß√£o) ---\n",
    "df_prices_clean = df_prices_clean[df_prices_clean['Volume'] > 0]\n",
    "\n",
    "linhas_removidas = len(df_prices_raw) - len(df_prices_clean)\n",
    "logger.info(f\"Dados de pre√ßo limpos: {df_prices_clean.shape[0]} linhas (removidas {linhas_removidas} linhas com NaN ou Volume 0)\")\n",
    "\n",
    "# --- 4. Salvar os Arquivos Limpos ---\n",
    "# Garante que o diret√≥rio existe\n",
    "output_dir = \"data/historical\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "parquet_path = os.path.join(output_dir, \"all_histories_cleaned.parquet\")\n",
    "csv_path = os.path.join(output_dir, \"all_histories_cleaned.csv\")\n",
    "\n",
    "try:\n",
    "    # Salvar em Parquet (preferencial para o pr√≥ximo passo)\n",
    "    df_prices_clean.to_parquet(parquet_path, index=False)\n",
    "    logger.info(f\"‚úÖ Arquivo limpo salvo em (Parquet): {parquet_path}\")\n",
    "\n",
    "    # Salvar em CSV (para sua verifica√ß√£o)\n",
    "    # date_format garante que a data seja salva em formato leg√≠vel\n",
    "    df_prices_clean.to_csv(csv_path, index=False, date_format='%Y-%m-%d')\n",
    "    logger.info(f\"‚úÖ Arquivo limpo salvo em (CSV): {csv_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Erro ao salvar arquivos limpos: {e}\")\n",
    "\n",
    "print(f\"\\nLimpeza conclu√≠da. {linhas_removidas} linhas de 'lookahead' removidas.\")\n",
    "print(df_prices_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae18351",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90af87d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# --- Configura√ß√£o ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "INPUT_FILE = \"data/historical/all_histories_cleaned.parquet\"\n",
    "OUTPUT_DIR = \"data/historical\"\n",
    "\n",
    "def gerar_precos_pivotados():\n",
    "    logger.info(\"Iniciando o Passo 1: Gera√ß√£o dos Pre√ßos Pivotados (Wide)...\")\n",
    "    \n",
    "    # 1. Carregar os dados de pre√ßo limpos\n",
    "    try:\n",
    "        df_prices_clean = pd.read_parquet(INPUT_FILE)\n",
    "        logger.info(f\"Dados limpos '{INPUT_FILE}' carregados.\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"ARQUIVO N√ÉO ENCONTRADO: {INPUT_FILE}\")\n",
    "        logger.error(\"Execute o 'Script 1: Limpeza' primeiro.\")\n",
    "        return\n",
    "\n",
    "    df_prices_clean['date'] = pd.to_datetime(df_prices_clean['date'])\n",
    "\n",
    "    # 2. Criar a base de pre√ßos mensais\n",
    "    logger.info(\"Reamostrando para Frequ√™ncia Mensal (M e MS)...\")\n",
    "    # 2a. Fechamento (√öltimo dia do m√™s 'M')\n",
    "    df_prices_mensal_raw = df_prices_clean.set_index('date').groupby('ticker').resample('M').last()\n",
    "\n",
    "    # 2b. Abertura (Primeiro dia do m√™s 'MS')\n",
    "    df_open_mensal_raw = df_prices_clean.set_index('date').groupby('ticker').resample('MS').first()\n",
    "\n",
    "    # 3. Pivotar para o formato \"wide\"\n",
    "    logger.info(\"Corrigindo MultiIndex e Pivotando...\")\n",
    "    \n",
    "    # CORRE√á√ÉO: Dropar a coluna 'ticker' duplicada antes de resetar o √≠ndice\n",
    "    df_prices_mensal_long = df_prices_mensal_raw.drop(columns='ticker', errors='ignore').reset_index()\n",
    "    df_open_mensal_long = df_open_mensal_raw.drop(columns='ticker', errors='ignore').reset_index()\n",
    "\n",
    "    # Agora o .pivot() funcionar√°\n",
    "    df_close_wide = df_prices_mensal_long.pivot(index='date', columns='ticker', values='Adj Close')\n",
    "    df_open_wide = df_open_mensal_long.pivot(index='date', columns='ticker', values='Open')\n",
    "\n",
    "    # 4. Sincronizar os √≠ndices de data\n",
    "    logger.info(\"Sincronizando e preenchendo √≠ndices de data...\")\n",
    "    idx_union = df_close_wide.index.union(df_open_wide.index)\n",
    "    \n",
    "    df_close_wide = df_close_wide.reindex(idx_union, method='ffill')\n",
    "    df_open_wide = df_open_wide.reindex(idx_union, method='ffill')\n",
    "\n",
    "    # Preenche NaNs (pr√©-IPO e p√≥s-delist)\n",
    "    df_close_wide = df_close_wide.ffill().bfill()\n",
    "    df_open_wide = df_open_wide.ffill().bfill()\n",
    "\n",
    "    # --- 5. SALVAR OS ARQUIVOS FALTOSOS ---\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    path_close = os.path.join(OUTPUT_DIR, \"prices_close_wide.parquet\")\n",
    "    path_open = os.path.join(OUTPUT_DIR, \"prices_open_wide.parquet\")\n",
    "    \n",
    "    df_close_wide.to_parquet(path_close)\n",
    "    df_open_wide.to_parquet(path_open)\n",
    "    \n",
    "    logger.info(f\"‚úÖ ARQUIVO FALTOSO GERADO: {path_close}\")\n",
    "    logger.info(f\"‚úÖ ARQUIVO FALTOSO GERADO: {path_open}\")\n",
    "    print(\"\\n--- Amostra de Fechamento (Wide) ---\")\n",
    "    print(df_close_wide.tail())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gerar_precos_pivotados()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979ffc10",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130560c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Configura√ß√£o de logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Iniciando Passo 2: C√°lculo da Volatilidade...\")\n",
    "\n",
    "# Carregar os dados de pre√ßo DI√ÅRIOS (limpos)\n",
    "df_prices_daily = pd.read_parquet(\"data/historical/all_histories_cleaned.parquet\")\n",
    "df_prices_daily['date'] = pd.to_datetime(df_prices_daily['date'])\n",
    "\n",
    "# 1. Calcular retornos di√°rios\n",
    "df_prices_daily['returns'] = df_prices_daily.groupby('ticker')['Adj Close'].pct_change()\n",
    "\n",
    "# 2. Calcular a volatilidade m√≥vel de 63 dias (~3 meses de negocia√ß√£o)\n",
    "# .std() calcula o desvio padr√£o (volatilidade)\n",
    "# reset_index(0, drop=True) √© necess√°rio ap√≥s o .rolling() em um groupby\n",
    "df_prices_daily['VOLATILIDADE'] = df_prices_daily.groupby('ticker')['returns'].rolling(window=63).std().reset_index(0, drop=True)\n",
    "\n",
    "# 3. Resample da volatilidade para MENSAL (pegamos o √∫ltimo valor do m√™s)\n",
    "df_vol_mensal = df_prices_daily.set_index('date').groupby('ticker').resample('M').last()['VOLATILIDADE'].reset_index()\n",
    "\n",
    "logger.info(f\"‚úÖ Volatilidade mensal calculada. {len(df_vol_mensal)} registros.\")\n",
    "print(df_vol_mensal.tail())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}