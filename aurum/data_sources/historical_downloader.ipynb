{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a68349e",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ—‚ï¸ HistÃ³rico â€” downloader & combinador (b3_hist_downloader.py)\n",
    "\n",
    "**Resumo rÃ¡pido**  \n",
    "Script que baixa histÃ³ricos de preÃ§os (via **yfinance**), salva por ticker em parquet (e opcionalmente CSV), e permite combinar todos os parquets em um Ãºnico arquivo *long-format*. Feito para rodar em batch com retry/backoff, pular tickers jÃ¡ salvos e gerar um resumo final.\n",
    "\n",
    "\n",
    "#### âœ¨ Principais responsabilidades\n",
    "- Baixar histÃ³ricos de preÃ§o por lotes (batch) com **yfinance**.  \n",
    "- Salvar cada ticker em **data/historical/{TICKER}.parquet** e **{TICKER}.csv**.  \n",
    "- Gerenciar retries (exponencial), fallback ticker-a-ticker e logs.  \n",
    "- Gerar CSV de resumo por execuÃ§Ã£o (**download_summary_YYYYMMDDTHHMMSSZ.csv**).  \n",
    "- Recombinar todos os parquets em um Ãºnico **all_histories.parquet** / **all_histories.csv**.\n",
    "\n",
    "\n",
    "#### ðŸ§© FunÃ§Ãµes principais (one-liners)\n",
    "- **ticker_exists_local(ticker)** â†’ verifica existÃªncia de **{ticker}.parquet**.  \n",
    "- **save_history_df(ticker, df, save_csv=True)** â†’ salva parquet e CSV; garante coluna **date**.  \n",
    "- **download_batch(batch, start, threads)** â†’ tenta baixar um batch via **yfinance.download** com retries e fallback.  \n",
    "- **download_all_histories(tickers, start, force, save_summary, save_csv_per_ticker)** â†’ orquestra o download em batches, salva e retorna um **DataFrame** resumo.  \n",
    "- **combine_all_to_single_parquet(out_path, out_csv, tickers)** â†’ concatena todos os parquets em formato long e salva.\n",
    "\n",
    "\n",
    "#### âš™ï¸ ParÃ¢metros principais (valores padrÃ£o)\n",
    "| ParÃ¢metro | Valor padrÃ£o |\n",
    "|---:|:---|\n",
    "| **HIST_DIR** | **data/historical/** |\n",
    "| **DEFAULT_START** | **\"2011-01-01\"** |\n",
    "| **BATCH_SIZE** | **15** |\n",
    "| **MAX_ATTEMPTS** | **4** |\n",
    "| **SLEEP_BETWEEN_BATCHES** | **1** (seg) |\n",
    "| **SLEEP_BETWEEN_TICKERS** | **0.2** (seg) |\n",
    "\n",
    "\n",
    "#### ðŸ“‚ SaÃ­das geradas\n",
    "- **data/historical/{TICKER}.parquet** â€” parquet por ticker.  \n",
    "- **data/historical/{TICKER}.csv** â€”  CSV por ticker.  \n",
    "- **data/historical/download_summary_{ts}.csv** â€” resumo da execuÃ§Ã£o.  \n",
    "- **data/historical/all_histories.parquet** & **all_histories.csv** â€” concat final (long format).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffeaf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 17:58:44,565 INFO Processando batch 1/7 (download 15/15)\n",
      "2025-12-11 17:58:44,573 INFO yfinance.download attempt 1 for batch size 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tickers a baixar: 97 ['ALOS3.SA', 'ABEV3.SA', 'ANIM3.SA', 'ASAI3.SA', 'AURE3.SA', 'AXIA3.SA', 'AXIA6.SA', 'AZZA3.SA', 'B3SA3.SA', 'BBSE3.SA']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 17:58:48,183 INFO Saved 3716 rows for ALOS3.SA -> ..\\data\\historical\\ALOS3.SA.parquet\n",
      "2025-12-11 17:58:48,244 INFO Saved CSV for ALOS3.SA -> ..\\data\\historical\\ALOS3.SA.csv\n",
      "2025-12-11 17:58:48,480 INFO Saved 3716 rows for ABEV3.SA -> ..\\data\\historical\\ABEV3.SA.parquet\n",
      "2025-12-11 17:58:48,585 INFO Saved CSV for ABEV3.SA -> ..\\data\\historical\\ABEV3.SA.csv\n",
      "2025-12-11 17:58:48,811 INFO Saved 3716 rows for ANIM3.SA -> ..\\data\\historical\\ANIM3.SA.parquet\n",
      "2025-12-11 17:58:48,885 INFO Saved CSV for ANIM3.SA -> ..\\data\\historical\\ANIM3.SA.csv\n",
      "2025-12-11 17:58:49,105 INFO Saved 3716 rows for ASAI3.SA -> ..\\data\\historical\\ASAI3.SA.parquet\n",
      "2025-12-11 17:58:49,180 INFO Saved CSV for ASAI3.SA -> ..\\data\\historical\\ASAI3.SA.csv\n",
      "2025-12-11 17:58:49,399 INFO Saved 3716 rows for AURE3.SA -> ..\\data\\historical\\AURE3.SA.parquet\n",
      "2025-12-11 17:58:49,453 INFO Saved CSV for AURE3.SA -> ..\\data\\historical\\AURE3.SA.csv\n",
      "2025-12-11 17:58:49,668 INFO Saved 3716 rows for AXIA3.SA -> ..\\data\\historical\\AXIA3.SA.parquet\n",
      "2025-12-11 17:58:49,751 INFO Saved CSV for AXIA3.SA -> ..\\data\\historical\\AXIA3.SA.csv\n",
      "2025-12-11 17:58:49,986 INFO Saved 3716 rows for AXIA6.SA -> ..\\data\\historical\\AXIA6.SA.parquet\n",
      "2025-12-11 17:58:50,083 INFO Saved CSV for AXIA6.SA -> ..\\data\\historical\\AXIA6.SA.csv\n",
      "2025-12-11 17:58:50,303 INFO Saved 3716 rows for AZZA3.SA -> ..\\data\\historical\\AZZA3.SA.parquet\n",
      "2025-12-11 17:58:50,363 INFO Saved CSV for AZZA3.SA -> ..\\data\\historical\\AZZA3.SA.csv\n",
      "2025-12-11 17:58:50,578 INFO Saved 3716 rows for B3SA3.SA -> ..\\data\\historical\\B3SA3.SA.parquet\n",
      "2025-12-11 17:58:50,648 INFO Saved CSV for B3SA3.SA -> ..\\data\\historical\\B3SA3.SA.csv\n",
      "2025-12-11 17:58:50,877 INFO Saved 3716 rows for BBSE3.SA -> ..\\data\\historical\\BBSE3.SA.parquet\n",
      "2025-12-11 17:58:50,941 INFO Saved CSV for BBSE3.SA -> ..\\data\\historical\\BBSE3.SA.csv\n",
      "2025-12-11 17:58:51,180 INFO Saved 3716 rows for BBDC3.SA -> ..\\data\\historical\\BBDC3.SA.parquet\n",
      "2025-12-11 17:58:51,243 INFO Saved CSV for BBDC3.SA -> ..\\data\\historical\\BBDC3.SA.csv\n",
      "2025-12-11 17:58:51,469 INFO Saved 3716 rows for BBDC4.SA -> ..\\data\\historical\\BBDC4.SA.parquet\n",
      "2025-12-11 17:58:51,595 INFO Saved CSV for BBDC4.SA -> ..\\data\\historical\\BBDC4.SA.csv\n",
      "2025-12-11 17:58:51,828 INFO Saved 3716 rows for BRAP4.SA -> ..\\data\\historical\\BRAP4.SA.parquet\n",
      "2025-12-11 17:58:51,905 INFO Saved CSV for BRAP4.SA -> ..\\data\\historical\\BRAP4.SA.csv\n",
      "2025-12-11 17:58:52,150 INFO Saved 3716 rows for BBAS3.SA -> ..\\data\\historical\\BBAS3.SA.parquet\n",
      "2025-12-11 17:58:52,217 INFO Saved CSV for BBAS3.SA -> ..\\data\\historical\\BBAS3.SA.csv\n",
      "2025-12-11 17:58:52,449 INFO Saved 3716 rows for BRKM5.SA -> ..\\data\\historical\\BRKM5.SA.parquet\n",
      "2025-12-11 17:58:52,520 INFO Saved CSV for BRKM5.SA -> ..\\data\\historical\\BRKM5.SA.csv\n",
      "2025-12-11 17:58:53,727 INFO Processando batch 2/7 (download 15/15)\n",
      "2025-12-11 17:58:53,729 INFO yfinance.download attempt 1 for batch size 15\n",
      "2025-12-11 17:58:55,995 INFO Saved 3716 rows for BRAV3.SA -> ..\\data\\historical\\BRAV3.SA.parquet\n",
      "2025-12-11 17:58:56,041 INFO Saved CSV for BRAV3.SA -> ..\\data\\historical\\BRAV3.SA.csv\n",
      "2025-12-11 17:58:56,273 INFO Saved 3716 rows for BPAC11.SA -> ..\\data\\historical\\BPAC11.SA.parquet\n",
      "2025-12-11 17:58:56,349 INFO Saved CSV for BPAC11.SA -> ..\\data\\historical\\BPAC11.SA.csv\n",
      "2025-12-11 17:58:56,572 INFO Saved 3716 rows for CXSE3.SA -> ..\\data\\historical\\CXSE3.SA.parquet\n",
      "2025-12-11 17:58:56,619 INFO Saved CSV for CXSE3.SA -> ..\\data\\historical\\CXSE3.SA.csv\n",
      "2025-12-11 17:58:56,928 INFO Saved 3716 rows for CEAB3.SA -> ..\\data\\historical\\CEAB3.SA.parquet\n",
      "2025-12-11 17:58:56,997 INFO Saved CSV for CEAB3.SA -> ..\\data\\historical\\CEAB3.SA.csv\n",
      "2025-12-11 17:58:57,228 INFO Saved 3716 rows for CMIG4.SA -> ..\\data\\historical\\CMIG4.SA.parquet\n",
      "2025-12-11 17:58:57,297 INFO Saved CSV for CMIG4.SA -> ..\\data\\historical\\CMIG4.SA.csv\n",
      "2025-12-11 17:58:57,522 INFO Saved 3716 rows for COGN3.SA -> ..\\data\\historical\\COGN3.SA.parquet\n",
      "2025-12-11 17:58:57,595 INFO Saved CSV for COGN3.SA -> ..\\data\\historical\\COGN3.SA.csv\n",
      "2025-12-11 17:58:57,816 INFO Saved 3716 rows for CSMG3.SA -> ..\\data\\historical\\CSMG3.SA.parquet\n",
      "2025-12-11 17:58:57,888 INFO Saved CSV for CSMG3.SA -> ..\\data\\historical\\CSMG3.SA.csv\n",
      "2025-12-11 17:58:58,104 INFO Saved 3716 rows for CPLE3.SA -> ..\\data\\historical\\CPLE3.SA.parquet\n",
      "2025-12-11 17:58:58,156 INFO Saved CSV for CPLE3.SA -> ..\\data\\historical\\CPLE3.SA.csv\n",
      "2025-12-11 17:58:58,369 INFO Saved 3716 rows for CPLE5.SA -> ..\\data\\historical\\CPLE5.SA.parquet\n",
      "2025-12-11 17:58:58,427 INFO Saved CSV for CPLE5.SA -> ..\\data\\historical\\CPLE5.SA.csv\n",
      "2025-12-11 17:58:58,649 INFO Saved 3716 rows for CSAN3.SA -> ..\\data\\historical\\CSAN3.SA.parquet\n",
      "2025-12-11 17:58:58,713 INFO Saved CSV for CSAN3.SA -> ..\\data\\historical\\CSAN3.SA.csv\n",
      "2025-12-11 17:58:58,931 INFO Saved 3716 rows for CPFE3.SA -> ..\\data\\historical\\CPFE3.SA.parquet\n",
      "2025-12-11 17:58:58,987 INFO Saved CSV for CPFE3.SA -> ..\\data\\historical\\CPFE3.SA.csv\n",
      "2025-12-11 17:58:59,206 INFO Saved 3716 rows for CMIN3.SA -> ..\\data\\historical\\CMIN3.SA.parquet\n",
      "2025-12-11 17:58:59,257 INFO Saved CSV for CMIN3.SA -> ..\\data\\historical\\CMIN3.SA.csv\n",
      "2025-12-11 17:58:59,476 INFO Saved 3716 rows for CURY3.SA -> ..\\data\\historical\\CURY3.SA.parquet\n",
      "2025-12-11 17:58:59,519 INFO Saved CSV for CURY3.SA -> ..\\data\\historical\\CURY3.SA.csv\n",
      "2025-12-11 17:58:59,745 INFO Saved 3716 rows for CVCB3.SA -> ..\\data\\historical\\CVCB3.SA.parquet\n",
      "2025-12-11 17:58:59,805 INFO Saved CSV for CVCB3.SA -> ..\\data\\historical\\CVCB3.SA.csv\n",
      "2025-12-11 17:59:00,026 INFO Saved 3716 rows for CYRE3.SA -> ..\\data\\historical\\CYRE3.SA.parquet\n",
      "2025-12-11 17:59:00,109 INFO Saved CSV for CYRE3.SA -> ..\\data\\historical\\CYRE3.SA.csv\n",
      "2025-12-11 17:59:01,314 INFO Processando batch 3/7 (download 15/15)\n",
      "2025-12-11 17:59:01,315 INFO yfinance.download attempt 1 for batch size 15\n",
      "2025-12-11 17:59:03,572 INFO Saved 3716 rows for DIRR3.SA -> ..\\data\\historical\\DIRR3.SA.parquet\n",
      "2025-12-11 17:59:03,621 INFO Saved CSV for DIRR3.SA -> ..\\data\\historical\\DIRR3.SA.csv\n",
      "2025-12-11 17:59:03,836 INFO Saved 3716 rows for ECOR3.SA -> ..\\data\\historical\\ECOR3.SA.parquet\n",
      "2025-12-11 17:59:03,892 INFO Saved CSV for ECOR3.SA -> ..\\data\\historical\\ECOR3.SA.csv\n",
      "2025-12-11 17:59:04,105 INFO Saved 3716 rows for EMBJ3.SA -> ..\\data\\historical\\EMBJ3.SA.parquet\n",
      "2025-12-11 17:59:04,140 INFO Saved CSV for EMBJ3.SA -> ..\\data\\historical\\EMBJ3.SA.csv\n",
      "2025-12-11 17:59:04,356 INFO Saved 3716 rows for ENGI11.SA -> ..\\data\\historical\\ENGI11.SA.parquet\n",
      "2025-12-11 17:59:04,409 INFO Saved CSV for ENGI11.SA -> ..\\data\\historical\\ENGI11.SA.csv\n",
      "2025-12-11 17:59:04,620 INFO Saved 3716 rows for ENEV3.SA -> ..\\data\\historical\\ENEV3.SA.parquet\n",
      "2025-12-11 17:59:04,678 INFO Saved CSV for ENEV3.SA -> ..\\data\\historical\\ENEV3.SA.csv\n",
      "2025-12-11 17:59:04,897 INFO Saved 3716 rows for EGIE3.SA -> ..\\data\\historical\\EGIE3.SA.parquet\n",
      "2025-12-11 17:59:04,962 INFO Saved CSV for EGIE3.SA -> ..\\data\\historical\\EGIE3.SA.csv\n",
      "2025-12-11 17:59:05,176 INFO Saved 3716 rows for EQTL3.SA -> ..\\data\\historical\\EQTL3.SA.parquet\n",
      "2025-12-11 17:59:05,235 INFO Saved CSV for EQTL3.SA -> ..\\data\\historical\\EQTL3.SA.csv\n",
      "2025-12-11 17:59:05,455 INFO Saved 3716 rows for EZTC3.SA -> ..\\data\\historical\\EZTC3.SA.parquet\n",
      "2025-12-11 17:59:05,529 INFO Saved CSV for EZTC3.SA -> ..\\data\\historical\\EZTC3.SA.csv\n",
      "2025-12-11 17:59:05,761 INFO Saved 3716 rows for FLRY3.SA -> ..\\data\\historical\\FLRY3.SA.parquet\n",
      "2025-12-11 17:59:05,826 INFO Saved CSV for FLRY3.SA -> ..\\data\\historical\\FLRY3.SA.csv\n",
      "2025-12-11 17:59:06,043 INFO Saved 3716 rows for GGBR4.SA -> ..\\data\\historical\\GGBR4.SA.parquet\n",
      "2025-12-11 17:59:06,110 INFO Saved CSV for GGBR4.SA -> ..\\data\\historical\\GGBR4.SA.csv\n",
      "2025-12-11 17:59:06,326 INFO Saved 3716 rows for GOAU4.SA -> ..\\data\\historical\\GOAU4.SA.parquet\n",
      "2025-12-11 17:59:06,387 INFO Saved CSV for GOAU4.SA -> ..\\data\\historical\\GOAU4.SA.csv\n",
      "2025-12-11 17:59:06,600 INFO Saved 3716 rows for GGPS3.SA -> ..\\data\\historical\\GGPS3.SA.parquet\n",
      "2025-12-11 17:59:06,671 INFO Saved CSV for GGPS3.SA -> ..\\data\\historical\\GGPS3.SA.csv\n",
      "2025-12-11 17:59:06,885 INFO Saved 3716 rows for GMAT3.SA -> ..\\data\\historical\\GMAT3.SA.parquet\n",
      "2025-12-11 17:59:06,977 INFO Saved CSV for GMAT3.SA -> ..\\data\\historical\\GMAT3.SA.csv\n",
      "2025-12-11 17:59:07,212 INFO Saved 3716 rows for HAPV3.SA -> ..\\data\\historical\\HAPV3.SA.parquet\n",
      "2025-12-11 17:59:07,297 INFO Saved CSV for HAPV3.SA -> ..\\data\\historical\\HAPV3.SA.csv\n",
      "2025-12-11 17:59:07,518 INFO Saved 3716 rows for HYPE3.SA -> ..\\data\\historical\\HYPE3.SA.parquet\n",
      "2025-12-11 17:59:07,587 INFO Saved CSV for HYPE3.SA -> ..\\data\\historical\\HYPE3.SA.csv\n",
      "2025-12-11 17:59:08,792 INFO Processando batch 4/7 (download 15/15)\n",
      "2025-12-11 17:59:08,795 INFO yfinance.download attempt 1 for batch size 15\n",
      "2025-12-11 17:59:10,730 INFO Saved 3716 rows for IGTI11.SA -> ..\\data\\historical\\IGTI11.SA.parquet\n",
      "2025-12-11 17:59:10,769 INFO Saved CSV for IGTI11.SA -> ..\\data\\historical\\IGTI11.SA.csv\n",
      "2025-12-11 17:59:10,999 INFO Saved 3716 rows for INTB3.SA -> ..\\data\\historical\\INTB3.SA.parquet\n",
      "2025-12-11 17:59:11,045 INFO Saved CSV for INTB3.SA -> ..\\data\\historical\\INTB3.SA.csv\n",
      "2025-12-11 17:59:11,263 INFO Saved 3716 rows for IRBR3.SA -> ..\\data\\historical\\IRBR3.SA.parquet\n",
      "2025-12-11 17:59:11,315 INFO Saved CSV for IRBR3.SA -> ..\\data\\historical\\IRBR3.SA.csv\n",
      "2025-12-11 17:59:11,534 INFO Saved 3716 rows for ISAE4.SA -> ..\\data\\historical\\ISAE4.SA.parquet\n",
      "2025-12-11 17:59:11,602 INFO Saved CSV for ISAE4.SA -> ..\\data\\historical\\ISAE4.SA.csv\n",
      "2025-12-11 17:59:11,819 INFO Saved 3716 rows for ITSA4.SA -> ..\\data\\historical\\ITSA4.SA.parquet\n",
      "2025-12-11 17:59:11,879 INFO Saved CSV for ITSA4.SA -> ..\\data\\historical\\ITSA4.SA.csv\n",
      "2025-12-11 17:59:12,100 INFO Saved 3716 rows for ITUB4.SA -> ..\\data\\historical\\ITUB4.SA.parquet\n",
      "2025-12-11 17:59:12,167 INFO Saved CSV for ITUB4.SA -> ..\\data\\historical\\ITUB4.SA.csv\n",
      "2025-12-11 17:59:12,382 INFO Saved 3716 rows for KLBN11.SA -> ..\\data\\historical\\KLBN11.SA.parquet\n",
      "2025-12-11 17:59:12,437 INFO Saved CSV for KLBN11.SA -> ..\\data\\historical\\KLBN11.SA.csv\n",
      "2025-12-11 17:59:12,655 INFO Saved 3716 rows for RENT3.SA -> ..\\data\\historical\\RENT3.SA.parquet\n",
      "2025-12-11 17:59:12,707 INFO Saved CSV for RENT3.SA -> ..\\data\\historical\\RENT3.SA.csv\n",
      "2025-12-11 17:59:12,925 INFO Saved 3716 rows for LREN3.SA -> ..\\data\\historical\\LREN3.SA.parquet\n",
      "2025-12-11 17:59:12,977 INFO Saved CSV for LREN3.SA -> ..\\data\\historical\\LREN3.SA.csv\n",
      "2025-12-11 17:59:13,191 INFO Saved 3716 rows for LWSA3.SA -> ..\\data\\historical\\LWSA3.SA.parquet\n",
      "2025-12-11 17:59:13,261 INFO Saved CSV for LWSA3.SA -> ..\\data\\historical\\LWSA3.SA.csv\n",
      "2025-12-11 17:59:13,475 INFO Saved 3716 rows for MGLU3.SA -> ..\\data\\historical\\MGLU3.SA.parquet\n",
      "2025-12-11 17:59:13,532 INFO Saved CSV for MGLU3.SA -> ..\\data\\historical\\MGLU3.SA.csv\n",
      "2025-12-11 17:59:13,747 INFO Saved 3716 rows for POMO4.SA -> ..\\data\\historical\\POMO4.SA.parquet\n",
      "2025-12-11 17:59:13,812 INFO Saved CSV for POMO4.SA -> ..\\data\\historical\\POMO4.SA.csv\n",
      "2025-12-11 17:59:14,037 INFO Saved 3716 rows for MBRF3.SA -> ..\\data\\historical\\MBRF3.SA.parquet\n",
      "2025-12-11 17:59:14,095 INFO Saved CSV for MBRF3.SA -> ..\\data\\historical\\MBRF3.SA.csv\n",
      "2025-12-11 17:59:14,316 INFO Saved 3716 rows for BEEF3.SA -> ..\\data\\historical\\BEEF3.SA.parquet\n",
      "2025-12-11 17:59:14,460 INFO Saved CSV for BEEF3.SA -> ..\\data\\historical\\BEEF3.SA.csv\n",
      "2025-12-11 17:59:14,671 INFO Saved 3716 rows for MOTV3.SA -> ..\\data\\historical\\MOTV3.SA.parquet\n",
      "2025-12-11 17:59:14,703 INFO Saved CSV for MOTV3.SA -> ..\\data\\historical\\MOTV3.SA.csv\n",
      "2025-12-11 17:59:15,908 INFO Processando batch 5/7 (download 15/15)\n",
      "2025-12-11 17:59:15,908 INFO yfinance.download attempt 1 for batch size 15\n",
      "2025-12-11 17:59:18,251 INFO Saved 3716 rows for MOVI3.SA -> ..\\data\\historical\\MOVI3.SA.parquet\n",
      "2025-12-11 17:59:18,296 INFO Saved CSV for MOVI3.SA -> ..\\data\\historical\\MOVI3.SA.csv\n",
      "2025-12-11 17:59:18,516 INFO Saved 3716 rows for MRVE3.SA -> ..\\data\\historical\\MRVE3.SA.parquet\n",
      "2025-12-11 17:59:18,568 INFO Saved CSV for MRVE3.SA -> ..\\data\\historical\\MRVE3.SA.csv\n",
      "2025-12-11 17:59:18,784 INFO Saved 3716 rows for MULT3.SA -> ..\\data\\historical\\MULT3.SA.parquet\n",
      "2025-12-11 17:59:18,848 INFO Saved CSV for MULT3.SA -> ..\\data\\historical\\MULT3.SA.csv\n",
      "2025-12-11 17:59:19,069 INFO Saved 3716 rows for NATU3.SA -> ..\\data\\historical\\NATU3.SA.parquet\n",
      "2025-12-11 17:59:19,119 INFO Saved CSV for NATU3.SA -> ..\\data\\historical\\NATU3.SA.csv\n",
      "2025-12-11 17:59:19,342 INFO Saved 3716 rows for PCAR3.SA -> ..\\data\\historical\\PCAR3.SA.parquet\n",
      "2025-12-11 17:59:19,396 INFO Saved CSV for PCAR3.SA -> ..\\data\\historical\\PCAR3.SA.csv\n",
      "2025-12-11 17:59:19,623 INFO Saved 3716 rows for PETR3.SA -> ..\\data\\historical\\PETR3.SA.parquet\n",
      "2025-12-11 17:59:19,691 INFO Saved CSV for PETR3.SA -> ..\\data\\historical\\PETR3.SA.csv\n",
      "2025-12-11 17:59:19,914 INFO Saved 3716 rows for PETR4.SA -> ..\\data\\historical\\PETR4.SA.parquet\n",
      "2025-12-11 17:59:19,997 INFO Saved CSV for PETR4.SA -> ..\\data\\historical\\PETR4.SA.csv\n",
      "2025-12-11 17:59:20,215 INFO Saved 3716 rows for RECV3.SA -> ..\\data\\historical\\RECV3.SA.parquet\n",
      "2025-12-11 17:59:20,296 INFO Saved CSV for RECV3.SA -> ..\\data\\historical\\RECV3.SA.csv\n",
      "2025-12-11 17:59:20,517 INFO Saved 3716 rows for PRIO3.SA -> ..\\data\\historical\\PRIO3.SA.parquet\n",
      "2025-12-11 17:59:20,580 INFO Saved CSV for PRIO3.SA -> ..\\data\\historical\\PRIO3.SA.csv\n",
      "2025-12-11 17:59:20,800 INFO Saved 3716 rows for PETZ3.SA -> ..\\data\\historical\\PETZ3.SA.parquet\n",
      "2025-12-11 17:59:20,842 INFO Saved CSV for PETZ3.SA -> ..\\data\\historical\\PETZ3.SA.csv\n",
      "2025-12-11 17:59:21,075 INFO Saved 3716 rows for PSSA3.SA -> ..\\data\\historical\\PSSA3.SA.parquet\n",
      "2025-12-11 17:59:21,137 INFO Saved CSV for PSSA3.SA -> ..\\data\\historical\\PSSA3.SA.csv\n",
      "2025-12-11 17:59:21,375 INFO Saved 3716 rows for RADL3.SA -> ..\\data\\historical\\RADL3.SA.parquet\n",
      "2025-12-11 17:59:21,437 INFO Saved CSV for RADL3.SA -> ..\\data\\historical\\RADL3.SA.csv\n",
      "2025-12-11 17:59:21,667 INFO Saved 3716 rows for RAIZ4.SA -> ..\\data\\historical\\RAIZ4.SA.parquet\n",
      "2025-12-11 17:59:21,719 INFO Saved CSV for RAIZ4.SA -> ..\\data\\historical\\RAIZ4.SA.csv\n",
      "2025-12-11 17:59:21,942 INFO Saved 3716 rows for RAPT4.SA -> ..\\data\\historical\\RAPT4.SA.parquet\n",
      "2025-12-11 17:59:22,011 INFO Saved CSV for RAPT4.SA -> ..\\data\\historical\\RAPT4.SA.csv\n",
      "2025-12-11 17:59:22,231 INFO Saved 3716 rows for RDOR3.SA -> ..\\data\\historical\\RDOR3.SA.parquet\n",
      "2025-12-11 17:59:22,278 INFO Saved CSV for RDOR3.SA -> ..\\data\\historical\\RDOR3.SA.csv\n",
      "2025-12-11 17:59:23,481 INFO Processando batch 6/7 (download 15/15)\n",
      "2025-12-11 17:59:23,483 INFO yfinance.download attempt 1 for batch size 15\n",
      "2025-12-11 17:59:26,771 INFO Saved 3716 rows for RAIL3.SA -> ..\\data\\historical\\RAIL3.SA.parquet\n",
      "2025-12-11 17:59:26,917 INFO Saved CSV for RAIL3.SA -> ..\\data\\historical\\RAIL3.SA.csv\n",
      "2025-12-11 17:59:27,133 INFO Saved 3716 rows for SBSP3.SA -> ..\\data\\historical\\SBSP3.SA.parquet\n",
      "2025-12-11 17:59:27,186 INFO Saved CSV for SBSP3.SA -> ..\\data\\historical\\SBSP3.SA.csv\n",
      "2025-12-11 17:59:27,403 INFO Saved 3716 rows for SAPR11.SA -> ..\\data\\historical\\SAPR11.SA.parquet\n",
      "2025-12-11 17:59:27,452 INFO Saved CSV for SAPR11.SA -> ..\\data\\historical\\SAPR11.SA.csv\n",
      "2025-12-11 17:59:27,695 INFO Saved 3716 rows for SANB11.SA -> ..\\data\\historical\\SANB11.SA.parquet\n",
      "2025-12-11 17:59:27,757 INFO Saved CSV for SANB11.SA -> ..\\data\\historical\\SANB11.SA.csv\n",
      "2025-12-11 17:59:27,979 INFO Saved 3716 rows for SMTO3.SA -> ..\\data\\historical\\SMTO3.SA.parquet\n",
      "2025-12-11 17:59:28,058 INFO Saved CSV for SMTO3.SA -> ..\\data\\historical\\SMTO3.SA.csv\n",
      "2025-12-11 17:59:28,297 INFO Saved 3716 rows for CSNA3.SA -> ..\\data\\historical\\CSNA3.SA.parquet\n",
      "2025-12-11 17:59:28,367 INFO Saved CSV for CSNA3.SA -> ..\\data\\historical\\CSNA3.SA.csv\n",
      "2025-12-11 17:59:28,600 INFO Saved 3716 rows for SLCE3.SA -> ..\\data\\historical\\SLCE3.SA.parquet\n",
      "2025-12-11 17:59:28,667 INFO Saved CSV for SLCE3.SA -> ..\\data\\historical\\SLCE3.SA.csv\n",
      "2025-12-11 17:59:28,890 INFO Saved 3716 rows for SMFT3.SA -> ..\\data\\historical\\SMFT3.SA.parquet\n",
      "2025-12-11 17:59:28,945 INFO Saved CSV for SMFT3.SA -> ..\\data\\historical\\SMFT3.SA.csv\n",
      "2025-12-11 17:59:29,172 INFO Saved 3716 rows for SUZB3.SA -> ..\\data\\historical\\SUZB3.SA.parquet\n",
      "2025-12-11 17:59:29,241 INFO Saved CSV for SUZB3.SA -> ..\\data\\historical\\SUZB3.SA.csv\n",
      "2025-12-11 17:59:29,475 INFO Saved 3716 rows for TAEE11.SA -> ..\\data\\historical\\TAEE11.SA.parquet\n",
      "2025-12-11 17:59:29,536 INFO Saved CSV for TAEE11.SA -> ..\\data\\historical\\TAEE11.SA.csv\n",
      "2025-12-11 17:59:29,765 INFO Saved 3716 rows for VIVT3.SA -> ..\\data\\historical\\VIVT3.SA.parquet\n",
      "2025-12-11 17:59:29,821 INFO Saved CSV for VIVT3.SA -> ..\\data\\historical\\VIVT3.SA.csv\n",
      "2025-12-11 17:59:30,052 INFO Saved 3716 rows for TEND3.SA -> ..\\data\\historical\\TEND3.SA.parquet\n",
      "2025-12-11 17:59:30,125 INFO Saved CSV for TEND3.SA -> ..\\data\\historical\\TEND3.SA.csv\n",
      "2025-12-11 17:59:30,346 INFO Saved 3716 rows for TIMS3.SA -> ..\\data\\historical\\TIMS3.SA.parquet\n",
      "2025-12-11 17:59:30,411 INFO Saved CSV for TIMS3.SA -> ..\\data\\historical\\TIMS3.SA.csv\n",
      "2025-12-11 17:59:30,631 INFO Saved 3716 rows for TOTS3.SA -> ..\\data\\historical\\TOTS3.SA.parquet\n",
      "2025-12-11 17:59:30,696 INFO Saved CSV for TOTS3.SA -> ..\\data\\historical\\TOTS3.SA.csv\n",
      "2025-12-11 17:59:30,935 INFO Saved 3716 rows for UGPA3.SA -> ..\\data\\historical\\UGPA3.SA.parquet\n",
      "2025-12-11 17:59:30,996 INFO Saved CSV for UGPA3.SA -> ..\\data\\historical\\UGPA3.SA.csv\n",
      "2025-12-11 17:59:32,205 INFO Processando batch 7/7 (download 7/7)\n",
      "2025-12-11 17:59:32,207 INFO yfinance.download attempt 1 for batch size 7\n",
      "2025-12-11 17:59:33,141 INFO Saved 3716 rows for USIM5.SA -> ..\\data\\historical\\USIM5.SA.parquet\n",
      "2025-12-11 17:59:33,197 INFO Saved CSV for USIM5.SA -> ..\\data\\historical\\USIM5.SA.csv\n",
      "2025-12-11 17:59:33,411 INFO Saved 3716 rows for VALE3.SA -> ..\\data\\historical\\VALE3.SA.parquet\n",
      "2025-12-11 17:59:33,466 INFO Saved CSV for VALE3.SA -> ..\\data\\historical\\VALE3.SA.csv\n",
      "2025-12-11 17:59:33,681 INFO Saved 3716 rows for VAMO3.SA -> ..\\data\\historical\\VAMO3.SA.parquet\n",
      "2025-12-11 17:59:33,727 INFO Saved CSV for VAMO3.SA -> ..\\data\\historical\\VAMO3.SA.csv\n",
      "2025-12-11 17:59:33,973 INFO Saved 3716 rows for VBBR3.SA -> ..\\data\\historical\\VBBR3.SA.parquet\n",
      "2025-12-11 17:59:34,050 INFO Saved CSV for VBBR3.SA -> ..\\data\\historical\\VBBR3.SA.csv\n",
      "2025-12-11 17:59:34,266 INFO Saved 3716 rows for VIVA3.SA -> ..\\data\\historical\\VIVA3.SA.parquet\n",
      "2025-12-11 17:59:34,324 INFO Saved CSV for VIVA3.SA -> ..\\data\\historical\\VIVA3.SA.csv\n",
      "2025-12-11 17:59:34,540 INFO Saved 3716 rows for WEGE3.SA -> ..\\data\\historical\\WEGE3.SA.parquet\n",
      "2025-12-11 17:59:34,596 INFO Saved CSV for WEGE3.SA -> ..\\data\\historical\\WEGE3.SA.csv\n",
      "2025-12-11 17:59:34,836 INFO Saved 3716 rows for YDUQ3.SA -> ..\\data\\historical\\YDUQ3.SA.parquet\n",
      "2025-12-11 17:59:34,898 INFO Saved CSV for YDUQ3.SA -> ..\\data\\historical\\YDUQ3.SA.csv\n",
      "2025-12-11 17:59:36,118 INFO Resumo salvo em ..\\data\\historical\\download_summary_20251211T205936Z.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ticker status  rows                         saved_parquet  \\\n",
      "0    ALOS3.SA     ok  3716   ..\\data\\historical\\ALOS3.SA.parquet   \n",
      "1    ABEV3.SA     ok  3716   ..\\data\\historical\\ABEV3.SA.parquet   \n",
      "2    ANIM3.SA     ok  3716   ..\\data\\historical\\ANIM3.SA.parquet   \n",
      "3    ASAI3.SA     ok  3716   ..\\data\\historical\\ASAI3.SA.parquet   \n",
      "4    AURE3.SA     ok  3716   ..\\data\\historical\\AURE3.SA.parquet   \n",
      "5    AXIA3.SA     ok  3716   ..\\data\\historical\\AXIA3.SA.parquet   \n",
      "6    AXIA6.SA     ok  3716   ..\\data\\historical\\AXIA6.SA.parquet   \n",
      "7    AZZA3.SA     ok  3716   ..\\data\\historical\\AZZA3.SA.parquet   \n",
      "8    B3SA3.SA     ok  3716   ..\\data\\historical\\B3SA3.SA.parquet   \n",
      "9    BBSE3.SA     ok  3716   ..\\data\\historical\\BBSE3.SA.parquet   \n",
      "10   BBDC3.SA     ok  3716   ..\\data\\historical\\BBDC3.SA.parquet   \n",
      "11   BBDC4.SA     ok  3716   ..\\data\\historical\\BBDC4.SA.parquet   \n",
      "12   BRAP4.SA     ok  3716   ..\\data\\historical\\BRAP4.SA.parquet   \n",
      "13   BBAS3.SA     ok  3716   ..\\data\\historical\\BBAS3.SA.parquet   \n",
      "14   BRKM5.SA     ok  3716   ..\\data\\historical\\BRKM5.SA.parquet   \n",
      "15   BRAV3.SA     ok  3716   ..\\data\\historical\\BRAV3.SA.parquet   \n",
      "16  BPAC11.SA     ok  3716  ..\\data\\historical\\BPAC11.SA.parquet   \n",
      "17   CXSE3.SA     ok  3716   ..\\data\\historical\\CXSE3.SA.parquet   \n",
      "18   CEAB3.SA     ok  3716   ..\\data\\historical\\CEAB3.SA.parquet   \n",
      "19   CMIG4.SA     ok  3716   ..\\data\\historical\\CMIG4.SA.parquet   \n",
      "20   COGN3.SA     ok  3716   ..\\data\\historical\\COGN3.SA.parquet   \n",
      "21   CSMG3.SA     ok  3716   ..\\data\\historical\\CSMG3.SA.parquet   \n",
      "22   CPLE3.SA     ok  3716   ..\\data\\historical\\CPLE3.SA.parquet   \n",
      "23   CPLE5.SA     ok  3716   ..\\data\\historical\\CPLE5.SA.parquet   \n",
      "24   CSAN3.SA     ok  3716   ..\\data\\historical\\CSAN3.SA.parquet   \n",
      "25   CPFE3.SA     ok  3716   ..\\data\\historical\\CPFE3.SA.parquet   \n",
      "26   CMIN3.SA     ok  3716   ..\\data\\historical\\CMIN3.SA.parquet   \n",
      "27   CURY3.SA     ok  3716   ..\\data\\historical\\CURY3.SA.parquet   \n",
      "28   CVCB3.SA     ok  3716   ..\\data\\historical\\CVCB3.SA.parquet   \n",
      "29   CYRE3.SA     ok  3716   ..\\data\\historical\\CYRE3.SA.parquet   \n",
      "30   DIRR3.SA     ok  3716   ..\\data\\historical\\DIRR3.SA.parquet   \n",
      "31   ECOR3.SA     ok  3716   ..\\data\\historical\\ECOR3.SA.parquet   \n",
      "32   EMBJ3.SA     ok  3716   ..\\data\\historical\\EMBJ3.SA.parquet   \n",
      "33  ENGI11.SA     ok  3716  ..\\data\\historical\\ENGI11.SA.parquet   \n",
      "34   ENEV3.SA     ok  3716   ..\\data\\historical\\ENEV3.SA.parquet   \n",
      "35   EGIE3.SA     ok  3716   ..\\data\\historical\\EGIE3.SA.parquet   \n",
      "36   EQTL3.SA     ok  3716   ..\\data\\historical\\EQTL3.SA.parquet   \n",
      "37   EZTC3.SA     ok  3716   ..\\data\\historical\\EZTC3.SA.parquet   \n",
      "38   FLRY3.SA     ok  3716   ..\\data\\historical\\FLRY3.SA.parquet   \n",
      "39   GGBR4.SA     ok  3716   ..\\data\\historical\\GGBR4.SA.parquet   \n",
      "40   GOAU4.SA     ok  3716   ..\\data\\historical\\GOAU4.SA.parquet   \n",
      "41   GGPS3.SA     ok  3716   ..\\data\\historical\\GGPS3.SA.parquet   \n",
      "42   GMAT3.SA     ok  3716   ..\\data\\historical\\GMAT3.SA.parquet   \n",
      "43   HAPV3.SA     ok  3716   ..\\data\\historical\\HAPV3.SA.parquet   \n",
      "44   HYPE3.SA     ok  3716   ..\\data\\historical\\HYPE3.SA.parquet   \n",
      "45  IGTI11.SA     ok  3716  ..\\data\\historical\\IGTI11.SA.parquet   \n",
      "46   INTB3.SA     ok  3716   ..\\data\\historical\\INTB3.SA.parquet   \n",
      "47   IRBR3.SA     ok  3716   ..\\data\\historical\\IRBR3.SA.parquet   \n",
      "48   ISAE4.SA     ok  3716   ..\\data\\historical\\ISAE4.SA.parquet   \n",
      "49   ITSA4.SA     ok  3716   ..\\data\\historical\\ITSA4.SA.parquet   \n",
      "\n",
      "                           saved_csv  \n",
      "0    ..\\data\\historical\\ALOS3.SA.csv  \n",
      "1    ..\\data\\historical\\ABEV3.SA.csv  \n",
      "2    ..\\data\\historical\\ANIM3.SA.csv  \n",
      "3    ..\\data\\historical\\ASAI3.SA.csv  \n",
      "4    ..\\data\\historical\\AURE3.SA.csv  \n",
      "5    ..\\data\\historical\\AXIA3.SA.csv  \n",
      "6    ..\\data\\historical\\AXIA6.SA.csv  \n",
      "7    ..\\data\\historical\\AZZA3.SA.csv  \n",
      "8    ..\\data\\historical\\B3SA3.SA.csv  \n",
      "9    ..\\data\\historical\\BBSE3.SA.csv  \n",
      "10   ..\\data\\historical\\BBDC3.SA.csv  \n",
      "11   ..\\data\\historical\\BBDC4.SA.csv  \n",
      "12   ..\\data\\historical\\BRAP4.SA.csv  \n",
      "13   ..\\data\\historical\\BBAS3.SA.csv  \n",
      "14   ..\\data\\historical\\BRKM5.SA.csv  \n",
      "15   ..\\data\\historical\\BRAV3.SA.csv  \n",
      "16  ..\\data\\historical\\BPAC11.SA.csv  \n",
      "17   ..\\data\\historical\\CXSE3.SA.csv  \n",
      "18   ..\\data\\historical\\CEAB3.SA.csv  \n",
      "19   ..\\data\\historical\\CMIG4.SA.csv  \n",
      "20   ..\\data\\historical\\COGN3.SA.csv  \n",
      "21   ..\\data\\historical\\CSMG3.SA.csv  \n",
      "22   ..\\data\\historical\\CPLE3.SA.csv  \n",
      "23   ..\\data\\historical\\CPLE5.SA.csv  \n",
      "24   ..\\data\\historical\\CSAN3.SA.csv  \n",
      "25   ..\\data\\historical\\CPFE3.SA.csv  \n",
      "26   ..\\data\\historical\\CMIN3.SA.csv  \n",
      "27   ..\\data\\historical\\CURY3.SA.csv  \n",
      "28   ..\\data\\historical\\CVCB3.SA.csv  \n",
      "29   ..\\data\\historical\\CYRE3.SA.csv  \n",
      "30   ..\\data\\historical\\DIRR3.SA.csv  \n",
      "31   ..\\data\\historical\\ECOR3.SA.csv  \n",
      "32   ..\\data\\historical\\EMBJ3.SA.csv  \n",
      "33  ..\\data\\historical\\ENGI11.SA.csv  \n",
      "34   ..\\data\\historical\\ENEV3.SA.csv  \n",
      "35   ..\\data\\historical\\EGIE3.SA.csv  \n",
      "36   ..\\data\\historical\\EQTL3.SA.csv  \n",
      "37   ..\\data\\historical\\EZTC3.SA.csv  \n",
      "38   ..\\data\\historical\\FLRY3.SA.csv  \n",
      "39   ..\\data\\historical\\GGBR4.SA.csv  \n",
      "40   ..\\data\\historical\\GOAU4.SA.csv  \n",
      "41   ..\\data\\historical\\GGPS3.SA.csv  \n",
      "42   ..\\data\\historical\\GMAT3.SA.csv  \n",
      "43   ..\\data\\historical\\HAPV3.SA.csv  \n",
      "44   ..\\data\\historical\\HYPE3.SA.csv  \n",
      "45  ..\\data\\historical\\IGTI11.SA.csv  \n",
      "46   ..\\data\\historical\\INTB3.SA.csv  \n",
      "47   ..\\data\\historical\\IRBR3.SA.csv  \n",
      "48   ..\\data\\historical\\ISAE4.SA.csv  \n",
      "49   ..\\data\\historical\\ITSA4.SA.csv  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 17:59:38,825 INFO Combined saved to ..\\data\\historical\\all_histories.parquet (rows=360452)\n",
      "2025-12-11 17:59:42,805 INFO Combined CSV saved to ..\\data\\historical\\all_histories.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined rows: 360452\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Dict, Iterable, List, Optional, Set\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "HIST_DIR = os.path.join(\"..\", \"data\", \"historical\")\n",
    "os.makedirs(HIST_DIR, exist_ok=True)\n",
    "\n",
    "DEFAULT_START = \"2011-01-01\"\n",
    "BATCH_SIZE = 15\n",
    "MAX_ATTEMPTS = 4          \n",
    "SLEEP_BETWEEN_BATCHES = 1  \n",
    "SLEEP_BETWEEN_TICKERS = 0.2\n",
    "\n",
    "def ticker_exists_local(ticker: str) -> bool:\n",
    "    \"\"\"Verifica se jÃ¡ existe parquet salvo para ticker (usa para pular downloads)\"\"\"\n",
    "    path = os.path.join(HIST_DIR, f\"{ticker}.parquet\")\n",
    "    return os.path.isfile(path)\n",
    "\n",
    "def save_history_df(ticker: str, df: pd.DataFrame, save_csv: bool = True):\n",
    "    \"\"\"Salva DataFrame em parquet e opcionalmente em CSV. Garante coluna 'date' se Ã­ndice for DatetimeIndex.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        raise ValueError(\"DataFrame nulo ou vazio\")\n",
    "    df = df.copy()\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index.name = \"date\"\n",
    "        df = df.reset_index()\n",
    "    \n",
    "    out_parquet = os.path.join(HIST_DIR, f\"{ticker}.parquet\")\n",
    "    out_csv = os.path.join(HIST_DIR, f\"{ticker}.csv\")\n",
    "    try:\n",
    "        df.to_parquet(out_parquet, index=False)\n",
    "        logging.info(\"Saved %s rows for %s -> %s\", len(df), ticker, out_parquet)\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Erro salvando parquet para %s: %s\", ticker, e)\n",
    "        raise\n",
    "    if save_csv:\n",
    "        try:\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')\n",
    "            df.to_csv(out_csv, index=False)\n",
    "            logging.info(\"Saved CSV for %s -> %s\", ticker, out_csv)\n",
    "        except Exception as e:\n",
    "            logging.exception(\"Erro salvando CSV para %s: %s\", ticker, e)\n",
    "\n",
    "def download_batch(batch: List[str], start: str = DEFAULT_START, threads: bool = True) -> Dict[str, Optional[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Tenta baixar um batch de tickers via yfinance.download.\n",
    "    Retorna dict ticker -> DataFrame or None (se falhou).\n",
    "    \"\"\"\n",
    "    joined = \" \".join(batch)\n",
    "    attempt = 0\n",
    "    last_exc = None\n",
    "    while attempt < MAX_ATTEMPTS:\n",
    "        try:\n",
    "            logging.info(\"yfinance.download attempt %d for batch size %d\", attempt+1, len(batch))\n",
    "            data = yf.download(tickers=joined, start=start, progress=False, threads=threads, group_by='ticker', auto_adjust=False, actions=True)\n",
    "            result = {}\n",
    "            if isinstance(data, pd.DataFrame) and isinstance(data.columns, pd.MultiIndex):\n",
    "                for ticker in batch:\n",
    "                    if ticker in data.columns.get_level_values(0):\n",
    "                        df_t = data[ticker].copy()\n",
    "                        result[ticker] = df_t\n",
    "                    else:\n",
    "                        try:\n",
    "                            single = yf.download(ticker, start=start, progress=False, actions=True)\n",
    "                            result[ticker] = single if not single.empty else None\n",
    "                        except Exception:\n",
    "                            result[ticker] = None\n",
    "            else:\n",
    "                for ticker in batch:\n",
    "                    try:\n",
    "                        df_t = yf.download(ticker, start=start, progress=False, actions=True)\n",
    "                        result[ticker] = df_t if not df_t.empty else None\n",
    "                    except Exception:\n",
    "                        result[ticker] = None\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "            logging.warning(\"Erro no yfinance.download (attempt %d): %s\", attempt+1, str(e))\n",
    "            attempt += 1\n",
    "            time.sleep(2 ** attempt) \n",
    "    logging.error(\"Todas tentativas falharam para batch (%s). Ãšltimo erro: %s\", joined, last_exc)\n",
    "    result = {}\n",
    "    for ticker in batch:\n",
    "        try:\n",
    "            df_t = yf.download(ticker, start=start, progress=False, actions=True)\n",
    "            result[ticker] = df_t if not df_t.empty else None\n",
    "        except Exception as e:\n",
    "            logging.warning(\"Fallback individual falhou para %s: %s\", ticker, e)\n",
    "            result[ticker] = None\n",
    "    return result\n",
    "\n",
    "def download_all_histories(tickers: List[str], start: str = DEFAULT_START, force: bool = False, save_summary: bool = True, save_csv_per_ticker: bool = True):\n",
    "    \"\"\"\n",
    "    Processo principal: recebe lista de tickers (strings), baixa histÃ³ricos e salva parquet + csv por ticker.\n",
    "    - force: se True, re-baixa mesmo que arquivo exista.\n",
    "    - save_csv_per_ticker: se True salva um CSV para cada ticker (alÃ©m do parquet).\n",
    "    - retorna um DataFrame resumo com status por ticker.\n",
    "    \"\"\"\n",
    "    os.makedirs(HIST_DIR, exist_ok=True)\n",
    "    tickers = [t for t in tickers if isinstance(t, str) and t.strip()]\n",
    "    tickers = list(dict.fromkeys(tickers))\n",
    "    summary = []\n",
    "    for i in range(0, len(tickers), BATCH_SIZE):\n",
    "        batch = tickers[i:i+BATCH_SIZE]\n",
    "        to_download = [t for t in batch if force or not ticker_exists_local(t)]\n",
    "        if not to_download:\n",
    "            logging.info(\"Batch %d: todos jÃ¡ existem localmente â€” pulando.\", i//BATCH_SIZE+1)\n",
    "            for t in batch:\n",
    "                summary.append({\n",
    "                    \"ticker\": t,\n",
    "                    \"status\": \"skipped_local\",\n",
    "                    \"rows\": None,\n",
    "                    \"saved_parquet\": os.path.join(HIST_DIR, f\"{t}.parquet\") if ticker_exists_local(t) else None,\n",
    "                    \"saved_csv\": os.path.join(HIST_DIR, f\"{t}.csv\") if os.path.exists(os.path.join(HIST_DIR, f\"{t}.csv\")) else None\n",
    "                })\n",
    "            continue\n",
    "\n",
    "        logging.info(\"Processando batch %d/%d (download %d/%d)\", i//BATCH_SIZE+1, (len(tickers)+BATCH_SIZE-1)//BATCH_SIZE, len(to_download), len(batch))\n",
    "        results = download_batch(to_download, start=start)\n",
    "        for t in batch:\n",
    "            df_t = results.get(t) if t in results else None\n",
    "            if df_t is None or (isinstance(df_t, pd.DataFrame) and df_t.empty):\n",
    "                logging.warning(\"Nenhum dado para %s em batch; tentativa isolada...\", t)\n",
    "                try:\n",
    "                    single = yf.download(t, start=start, progress=False, actions=True)\n",
    "                    df_t = single if not single.empty else None\n",
    "                except Exception:\n",
    "                    df_t = None\n",
    "            if df_t is None or df_t.empty:\n",
    "                logging.error(\"Falha obtendo dados para %s\", t)\n",
    "                summary.append({\"ticker\": t, \"status\": \"failed\", \"rows\": 0, \"saved_parquet\": None, \"saved_csv\": None})\n",
    "            else:\n",
    "                try:\n",
    "                    save_history_df(t, df_t, save_csv=save_csv_per_ticker)\n",
    "                    summary.append({\n",
    "                        \"ticker\": t,\n",
    "                        \"status\": \"ok\",\n",
    "                        \"rows\": len(df_t),\n",
    "                        \"saved_parquet\": os.path.join(HIST_DIR, f\"{t}.parquet\"),\n",
    "                        \"saved_csv\": os.path.join(HIST_DIR, f\"{t}.csv\") if save_csv_per_ticker else None\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    logging.exception(\"Erro salvando para %s: %s\", t, e)\n",
    "                    summary.append({\"ticker\": t, \"status\": \"save_error\", \"rows\": len(df_t) if isinstance(df_t, pd.DataFrame) else None, \"saved_parquet\": None, \"saved_csv\": None})\n",
    "            time.sleep(SLEEP_BETWEEN_TICKERS)\n",
    "        time.sleep(SLEEP_BETWEEN_BATCHES)\n",
    "\n",
    "    df_summary = pd.DataFrame(summary)\n",
    "    if save_summary:\n",
    "        ts = datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "        summary_path = os.path.join(HIST_DIR, f\"download_summary_{ts}.csv\")\n",
    "        df_summary.to_csv(summary_path, index=False)\n",
    "        logging.info(\"Resumo salvo em %s\", summary_path)\n",
    "    return df_summary\n",
    "\n",
    "def combine_all_to_single_parquet(out_path: str = os.path.join(HIST_DIR, \"all_histories.parquet\"), out_csv: Optional[str] = os.path.join(HIST_DIR, \"all_histories.csv\"), tickers: Optional[List[str]] = None):\n",
    "    \"\"\"\n",
    "    LÃª todos os parquets em HIST_DIR (ou tickers list) e concatena em formato long:\n",
    "    columns: ['ticker','date', 'Open','High','Low','Close','Adj Close','Volume', 'Dividends','Stock Splits']\n",
    "    Salva em parquet e opcionalmente em csv.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    if tickers:\n",
    "        files = [os.path.join(HIST_DIR, f\"{t}.parquet\") for t in tickers if os.path.exists(os.path.join(HIST_DIR, f\"{t}.parquet\"))]\n",
    "    else:\n",
    "        files = [os.path.join(HIST_DIR, f) for f in os.listdir(HIST_DIR) if f.endswith(\".parquet\")]\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = pd.read_parquet(f)\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "            fname = os.path.basename(f).replace(\".parquet\",\"\")\n",
    "            if 'ticker' not in df.columns:\n",
    "                df.insert(0, 'ticker', fname)\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            logging.warning(\"Erro lendo %s: %s\", f, e)\n",
    "    if not dfs:\n",
    "        raise RuntimeError(\"Nenhum parquet encontrado para combinar.\")\n",
    "    big = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "    big.to_parquet(out_path, index=False)\n",
    "    logging.info(\"Combined saved to %s (rows=%d)\", out_path, len(big))\n",
    "    if out_csv:\n",
    "        try:\n",
    "            if 'date' in big.columns:\n",
    "                big['date'] = pd.to_datetime(big['date']).dt.strftime('%Y-%m-%d')\n",
    "            big.to_csv(out_csv, index=False)\n",
    "            logging.info(\"Combined CSV saved to %s\", out_csv)\n",
    "        except Exception as e:\n",
    "            logging.exception(\"Erro salvando combined CSV: %s\", e)\n",
    "    return big\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tickers_file = os.path.join(\"..\", \"data\", \"tickers_ibrx100_full.csv\")\n",
    "    if os.path.exists(tickers_file):\n",
    "        df = pd.read_csv(tickers_file)\n",
    "        if 'Ticker' in df.columns:\n",
    "            tickers = df['Ticker'].dropna().astype(str).tolist()\n",
    "        else:\n",
    "            tickers = df.iloc[:,0].dropna().astype(str).tolist()\n",
    "    else:\n",
    "        raise RuntimeError(f\"NÃ£o encontrou {tickers_file}. Coloque seu CSV de tickers na pasta 'aurum/data/' ou edite este script.\")\n",
    "\n",
    "    def normalize(t):\n",
    "        t = str(t).strip().upper()\n",
    "        if not t.endswith(\".SA\"):\n",
    "            t = t.replace(\".SA\",\"\") + \".SA\"\n",
    "        return t\n",
    "    tickers = [normalize(t) for t in tickers]\n",
    "    print(\"Tickers a baixar:\", len(tickers), tickers[:10])\n",
    "\n",
    "    summary_df = download_all_histories(tickers, start=DEFAULT_START, force=False, save_summary=True, save_csv_per_ticker=True)\n",
    "    print(summary_df.head(50))\n",
    "\n",
    "    combined = combine_all_to_single_parquet()\n",
    "    print(\"Combined rows:\", len(combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a1e096",
   "metadata": {},
   "source": [
    "#### ðŸ“‹ DescriÃ§Ã£o das colunas do dataset histÃ³rico\n",
    "\n",
    "| Coluna         | Tipo        | Breve descriÃ§Ã£o |\n",
    "|---------------:|:-----------:|:----------------|\n",
    "| **date**         | **date**      | Data da observaÃ§Ã£o no formato ISO (**YYYY-MM-DD**). Use como Ã­ndice temporal para sÃ©ries. |\n",
    "| **Open**         | **float**     | PreÃ§o de abertura do pregÃ£o (primeira transaÃ§Ã£o considerada naquele dia). |\n",
    "| **High**         | **float**     | PreÃ§o mÃ¡ximo registrado durante o pregÃ£o. |\n",
    "| **Low**          | **float**     | PreÃ§o mÃ­nimo registrado durante o pregÃ£o. |\n",
    "| **Close**        | **float**     | PreÃ§o de fechamento (Ãºltima transaÃ§Ã£o do dia). NÃ£o considera ajustes por eventos corporativos. |\n",
    "| **Adj Close**    | **float**     | PreÃ§o de fechamento **ajustado** por splits e dividendos â€” use para cÃ¡lculo de retornos total/consistentes em sÃ©ries histÃ³ricas. |\n",
    "| **Volume**       | **int**       | Quantidade de aÃ§Ãµes negociadas no dia (unidades). |\n",
    "| **Dividends**    | **float**     | Valor do dividendo pago por aÃ§Ã£o na data (se houver). Geralmente em moeda local (ex.: BRL para B3). |\n",
    "| **Stock Splits** | **float**     | Fator de desdobramento/agrupamento (ex.: **2.0** â†’ split 2-por-1). Zero ou **0.0** quando nÃ£o houve evento. |\n",
    "\n",
    "**Notas rÃ¡pidas**\n",
    "- **Adj Close** Ã© a coluna recomendada para backtests e cÃ¡lculo de retornos contÃ­nuos (corrige preÃ§os para manter consistÃªncia apÃ³s eventos corporativos).  \n",
    "- Converta **date** para **datetime** e defina como Ã­ndice para operaÃ§Ãµes de sÃ©rie temporal (**df['date'] = pd.to_datetime(df['date']); df.set_index('date', inplace=True)**).  \n",
    "- Verifique a unidade/moeda conforme a fonte (normalmente moeda local do mercado, ex.: BRL para B3).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb31ca5f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Š DocumentaÃ§Ã£o: Ferramenta de AnÃ¡lise ExploratÃ³ria de HistÃ³rico\n",
    "\n",
    "#### ðŸ“ VisÃ£o Geral\n",
    "\n",
    "Este script (**analyze_dataframe**) atua como uma **ferramenta de diagnÃ³stico** dentro do pipeline de dados do **Projeto Aurum**.\n",
    "\n",
    "Seu objetivo principal Ã© validar a integridade do dataset mestre de preÃ§os (**all_histories.csv**) logo apÃ³s o processo de unificaÃ§Ã£o dos dados histÃ³ricos. Ele serve para garantir que o arquivo CSV consolidado foi gerado corretamente antes que ele seja utilizado em etapas crÃ­ticas como o cÃ¡lculo de indicadores ou backtesting.\n",
    "\n",
    "\n",
    "\n",
    "### âš™ï¸ Funcionalidades Principais\n",
    "\n",
    "O script executa uma auditoria sequencial no arquivo de dados:\n",
    "\n",
    "1.  **VerificaÃ§Ã£o de ExistÃªncia:** Checa se o arquivo existe no caminho especificado antes de tentar carregar, evitando erros em tempo de execuÃ§Ã£o.\n",
    "2.  **Carregamento Otimizado:** LÃª o CSV convertendo automaticamente a coluna de datas para o formato **datetime** (essencial para sÃ©ries temporais).\n",
    "3.  **Auditoria de Estrutura:**\n",
    "      * Mostra tipos de dados (**dtypes**).\n",
    "      * Identifica valores nulos (**NaN**) por coluna.\n",
    "      * Exibe estatÃ­sticas descritivas (MÃ©dia, MÃ­nimo, MÃ¡ximo, Desvio PadrÃ£o) para preÃ§os e volumes.\n",
    "4.  **ValidaÃ§Ã£o de Tickers:** Conta e lista os ativos Ãºnicos presentes no arquivo para garantir que a consolidaÃ§Ã£o abrangeu todo o universo (ex: IBRX-100).\n",
    "\n",
    "\n",
    "### ðŸ” Detalhamento do CÃ³digo\n",
    "\n",
    "#### Bibliotecas Utilizadas\n",
    "\n",
    "  * **pandas**: Motor principal para manipulaÃ§Ã£o e anÃ¡lise tabular.\n",
    "  * **os**: Utilizado para verificar caminhos e existÃªncia de arquivos no sistema operacional.\n",
    "  * **io**: Utilizado especificamente (**io.StringIO**) para capturar o output da funÃ§Ã£o **df.info()**, que nativamente imprime no console, permitindo manipulÃ¡-lo como string se necessÃ¡rio.\n",
    "\n",
    "### FunÃ§Ãµes\n",
    "\n",
    "#### 1\\. **print_header(title)**\n",
    "\n",
    "Uma funÃ§Ã£o auxiliar estÃ©tica.\n",
    "\n",
    "  * **Objetivo:** Criar separadores visuais no log do console.\n",
    "  * **Utilidade:** Facilita a leitura rÃ¡pida do relatÃ³rio quando executado em terminais com muito texto.\n",
    "\n",
    "#### 2\\. **analyze_dataframe(file_path)**\n",
    "\n",
    "A funÃ§Ã£o *core* do script. Executa os seguintes passos:\n",
    "\n",
    "  * **Tratamento de Erros:** Envolve o carregamento (**pd.read_csv**) em um bloco **try-except** para capturar arquivos corrompidos ou mal formatados.\n",
    "  * **Parsing de Datas:** Usa o argumento **parse_dates=['date']**. Isso Ã© crucial para o Aurum, pois permite operaÃ§Ãµes de data (ex: filtrar Ãºltimos 5 anos) sem precisar converter strings manualmente depois.\n",
    "  * **DetecÃ§Ã£o de Nulos:**\n",
    "    **python\n",
    "    null_counts = df.isnull().sum()\n",
    "    if null_counts.sum() == 0: ...\n",
    "    **\n",
    "    Isso Ã© vital para dados financeiros. **Nulls** em preÃ§os de fechamento quebram backtests. Este bloco avisa imediatamente se hÃ¡ buracos nos dados.\n",
    "  * **VerificaÃ§Ã£o de Tickers:** Confirma quantos ativos Ãºnicos existem. Se vocÃª espera 100 aÃ§Ãµes (IBRX-100) e o script retorna 50, vocÃª sabe que houve erro na etapa de coleta.\n",
    "\n",
    "\n",
    "#### ðŸš€ Como Utilizar\n",
    "\n",
    "1.  Certifique-se de que o arquivo consolidado existe no diretÃ³rio:\n",
    "    **../data/historical/all_histories.csv**\n",
    "2.  Execute o script via terminal:\n",
    "    ```bash\n",
    "    python analise_historico.py\n",
    "    ```\n",
    "3.  **InterpretaÃ§Ã£o do Output:**\n",
    "      * **Sucesso:** O script imprimirÃ¡ as 5 primeiras linhas, o resumo de memÃ³ria e confirmarÃ¡ \"NÃ£o hÃ¡ valores nulos\".\n",
    "      * **AtenÃ§Ã£o:** Se houver valores nulos em `Close` ou `Adj Close`, vocÃª deve revisitar o script de limpeza de dados.\n",
    "\n",
    "\n",
    "### ðŸ”— Contexto no Projeto Aurum\n",
    "\n",
    "Dentro da arquitetura do Aurum, este script se encaixa na etapa de **ValidaÃ§Ã£o de Qualidade de Dados (Data Quality)**.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[Coleta Yahoo Finance] --> B[Dataset Bruto]\n",
    "    B --> C[Limpeza e UnificaÃ§Ã£o]\n",
    "    C --> D[(all_histories.csv)]\n",
    "    D --> E{Script de AnÃ¡lise}\n",
    "    E -- OK --> F[CÃ¡lculo de Indicadores]\n",
    "    E -- Erro --> G[Revisar Coleta]\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "*DocumentaÃ§Ã£o gerada automaticamente para o LaboratÃ³rio Quantitativo Aurum.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd5bc8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando AnÃ¡lise do Arquivo: ../data/historical/all_histories.csv ---\n",
      "\n",
      "[SUCESSO] Arquivo carregado. Total de 360452 linhas e 10 colunas.\n",
      "\n",
      "======================================================================\n",
      " 1. AMOSTRA DOS DADOS (PRIMEIRAS 5 LINHAS) \n",
      "======================================================================\n",
      "     ticker       date      Open      High       Low     Close  Adj Close     Volume  Dividends  Stock Splits\n",
      "0  ABEV3.SA 2011-01-03  8.632311  8.728203  8.630313  8.690246   4.694567   576145.0        0.0           0.0\n",
      "1  ABEV3.SA 2011-01-04  8.784141  8.784141  8.630313  8.692244   4.695647   328368.0        0.0           0.0\n",
      "2  ABEV3.SA 2011-01-05  8.672266  8.718215  8.448517  8.530425   4.608231   299836.0        0.0           0.0\n",
      "3  ABEV3.SA 2011-01-06  8.560392  8.590358  8.396576  8.450515   4.565063   731319.0        0.0           0.0\n",
      "4  ABEV3.SA 2011-01-07  8.450515  8.550403  8.368607  8.416553   4.546715  1090222.0        0.0           0.0\n",
      "\n",
      "======================================================================\n",
      " 2. INFORMAÃ‡Ã•ES DO DATAFRAME (TIPOS DE COLUNA E NULOS) \n",
      "======================================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 360452 entries, 0 to 360451\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count   Dtype         \n",
      "---  ------        --------------   -----         \n",
      " 0   ticker        360452 non-null  object        \n",
      " 1   date          360452 non-null  datetime64[ns]\n",
      " 2   Open          288138 non-null  float64       \n",
      " 3   High          288138 non-null  float64       \n",
      " 4   Low           288138 non-null  float64       \n",
      " 5   Close         288138 non-null  float64       \n",
      " 6   Adj Close     288138 non-null  float64       \n",
      " 7   Volume        288138 non-null  float64       \n",
      " 8   Dividends     288138 non-null  float64       \n",
      " 9   Stock Splits  288138 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(8), object(1)\n",
      "memory usage: 27.5+ MB\n",
      "\n",
      "\n",
      "======================================================================\n",
      " 3. RESUMO DE VALORES NULOS POR COLUNA \n",
      "======================================================================\n",
      "Open            72314\n",
      "High            72314\n",
      "Low             72314\n",
      "Close           72314\n",
      "Adj Close       72314\n",
      "Volume          72314\n",
      "Dividends       72314\n",
      "Stock Splits    72314\n",
      "\n",
      "======================================================================\n",
      " 4. ESTATÃSTICAS DESCRITIVAS (COLUNAS NUMÃ‰RICAS) \n",
      "======================================================================\n",
      "                                date           Open           High            Low          Close      Adj Close        Volume      Dividends   Stock Splits\n",
      "count                         360452  288138.000000  288138.000000  288138.000000  288138.000000  288138.000000  2.881380e+05  288138.000000  288138.000000\n",
      "mean   2018-06-28 00:18:59.289559040      20.612407      20.933691      20.275248      20.602017      16.726627  7.383679e+06       0.003374       0.001235\n",
      "min              2011-01-03 00:00:00       0.139000       0.150000       0.137000       0.138000       0.137779  0.000000e+00       0.000000       0.000000\n",
      "25%              2014-10-01 18:00:00       8.813333       8.963028       8.660000       8.807338       5.793751  1.320900e+06       0.000000       0.000000\n",
      "50%              2018-06-23 12:00:00      15.144137      15.390000      14.890909      15.137142      10.711126  3.334800e+06       0.000000       0.000000\n",
      "75%              2022-03-28 06:00:00      24.240000      24.592817      23.866667      24.230000      19.137570  8.100372e+06       0.000000       0.000000\n",
      "max              2025-12-11 00:00:00    1025.463867    1037.195312    1007.751709    1032.824707    1028.702637  6.989506e+08      17.969149      50.000000\n",
      "std                              NaN      33.703948      34.249572      33.110919      33.698367      33.068241  1.236844e+07       0.077229       0.113659\n",
      "\n",
      "======================================================================\n",
      " 5. ANÃLISE DA COLUNA 'TICKER' \n",
      "======================================================================\n",
      "Total de tickers Ãºnicos encontrados: 97\n",
      "Amostra de tickers: ['ABEV3.SA' 'ALOS3.SA' 'ANIM3.SA' 'ASAI3.SA' 'AURE3.SA' 'AXIA3.SA'\n",
      " 'AXIA6.SA' 'AZZA3.SA' 'B3SA3.SA' 'BBAS3.SA']...\n",
      "\n",
      "======================================================================\n",
      "--- ANÃLISE CONCLUÃDA ---\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "\n",
    "def print_header(title):\n",
    "    \"\"\"\n",
    "    FunÃ§Ã£o auxiliar para imprimir um cabeÃ§alho formatado no log.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\" {title.upper()} \")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "def analyze_dataframe(file_path):\n",
    "    \"\"\"\n",
    "    Carrega e analisa um DataFrame de histÃ³rico de aÃ§Ãµes.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"--- Iniciando AnÃ¡lise do Arquivo: {file_path} ---\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print_header(\"[ERRO] Arquivo nÃ£o encontrado\")\n",
    "        print(f\"O arquivo no caminho '{file_path}' nÃ£o foi localizado.\")\n",
    "        print(\"Por favor, verifique se o caminho estÃ¡ correto e a pasta 'historical' existe.\")\n",
    "        print(\"=\" * 70)\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, parse_dates=['date'])\n",
    "        print(f\"\\n[SUCESSO] Arquivo carregado. Total de {len(df)} linhas e {len(df.columns)} colunas.\")\n",
    "    except Exception as e:\n",
    "        print_header(\"[ERRO] Falha ao carregar o arquivo\")\n",
    "        print(f\"Ocorreu um erro ao tentar ler o arquivo CSV: {e}\")\n",
    "        print(\"=\" * 70)\n",
    "        return\n",
    "\n",
    "    print_header(\"1. Amostra dos Dados (Primeiras 5 Linhas)\")\n",
    "    print(df.head().to_string())\n",
    "\n",
    "    print_header(\"2. InformaÃ§Ãµes do DataFrame (Tipos de Coluna e Nulos)\")\n",
    "    buffer = io.StringIO()\n",
    "    df.info(buf=buffer)\n",
    "    info_str = buffer.getvalue()\n",
    "    print(info_str)\n",
    "\n",
    "    print_header(\"3. Resumo de Valores Nulos por Coluna\")\n",
    "    null_counts = df.isnull().sum()\n",
    "    \n",
    "    if null_counts.sum() == 0:\n",
    "        print(\"Ã“timo! NÃ£o hÃ¡ valores nulos em nenhuma coluna.\")\n",
    "    else:\n",
    "        print(null_counts[null_counts > 0].to_string())\n",
    "\n",
    "    print_header(\"4. EstatÃ­sticas Descritivas (Colunas NumÃ©ricas)\")\n",
    "    try:\n",
    "        print(df.describe().to_string())\n",
    "    except Exception as e:\n",
    "        print(f\"NÃ£o foi possÃ­vel calcular estatÃ­sticas descritivas: {e}\")\n",
    "\n",
    "    print_header(\"5. AnÃ¡lise da Coluna 'ticker'\")\n",
    "    if 'ticker' in df.columns:\n",
    "        unique_tickers = df['ticker'].unique()\n",
    "        num_unique_tickers = len(unique_tickers)\n",
    "        print(f\"Total de tickers Ãºnicos encontrados: {num_unique_tickers}\")\n",
    "        \n",
    "        if num_unique_tickers > 10:\n",
    "            print(f\"Amostra de tickers: {unique_tickers[:10]}...\")\n",
    "        else:\n",
    "            print(f\"Tickers presentes: {unique_tickers}\")\n",
    "    else:\n",
    "        print(\"Coluna 'ticker' nÃ£o encontrada.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"--- ANÃLISE CONCLUÃDA ---\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    path_do_arquivo = '../data/historical/all_histories.csv'\n",
    "    \n",
    "    analyze_dataframe(path_do_arquivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ef5a7f",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "\n",
    "### ðŸ§¹ ETL: SanitizaÃ§Ã£o de Dados HistÃ³ricos de Mercado\n",
    "\n",
    "#### ðŸ“ VisÃ£o Geral\n",
    "\n",
    "Este script Ã© responsÃ¡vel pela etapa de **Limpeza e NormalizaÃ§Ã£o (Data Cleaning)** dos dados brutos de preÃ§os (OHLCV) coletados.\n",
    "\n",
    "Ele atua como um filtro de qualidade, removendo registros inconsistentes, dias sem negociaÃ§Ã£o ou dados corrompidos antes que eles entrem no pipeline de cÃ¡lculo de indicadores ou backtesting. Isso Ã© crucial para evitar erros de divisÃ£o por zero ou distorÃ§Ãµes estatÃ­sticas.\n",
    "\n",
    "-----\n",
    "\n",
    "### âš™ï¸ Fluxo de ExecuÃ§Ã£o\n",
    "\n",
    "O script segue uma lÃ³gica de **Fallback** (tentativa e erro) para o carregamento e uma lÃ³gica estrita para a limpeza.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    Start([InÃ­cio]) --> TryParquet{Existe<br/>.parquet?}\n",
    "    \n",
    "    TryParquet -- Sim --> LoadP[Carregar Parquet]\n",
    "    TryParquet -- NÃ£o --> TryCSV{Existe<br/>.csv?}\n",
    "    \n",
    "    TryCSV -- Sim --> LoadC[Carregar CSV]\n",
    "    TryCSV -- NÃ£o --> Error[âŒ Erro Fatal]\n",
    "    \n",
    "    LoadP --> Clean[SanitizaÃ§Ã£o]\n",
    "    LoadC --> Clean\n",
    "    \n",
    "    subgraph SanitizaÃ§Ã£o [Regras de Limpeza]\n",
    "        R1[Drop NaN em 'Adj Close']\n",
    "        R2[Drop NaN em 'Volume']\n",
    "        R3[Filtro: Volume > 0]\n",
    "    end\n",
    "    \n",
    "    Clean --> Save[Salvar Arquivos Limpos]\n",
    "    Save --> End([Fim])\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "### ðŸ” Detalhes da ImplementaÃ§Ã£o\n",
    "\n",
    "#### 1\\. IngestÃ£o Inteligente (Input)\n",
    "\n",
    "O script utiliza um mecanismo robusto de carregamento:\n",
    "\n",
    "  * **Prioridade:** Tenta carregar o formato `.parquet` (mais rÃ¡pido e eficiente em memÃ³ria).\n",
    "  * **Fallback:** Se o Parquet nÃ£o existir, recorre automaticamente ao `.csv` bruto.\n",
    "  * **Parser:** Ao carregar o CSV, jÃ¡ converte a coluna de data (`parse_dates=['date']`), garantindo a tipagem correta.\n",
    "\n",
    "#### 2\\. Regras de Limpeza (Business Logic)\n",
    "\n",
    "O script aplica dois filtros rigorosos para garantir a integridade do dataset `df_prices_clean`:\n",
    "\n",
    "1.  **RemoÃ§Ã£o de Nulos (`dropna`):**\n",
    "      * Remove linhas onde o PreÃ§o Ajustado (`Adj Close`) ou Volume sejam nulos/NaN.\n",
    "      * *Motivo:* Dados nulos quebram cÃ¡lculos de mÃ©dias mÃ³veis e indicadores tÃ©cnicos.\n",
    "2.  **Filtro de Liquidez (`Volume > 0`):**\n",
    "      * Remove dias onde o volume negociado foi zero (feriados locais onde a bolsa mundial abriu, dias de suspensÃ£o de negociaÃ§Ã£o do ativo, ou erros de coleta).\n",
    "      * *Motivo:* Dias com volume zero distorcem a mÃ©dia de liquidez e podem gerar sinais falsos de estabilidade de preÃ§o.\n",
    "\n",
    "#### 3\\. PersistÃªncia (Output)\n",
    "\n",
    "Os dados limpos sÃ£o salvos no diretÃ³rio `../data/historical/` em dois formatos simultaneamente:\n",
    "\n",
    "  * `all_histories_cleaned.parquet`: Para leitura rÃ¡pida nos prÃ³ximos scripts do Python.\n",
    "  * `all_histories_cleaned.csv`: Para inspeÃ§Ã£o visual rÃ¡pida (Excel/Notepad) ou depuraÃ§Ã£o.\n",
    "\n",
    "-----\n",
    "\n",
    "#### ðŸ“Š MÃ©tricas de Qualidade\n",
    "\n",
    "O script gera logs informativos (`logging`) que permitem monitorar a \"saÃºde\" dos dados:\n",
    "\n",
    "  * **Contagem de Linhas:** Informa quantas linhas existiam antes e quantas restaram.\n",
    "  * **Perda de Dados:** Calcula e exibe quantas linhas foram descartadas (`linhas_removidas`). Uma taxa de remoÃ§Ã£o muito alta pode indicar problemas na fonte de dados (Yahoo Finance/B3).\n",
    "\n",
    "-----\n",
    "\n",
    "### ðŸš€ Como Executar\n",
    "\n",
    "Certifique-se de que o arquivo bruto (`all_histories.parquet` ou `.csv`) exista na pasta de dados.\n",
    "\n",
    "```bash\n",
    "python clean_market_data.py\n",
    "```\n",
    "\n",
    "### Exemplo de SaÃ­da no Console:\n",
    "\n",
    "```text\n",
    "2025-12-13 16:30:00 - INFO - Dados de preÃ§o brutos carregados do PARQUET: 150000 linhas\n",
    "2025-12-13 16:30:01 - INFO - Dados de preÃ§o limpos: 148500 linhas (removidas 1500 linhas com NaN ou Volume 0)\n",
    "2025-12-13 16:30:02 - INFO - âœ… Arquivo limpo salvo em (Parquet): ../data/historical/all_histories_cleaned.parquet\n",
    "2025-12-13 16:30:02 - INFO - âœ… Arquivo limpo salvo em (CSV): ../data/historical/all_histories_cleaned.csv\n",
    "\n",
    "Limpeza concluÃ­da. 1500 linhas removidas.\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "*DocumentaÃ§Ã£o do mÃ³dulo ETL do Projeto Aurum.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85e9d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-13 15:56:26,391 - INFO - Dados de preÃ§o brutos carregados do PARQUET: 360452 linhas\n",
      "2025-12-13 15:56:26,456 - INFO - Dados de preÃ§o limpos: 276324 linhas (removidas 84128 linhas com NaN ou Volume 0)\n",
      "2025-12-13 15:56:26,738 - INFO - âœ… Arquivo limpo salvo em (Parquet): ../data/historical\\all_histories_cleaned.parquet\n",
      "2025-12-13 15:56:30,192 - INFO - âœ… Arquivo limpo salvo em (CSV): ../data/historical\\all_histories_cleaned.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Limpeza concluÃ­da. 84128 linhas de 'lookahead' removidas.\n",
      "     ticker       date      Open      High       Low     Close  Adj Close  \\\n",
      "0  ABEV3.SA 2011-01-03  8.632311  8.728203  8.630313  8.690246   4.694567   \n",
      "1  ABEV3.SA 2011-01-04  8.784141  8.784141  8.630313  8.692244   4.695647   \n",
      "2  ABEV3.SA 2011-01-05  8.672266  8.718215  8.448517  8.530425   4.608231   \n",
      "3  ABEV3.SA 2011-01-06  8.560392  8.590358  8.396576  8.450515   4.565063   \n",
      "4  ABEV3.SA 2011-01-07  8.450515  8.550403  8.368607  8.416553   4.546715   \n",
      "\n",
      "      Volume  Dividends  Stock Splits  \n",
      "0   576145.0        0.0           0.0  \n",
      "1   328368.0        0.0           0.0  \n",
      "2   299836.0        0.0           0.0  \n",
      "3   731319.0        0.0           0.0  \n",
      "4  1090222.0        0.0           0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "try:\n",
    "    df_prices_raw = pd.read_parquet(\"../data/historical/all_histories.parquet\")\n",
    "    logger.info(f\"Dados de preÃ§o brutos carregados do PARQUET: {df_prices_raw.shape[0]} linhas\")\n",
    "except FileNotFoundError:\n",
    "    logger.warning(\"Arquivo all_histories.parquet nÃ£o encontrado! Tentando carregar o CSV...\")\n",
    "    try:\n",
    "        df_prices_raw = pd.read_csv(\"../data/historical/all_histories.csv\", parse_dates=['date'])\n",
    "        logger.info(f\"Dados de preÃ§o brutos carregados do CSV: {df_prices_raw.shape[0]} linhas\")\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(\"Nenhum arquivo de histÃ³rico (parquet ou csv) encontrado. Execute o download primeiro.\")\n",
    "        raise e\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erro ao carregar dados: {e}\")\n",
    "    raise\n",
    "\n",
    "df_prices_clean = df_prices_raw.dropna(subset=['Adj Close', 'Volume'])\n",
    "\n",
    "df_prices_clean = df_prices_clean[df_prices_clean['Volume'] > 0]\n",
    "\n",
    "linhas_removidas = len(df_prices_raw) - len(df_prices_clean)\n",
    "logger.info(f\"Dados de preÃ§o limpos: {df_prices_clean.shape[0]} linhas (removidas {linhas_removidas} linhas com NaN ou Volume 0)\")\n",
    "\n",
    "output_dir = \"../data/historical\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "parquet_path = os.path.join(output_dir, \"all_histories_cleaned.parquet\")\n",
    "csv_path = os.path.join(output_dir, \"all_histories_cleaned.csv\")\n",
    "\n",
    "try:\n",
    "    df_prices_clean.to_parquet(parquet_path, index=False)\n",
    "    logger.info(f\"âœ… Arquivo limpo salvo em (Parquet): {parquet_path}\")\n",
    "\n",
    "    df_prices_clean.to_csv(csv_path, index=False, date_format='%Y-%m-%d')\n",
    "    logger.info(f\"âœ… Arquivo limpo salvo em (CSV): {csv_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"âŒ Erro ao salvar arquivos limpos: {e}\")\n",
    "\n",
    "print(f\"\\nLimpeza concluÃ­da. {linhas_removidas} linhas de 'lookahead' removidas.\")\n",
    "print(df_prices_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae18351",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "\n",
    "### ðŸ”„ ETL: TransformaÃ§Ã£o e Pivoteamento de PreÃ§os (Mensal)\n",
    "\n",
    "#### ðŸ“ VisÃ£o Geral\n",
    "\n",
    "Este script Ã© responsÃ¡vel por **alterar a dimensionalidade** dos dados de mercado, preparando o terreno para a etapa de Backtesting e geraÃ§Ã£o de sinais.\n",
    "\n",
    "Ele transforma o dataset \"Longo\" (onde cada linha Ã© um registro diÃ¡rio de um ticker) em datasets \"Wide\" (Matrizes), onde o Ã­ndice Ã© a Data, as colunas sÃ£o os Tickers e os valores sÃ£o os preÃ§os. AlÃ©m disso, ele realiza a **reamostragem temporal** (Resampling) de dados diÃ¡rios para **mensais**.\n",
    "\n",
    "-----\n",
    "\n",
    "### âš™ï¸ Fluxo de TransformaÃ§Ã£o\n",
    "\n",
    "O script realiza uma conversÃ£o crucial de formato de dados:\n",
    "\n",
    "**De: Formato Longo (Transactional)**\n",
    "| Data | Ticker | Adj Close |\n",
    "| :--- | :--- | :--- |\n",
    "| 2024-01-01 | PETR4 | 35.00 |\n",
    "| 2024-01-01 | VALE3 | 70.00 |\n",
    "| 2024-01-02 | PETR4 | 36.00 |\n",
    "\n",
    "**Para: Formato Wide (Matrix)**\n",
    "| Data (Index) | PETR4 | VALE3 | ... |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| 2024-01-31 | 38.00 | 72.00 | ... |\n",
    "| 2024-02-29 | 39.50 | 71.00 | ... |\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    Input[(\"<b>all_histories_cleaned.parquet</b><br/>(Dados DiÃ¡rios Longos)\")] --> Process\n",
    "    \n",
    "    subgraph Process [Processamento]\n",
    "        A[\"<b>Resample Mensal ('M')</b><br/>Captura o Ãºltimo preÃ§o do mÃªs\"]\n",
    "        B[\"<b>Resample Mensal ('MS')</b><br/>Captura o preÃ§o de abertura do mÃªs\"]\n",
    "        C[\"<b>Pivot Table</b><br/>Transforma Tickers em Colunas\"]\n",
    "        D[\"<b>Fill NA</b><br/>Preenchimento de lacunas (ffill/bfill)\"]\n",
    "    end\n",
    "    \n",
    "    Input --> A & B\n",
    "    A --> C\n",
    "    B --> C\n",
    "    C --> D\n",
    "    \n",
    "    D --> Out1[(\"<b>prices_close_wide.parquet</b><br/>Matriz de Fechamentos\")]\n",
    "    D --> Out2[(\"<b>prices_open_wide.parquet</b><br/>Matriz de Aberturas\")]\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "### ðŸ” Detalhes da LÃ³gica\n",
    "\n",
    "#### 1\\. Reamostragem (Resampling)\n",
    "\n",
    "Para simular uma estratÃ©gia de rebalanceamento mensal, nÃ£o precisamos de 252 dias Ãºteis por ano, apenas do preÃ§o de entrada e saÃ­da de cada mÃªs.\n",
    "\n",
    "  * **Fechamento (`Close`):** Usa a frequÃªncia `'M'` (Month End) e a funÃ§Ã£o `.last()`. Pega o preÃ§o do Ãºltimo dia de negociaÃ§Ã£o do mÃªs.\n",
    "  * **Abertura (`Open`):** Usa a frequÃªncia `'MS'` (Month Start) e a funÃ§Ã£o `.first()`. Pega o preÃ§o do primeiro dia de negociaÃ§Ã£o do mÃªs.\n",
    "\n",
    "#### 2\\. Tratamento de Lacunas (Gap Filling)\n",
    "\n",
    "Matrizes de preÃ§os nÃ£o podem ter buracos (NaN) para cÃ¡lculos vetoriais. O script aplica uma estratÃ©gia agressiva de preenchimento:\n",
    "\n",
    "  * **Forward Fill (`ffill`):** Se um ativo nÃ£o foi negociado em um mÃªs especÃ­fico, ele repete o preÃ§o do mÃªs anterior.\n",
    "  * **Backward Fill (`bfill`):** Se o ativo nÃ£o tinha dados no inÃ­cio do histÃ³rico, ele puxa o primeiro preÃ§o vÃ¡lido para trÃ¡s (para evitar NaNs no comeÃ§o da sÃ©rie).\n",
    "\n",
    "-----\n",
    "\n",
    "### ðŸ“‚ Entradas e SaÃ­das\n",
    "\n",
    "#### Input\n",
    "\n",
    "  * **Arquivo:** `../data/historical/all_histories_cleaned.parquet`\n",
    "  * **ConteÃºdo:** HistÃ³rico diÃ¡rio limpo de todos os ativos (formato longo).\n",
    "\n",
    "#### Outputs\n",
    "\n",
    "Salvos em `../data/historical/`:\n",
    "\n",
    "1.  **`prices_close_wide.parquet`**:\n",
    "      * Matriz contendo apenas os preÃ§os de fechamento ajustados (`Adj Close`) de fim de mÃªs.\n",
    "      * Essencial para calcular a rentabilidade da carteira.\n",
    "2.  **`prices_open_wide.parquet`**:\n",
    "      * Matriz contendo os preÃ§os de abertura (`Open`) de inÃ­cio de mÃªs.\n",
    "      * Utilizado para simular o preÃ§o de execuÃ§Ã£o da compra no rebalanceamento.\n",
    "\n",
    "-----\n",
    "\n",
    "### ðŸš€ Como Executar\n",
    "\n",
    "Este script depende da execuÃ§Ã£o prÃ©via do script de limpeza.\n",
    "\n",
    "```bash\n",
    "python gerar_matriz_precos.py\n",
    "```\n",
    "\n",
    "### Exemplo de Log de Sucesso:\n",
    "\n",
    "```text\n",
    "2025-12-13 16:45:00 - INFO - Iniciando o Passo 1: GeraÃ§Ã£o dos PreÃ§os Pivotados (Wide)...\n",
    "2025-12-13 16:45:01 - INFO - Dados limpos carregados.\n",
    "2025-12-13 16:45:02 - INFO - Reamostrando para FrequÃªncia Mensal (M e MS)...\n",
    "2025-12-13 16:45:03 - INFO - Corrigindo MultiIndex e Pivotando...\n",
    "2025-12-13 16:45:04 - INFO - âœ… ARQUIVO FALTOSO GERADO: ../data/historical/prices_close_wide.parquet\n",
    "```\n",
    "\n",
    "-----\n",
    "\n",
    "*DocumentaÃ§Ã£o do mÃ³dulo de TransformaÃ§Ã£o de Dados do Projeto Aurum.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90af87d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-13 15:56:30,226 - INFO - Iniciando o Passo 1: GeraÃ§Ã£o dos PreÃ§os Pivotados (Wide)...\n",
      "2025-12-13 15:56:30,309 - INFO - Dados limpos '../data/historical/all_histories_cleaned.parquet' carregados.\n",
      "2025-12-13 15:56:30,323 - INFO - Reamostrando para FrequÃªncia Mensal (M e MS)...\n",
      "C:\\Users\\kaike\\AppData\\Local\\Temp\\ipykernel_5140\\2232062475.py:26: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  df_prices_mensal_raw = df_prices_clean.set_index('date').groupby('ticker').resample('M').last()\n",
      "C:\\Users\\kaike\\AppData\\Local\\Temp\\ipykernel_5140\\2232062475.py:26: FutureWarning: DataFrameGroupBy.resample operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_prices_mensal_raw = df_prices_clean.set_index('date').groupby('ticker').resample('M').last()\n",
      "C:\\Users\\kaike\\AppData\\Local\\Temp\\ipykernel_5140\\2232062475.py:28: FutureWarning: DataFrameGroupBy.resample operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_open_mensal_raw = df_prices_clean.set_index('date').groupby('ticker').resample('MS').first()\n",
      "2025-12-13 15:56:31,971 - INFO - Corrigindo MultiIndex e Pivotando...\n",
      "2025-12-13 15:56:32,009 - INFO - Sincronizando e preenchendo Ã­ndices de data...\n",
      "2025-12-13 15:56:32,082 - INFO - âœ… ARQUIVO FALTOSO GERADO: ../data/historical\\prices_close_wide.parquet\n",
      "2025-12-13 15:56:32,083 - INFO - âœ… ARQUIVO FALTOSO GERADO: ../data/historical\\prices_open_wide.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Amostra de Fechamento (Wide) ---\n",
      "ticker      ABEV3.SA   ALOS3.SA  ANIM3.SA   ASAI3.SA   AURE3.SA   AXIA3.SA  \\\n",
      "date                                                                         \n",
      "2011-01-01  4.097764  32.057922  5.146394  14.210447  12.438302  12.054853   \n",
      "2011-01-31  4.097764  32.057922  5.146394  14.210447  12.438302  12.054853   \n",
      "2011-02-01  4.097764  32.057922  5.146394  14.210447  12.438302  12.054853   \n",
      "2011-02-28  4.041646  32.057922  5.146394  14.210447  12.438302  12.646833   \n",
      "2011-03-01  4.041646  32.057922  5.146394  14.210447  12.438302  12.646833   \n",
      "\n",
      "ticker       AXIA6.SA   AZZA3.SA  B3SA3.SA  BBAS3.SA  ...  TOTS3.SA  UGPA3.SA  \\\n",
      "date                                                  ...                       \n",
      "2011-01-01  10.141385  14.825628  6.694447  5.331043  ...  8.352773  2.950102   \n",
      "2011-01-31  10.141385  14.825628  6.694447  5.331043  ...  8.352773  2.950102   \n",
      "2011-02-01  10.141385  14.825628  6.694447  5.331043  ...  8.352773  2.950102   \n",
      "2011-02-28  11.386950  14.825628  6.503195  5.386558  ...  8.092393  2.950102   \n",
      "2011-03-01  11.386950  14.825628  6.503195  5.386558  ...  8.092393  2.950102   \n",
      "\n",
      "ticker       USIM5.SA   VALE3.SA  VAMO3.SA   VBBR3.SA   VIVA3.SA  VIVT3.SA  \\\n",
      "date                                                                         \n",
      "2011-01-01  14.937788  23.735777  7.560618  12.779866  22.094751  6.297232   \n",
      "2011-01-31  14.937788  23.735777  7.560618  12.779866  22.094751  6.297232   \n",
      "2011-02-01  14.937788  23.735777  7.560618  12.779866  22.094751  6.297232   \n",
      "2011-02-28  14.607204  23.411423  7.560618  12.779866  22.094751  6.543143   \n",
      "2011-03-01  14.607204  23.411423  7.560618  12.779866  22.094751  6.543143   \n",
      "\n",
      "ticker      WEGE3.SA  YDUQ3.SA  \n",
      "date                            \n",
      "2011-01-01  2.167035  5.368343  \n",
      "2011-01-31  2.167035  5.368343  \n",
      "2011-02-01  2.167035  5.368343  \n",
      "2011-02-28  2.205022  5.435447  \n",
      "2011-03-01  2.205022  5.435447  \n",
      "\n",
      "[5 rows x 97 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "INPUT_FILE = \"../data/historical/all_histories_cleaned.parquet\"\n",
    "OUTPUT_DIR = \"../data/historical\"\n",
    "\n",
    "def gerar_precos_pivotados():\n",
    "    logger.info(\"Iniciando o Passo 1: GeraÃ§Ã£o dos PreÃ§os Pivotados (Wide)...\")\n",
    "    \n",
    "    try:\n",
    "        df_prices_clean = pd.read_parquet(INPUT_FILE)\n",
    "        logger.info(f\"Dados limpos '{INPUT_FILE}' carregados.\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"ARQUIVO NÃƒO ENCONTRADO: {INPUT_FILE}\")\n",
    "        logger.error(\"Execute o 'Script 1: Limpeza' primeiro.\")\n",
    "        return\n",
    "\n",
    "    df_prices_clean['date'] = pd.to_datetime(df_prices_clean['date'])\n",
    "\n",
    "    logger.info(\"Reamostrando para FrequÃªncia Mensal (M e MS)...\")\n",
    "    df_prices_mensal_raw = df_prices_clean.set_index('date').groupby('ticker').resample('M').last()\n",
    "\n",
    "    df_open_mensal_raw = df_prices_clean.set_index('date').groupby('ticker').resample('MS').first()\n",
    "\n",
    "    logger.info(\"Corrigindo MultiIndex e Pivotando...\")\n",
    "    \n",
    "    df_prices_mensal_long = df_prices_mensal_raw.drop(columns='ticker', errors='ignore').reset_index()\n",
    "    df_open_mensal_long = df_open_mensal_raw.drop(columns='ticker', errors='ignore').reset_index()\n",
    "\n",
    "    df_close_wide = df_prices_mensal_long.pivot(index='date', columns='ticker', values='Adj Close')\n",
    "    df_open_wide = df_open_mensal_long.pivot(index='date', columns='ticker', values='Open')\n",
    "\n",
    "    logger.info(\"Sincronizando e preenchendo Ã­ndices de data...\")\n",
    "    idx_union = df_close_wide.index.union(df_open_wide.index)\n",
    "    \n",
    "    df_close_wide = df_close_wide.reindex(idx_union, method='ffill')\n",
    "    df_open_wide = df_open_wide.reindex(idx_union, method='ffill')\n",
    "\n",
    "    df_close_wide = df_close_wide.ffill().bfill()\n",
    "    df_open_wide = df_open_wide.ffill().bfill()\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    path_close = os.path.join(OUTPUT_DIR, \"prices_close_wide.parquet\")\n",
    "    path_open = os.path.join(OUTPUT_DIR, \"prices_open_wide.parquet\")\n",
    "    \n",
    "    df_close_wide.to_parquet(path_close)\n",
    "    df_open_wide.to_parquet(path_open)\n",
    "    \n",
    "    logger.info(f\"âœ… ARQUIVO FALTOSO GERADO: {path_close}\")\n",
    "    logger.info(f\"âœ… ARQUIVO FALTOSO GERADO: {path_open}\")\n",
    "    print(\"\\n--- Amostra de Fechamento (Wide) ---\")\n",
    "    print(df_close_wide.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gerar_precos_pivotados()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979ffc10",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "130560c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-13 15:56:32,122 - INFO - Iniciando Passo 2: CÃ¡lculo da Volatilidade...\n",
      "C:\\Users\\kaike\\AppData\\Local\\Temp\\ipykernel_5140\\3230729094.py:18: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  df_vol_mensal = df_prices_daily.set_index('date').groupby('ticker').resample('M').last()['VOLATILIDADE'].reset_index()\n",
      "C:\\Users\\kaike\\AppData\\Local\\Temp\\ipykernel_5140\\3230729094.py:18: FutureWarning: DataFrameGroupBy.resample operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_vol_mensal = df_prices_daily.set_index('date').groupby('ticker').resample('M').last()['VOLATILIDADE'].reset_index()\n",
      "2025-12-13 15:56:33,107 - INFO - âœ… Volatilidade mensal calculada. 13829 registros.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ticker       date  VOLATILIDADE\n",
      "13824  YDUQ3.SA 2025-08-31      0.026134\n",
      "13825  YDUQ3.SA 2025-09-30      0.027915\n",
      "13826  YDUQ3.SA 2025-10-31      0.026777\n",
      "13827  YDUQ3.SA 2025-11-30      0.026897\n",
      "13828  YDUQ3.SA 2025-12-31      0.028388\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Iniciando Passo 2: CÃ¡lculo da Volatilidade...\")\n",
    "\n",
    "df_prices_daily = pd.read_parquet(\"../data/historical/all_histories_cleaned.parquet\")\n",
    "df_prices_daily['date'] = pd.to_datetime(df_prices_daily['date'])\n",
    "\n",
    "df_prices_daily['returns'] = df_prices_daily.groupby('ticker')['Adj Close'].pct_change()\n",
    "\n",
    "df_prices_daily['VOLATILIDADE'] = df_prices_daily.groupby('ticker')['returns'].rolling(window=63).std().reset_index(0, drop=True)\n",
    "\n",
    "df_vol_mensal = df_prices_daily.set_index('date').groupby('ticker').resample('M').last()['VOLATILIDADE'].reset_index()\n",
    "\n",
    "logger.info(f\"âœ… Volatilidade mensal calculada. {len(df_vol_mensal)} registros.\")\n",
    "print(df_vol_mensal.tail())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
