{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05da2464",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **üì¶ cvm_downloader** ‚Äî Download & Unzip autom√°tico (CVM: DFP / ITR)\n",
    "\n",
    "**1. Descri√ß√£o curta**  \n",
    "Script robusto para baixar (streaming com progress bar) e descompactar os arquivos p√∫blicos da CVM (`DFP` e `ITR`) por ano. Inclui retries, backoff, grava√ß√£o segura (.part ‚Üí rename) e verifica√ß√£o para pular arquivos j√° existentes. Tem suporte a execu√ß√£o sequencial ou com --workers (thread pool).\n",
    "\n",
    "**2. Principais responsabilidades**\n",
    "- Construir URLs padr√£o: **https://dados.cvm.gov.br/dados/CIA_ABERTA/DOC/{doc_type}/DADOS/{doc_type}_cia_aberta_{year}.zip**\n",
    "- Baixar zips com **requests.Session** + retries\n",
    "- Exibir barra de progresso (**tqdm**)\n",
    "- Descompactar zips (**zipfile.ZipFile**)\n",
    "- Pular arquivos j√° baixados / extra√≠dos\n",
    "- Opcional: paralelizar downloads via **ThreadPoolExecutor**\n",
    "\n",
    "**3. Entradas / Par√¢metros**\n",
    "- --doc-types (ex.: DFP,ITR)\n",
    "- --start-year / --end-year (ex.: 2011 / 2025)\n",
    "- --workers (n¬∫ threads; default 1)\n",
    "\n",
    "**4. Sa√≠das**\n",
    "- data/cvm/zip/{DOC}_cia_aberta_{YEAR}.zip\n",
    "- data/cvm/unzipped/{DOC}_{YEAR}/... (conte√∫do extra√≠do)\n",
    "- Retorna resumo (ok / skipped / failed) e c√≥digo de sa√≠da CLI\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973591e1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": "import datetime\nimport logging\nimport os\nimport time\nfrom zipfile import ZipFile\n\nimport pandas as pd\n\nfrom typing import Dict, Iterable, List, Optional, Set\n\nimport requests\nimport tqdm\n\n\n# --- Configura√ß√£o ---\n# Onde os dados ser√£o salvos (pasta data no n√≠vel do aurum, n√£o data_sources)\nBASE_DIR = os.path.join(\"..\", \"data\", \"cvm\")\nZIP_DIR = os.path.join(BASE_DIR, \"zip\")\nUNZIPPED_DIR = os.path.join(BASE_DIR, \"unzipped\")\n\n# Cria as pastas se n√£o existirem\nos.makedirs(ZIP_DIR, exist_ok=True)\nos.makedirs(UNZIPPED_DIR, exist_ok=True)\n\n# URL base para os arquivos da CVM\nURL_BASE = \"https://dados.cvm.gov.br/dados/CIA_ABERTA/DOC/{doc_type}/DADOS/\"\n\n# Tipos de documentos que queremos\n# DFP -> Anual, ITR -> Trimestral\nDOC_TYPES = ['DFP', 'ITR']\n\nYEARS = range(2011, 2026) \n\ndef download_and_unzip(url, zip_path, unzipped_path):\n    \"\"\"Baixa e descompacta um arquivo ZIP se ele n√£o existir localmente.\"\"\"\n    if os.path.exists(zip_path):\n        print(f\"Arquivo j√° existe, pulando download: {os.path.basename(zip_path)}\")\n    else:\n        print(f\"Baixando: {os.path.basename(zip_path)}\")\n        try:\n            response = requests.get(url, stream=True)\n            response.raise_for_status() # Lan√ßa erro se a requisi√ß√£o falhar\n            \n            total_size = int(response.headers.get('content-length', 0))\n            \n            with open(zip_path, 'wb') as f, tqdm.tqdm(\n                desc=os.path.basename(zip_path),\n                total=total_size,\n                unit='iB',\n                unit_scale=True,\n                unit_divisor=1024,\n            ) as bar:\n                for data in response.iter_content(chunk_size=1024):\n                    size = f.write(data)\n                    bar.update(size)\n            print(\"Download completo.\")\n        except requests.exceptions.RequestException as e:\n            print(f\"Erro no download de {url}: {e}\")\n            return # Sai da fun√ß√£o se o download falhar\n\n    print(f\"Descompactando: {os.path.basename(zip_path)}\")\n    try:\n        with ZipFile(zip_path, 'r') as zip_ref:\n            zip_ref.extractall(unzipped_path)\n        print(\"Descompactado com sucesso.\")\n    except Exception as e:\n        print(f\"Erro ao descompactar {zip_path}: {e}\")\n\nif __name__ == \"__main__\":\n    for doc_type in DOC_TYPES:\n        for year in YEARS:\n            filename = f\"{doc_type}_cia_aberta_{year}.zip\"\n            url = URL_BASE.format(doc_type=doc_type) + filename\n            \n            zip_path = os.path.join(ZIP_DIR, filename)\n            unzipped_path = os.path.join(UNZIPPED_DIR, f\"{doc_type}_{year}\")\n            \n            download_and_unzip(url, zip_path, unzipped_path)\n            print(\"-\" * 50)\n            \n    print(\"\\nProcesso de download e extra√ß√£o conclu√≠do!\")\n"
  },
  {
   "cell_type": "markdown",
   "id": "3bbdc2ea",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### üß© cvm_parser ‚Äî Parser / Busca consolidada (_con_ files) e prepara√ß√£o\n",
    "\n",
    "**1. Descri√ß√£o curta**  \n",
    "Pipeline para localizar arquivos CSV consolidados _con_*.csv extra√≠dos da CVM, aplicar filtros (√öLTIMO/PEN√öLTIMO, escala MIL/UNIDADE), normalizar VL_CONTA, diagnosticar CNPJ(s) de interesse (ex.: Sanepar), concatenar, deduplicar e salvar resultado processado.\n",
    "\n",
    "**2. Principais responsabilidades**\n",
    "- Encontrar arquivos consolidados recursivamente (`UNZIPPED_DIR/**/*_con_*.csv`)\n",
    "- Ler CSVs com pd.read_csv(..., encoding='latin-1', sep=';')\n",
    "- Filtrar por ORDEM_EXERC (`√öLTIMO` / `PEN√öLTIMO`) e ESCALA_MOEDA (`MIL` / `UNIDADE`)\n",
    "- Converter VL_CONTA para num√©rico e aplicar ajuste (multiplica por 1000 quando `ESCALA_MOEDA == 'MIL'`)\n",
    "- Diagn√≥stico: detectar CNPJ_SAPR e imprimir valores de `ORDEM_EXERC` / `ESCALA_MOEDA`\n",
    "- Concatenar chunks, ordenar por CNPJ_CIA, DT_FIM_EXERC, VERSAO\n",
    "- Drop duplicates por ['CNPJ_CIA','DT_FIM_EXERC','CD_CONTA']\n",
    "- Salvar processed/raw_{doc}.parquet e processed/raw_{doc}.csv\n",
    "\n",
    "**3. Entradas**\n",
    "- data/cvm/unzipped/... (arquivos _con_*.csv)\n",
    "\n",
    "**4. Sa√≠das**\n",
    "- data/cvm/processed/raw_{dre|bpa|bpp}.parquet\n",
    "- data/cvm/processed/raw_{dre|bpa|bpp}.csv\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc926da",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport logging\nimport gc\n\n# -----------------------\n# Configura√ß√£o (edite aqui)\n# -----------------------\nBASE_DIR = Path(\"..\") / \"data\" / \"cvm\"\nUNZIPPED_DIR = BASE_DIR / \"unzipped\"\nPROCESSED_DIR = BASE_DIR / \"processed\"\nPROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n\nDOC_PATTERNS_BROAD = {\n    \"dre\": \"*_DRE_con_*.csv\",\n    \"bpa\": \"*_BPA_con_*.csv\",\n    \"bpp\": \"*_BPP_con_*.csv\",\n}\n\nCNPJ_SAPR = \"76.484.013/0001-45\"\nCNPJs_DEBUG = [CNPJ_SAPR, \"33.839.910/0001-11\"]  # VIVA3\n\nCATEGORY_COLS = [\n    \"CNPJ_CIA\", \"DENOM_CIA\", \"GRUPO_DFP\", \"MOEDA\",\n    \"ESCALA_MOEDA\", \"ORDEM_EXERC\", \"CD_CONTA\",\n    \"DS_CONTA\", \"ST_CONTA_FIXA\"\n]\n\n# -----------------------\n# Logging\n# -----------------------\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\nlogger = logging.getLogger(\"cvm_parser\")\n\n# -----------------------\n# Helpers\n# -----------------------\ndef find_consolidated_files(unzipped_dir: Path, pattern: str) -> List[Path]:\n    \"\"\"Retorna lista de arquivos que batem com o padr√£o (recursivo).\"\"\"\n    return list(unzipped_dir.rglob(pattern))\n\n\ndef _ensure_category(df: pd.DataFrame, col: str) -> None:\n    \"\"\"Converte coluna para category quando apropriado (silencioso).\"\"\"\n    try:\n        if col in df.columns and not pd.api.types.is_categorical_dtype(df[col]):\n            df[col] = df[col].astype(\"category\")\n    except Exception:\n        # n√£o quebrar a execu√ß√£o por problemas de convers√£o\n        pass\n\n\ndef _read_csv_safe(path: Path) -> Optional[pd.DataFrame]:\n    \"\"\"L√™ um CSV com par√¢metros padr√£o usados no pipeline; captura exce√ß√µes e retorna None se falhar.\"\"\"\n    try:\n        df = pd.read_csv(path, encoding=\"latin-1\", sep=\";\", low_memory=False, dtype={\"CNPJ_CIA\": str})\n        return df\n    except Exception as exc:\n        logger.warning(\"Falha lendo %s: %s\", path, exc)\n        return None\n\n\ndef _process_df_chunk(df: pd.DataFrame) -> Optional[pd.DataFrame]:\n    \"\"\"\n    Aplicar filtros e tratamentos ao chunk (j√° lido).\n    Retorna DataFrame filtrado pronto para concatenar ou None se vazio.\n    \"\"\"\n    # garantir colunas chave\n    if \"DT_FIM_EXERC\" not in df.columns:\n        return None\n\n    # normalizar e otimizar\n    for col in CATEGORY_COLS:\n        if col in df.columns:\n            _ensure_category(df, col)\n\n    # converter data e remover linhas sem data v√°lida\n    df[\"DT_FIM_EXERC\"] = pd.to_datetime(df[\"DT_FIM_EXERC\"], errors=\"coerce\")\n    df = df.dropna(subset=[\"DT_FIM_EXERC\"])\n    if df.empty:\n        return None\n\n    # aplicar filtro ORDEM_EXERC & ESCALA_MOEDA quando existirem\n    if \"ORDEM_EXERC\" in df.columns and \"ESCALA_MOEDA\" in df.columns:\n        mask = (\n            df[\"ORDEM_EXERC\"].isin([\"√öLTIMO\", \"PEN√öLTIMO\"]) &\n            df[\"ESCALA_MOEDA\"].isin([\"MIL\", \"UNIDADE\"])\n        )\n        df = df.loc[mask].copy()\n\n    if df.empty:\n        return None\n\n    # garantir VL_CONTA num√©rico; remover nulos\n    if \"VL_CONTA\" in df.columns:\n        df[\"VL_CONTA\"] = pd.to_numeric(df[\"VL_CONTA\"], errors=\"coerce\")\n        df = df.dropna(subset=[\"VL_CONTA\"])\n    else:\n        return None\n\n    # ajustar escala MIL ‚Üí multiplicar por 1000\n    if \"ESCALA_MOEDA\" in df.columns:\n        if pd.api.types.is_categorical_dtype(df[\"ESCALA_MOEDA\"]):\n            cats = df[\"ESCALA_MOEDA\"].cat.categories\n            is_mil = df[\"ESCALA_MOEDA\"].cat.codes == int(np.where(cats == \"MIL\")[0][0]) if \"MIL\" in cats else False\n        else:\n            is_mil = df[\"ESCALA_MOEDA\"].astype(str) == \"MIL\"\n        # aplicar ajuste com np.where (vetorizado)\n        df[\"VL_CONTA\"] = np.where(is_mil, df[\"VL_CONTA\"] * 1000.0, df[\"VL_CONTA\"])\n\n    return df\n\n\n# -----------------------\n# Pipeline principal\n# -----------------------\ndef parse_and_consolidate_final(\n    doc_name: str,\n    broad_pattern: str,\n    unzipped_dir: Path = UNZIPPED_DIR,\n    processed_dir: Path = PROCESSED_DIR,\n) -> Dict[str, object]:\n    \"\"\"\n    Encontra arquivos consolidados (_con_), processa, concatena, deduplica e salva parquet/csv.\n    Retorna dicion√°rio com estat√≠sticas do processamento.\n    \"\"\"\n    logger.info(\"Iniciando processamento CONSOLIDADO para: %s (padr√£o: %s)\", doc_name.upper(), broad_pattern)\n    files = find_consolidated_files(unzipped_dir, broad_pattern)\n    if not files:\n        logger.info(\"Nenhum arquivo encontrado para o padr√£o: %s\", broad_pattern)\n        return {\"status\": \"no_files\", \"files_count\": 0}\n\n    df_chunks = []\n    total_rows_read = 0\n    total_rows_after_filter = 0\n    sapr_found = False\n\n    for path in tqdm.tqdm(files, desc=f\"Processando {doc_name.upper()}\"):\n        df = _read_csv_safe(path)\n        if df is None:\n            continue\n\n        total_rows_read += len(df)\n\n        # diagnostico SAPR (apenas relat√≥rio, sem interromper)\n        if \"CNPJ_CIA\" in df.columns:\n            df[\"CNPJ_CIA\"] = df[\"CNPJ_CIA\"].astype(str).str.strip()\n            df_sapr = df[df[\"CNPJ_CIA\"] == CNPJ_SAPR]\n            if not df_sapr.empty and not sapr_found:\n                sapr_found = True\n                logger.info(\"[DIAGN√ìSTICO SAPR11] Encontrado em: %s\", path.name)\n                if \"ORDEM_EXERC\" in df_sapr.columns:\n                    logger.info(\"  ORDEM_EXERC values: %s\", df_sapr[\"ORDEM_EXERC\"].unique())\n                if \"ESCALA_MOEDA\" in df_sapr.columns:\n                    logger.info(\"  ESCALA_MOEDA values: %s\", df_sapr[\"ESCALA_MOEDA\"].unique())\n\n        # processa e filtra o chunk\n        try:\n            processed = _process_df_chunk(df)\n            if processed is not None and not processed.empty:\n                total_rows_after_filter += len(processed)\n                df_chunks.append(processed)\n        except Exception as exc:\n            logger.warning(\"Erro processando arquivo %s: %s\", path, exc)\n        finally:\n            # liberar mem√≥ria\n            del df\n            gc.collect()\n\n    logger.info(\"Totais: linhas lidas=%d, linhas ap√≥s filtros=%d\", total_rows_read, total_rows_after_filter)\n    if not sapr_found:\n        logger.warning(\"[DIAGN√ìSTICO SAPR11] CNPJ Sanepar (%s) N√ÉO encontrado na busca consolidada.\", CNPJ_SAPR)\n\n    if not df_chunks:\n        logger.info(\"Nenhum chunk com dados v√°lidos ap√≥s filtros. Abortando concatena√ß√£o.\")\n        return {\"status\": \"no_data_after_filter\", \"files_count\": len(files)}\n\n    # concat + sort + dedupe\n    logger.info(\"Concatenando %d chunks...\", len(df_chunks))\n    consolidated_df = pd.concat(df_chunks, ignore_index=True)\n    consolidated_df.sort_values(by=[\"CNPJ_CIA\", \"DT_FIM_EXERC\", \"VERSAO\"], ascending=[True, True, False], inplace=True)\n\n    dedup_subset = [\"CNPJ_CIA\", \"DT_FIM_EXERC\", \"CD_CONTA\"]\n    final_df = consolidated_df.drop_duplicates(subset=dedup_subset, keep=\"first\").copy()\n\n    # diagn√≥stico r√°pido para CNPJs de debug\n    if \"CNPJ_CIA\" in final_df.columns:\n        debug_mask = final_df[\"CNPJ_CIA\"].astype(str).isin([str(x) for x in CNPJs_DEBUG])\n        debug_data = final_df.loc[debug_mask]\n        if not debug_data.empty:\n            grouped = debug_data.groupby([\"CNPJ_CIA\", \"DT_FIM_EXERC\"]).size().reset_index(name=\"contagem_contas\")\n            logger.info(\"Dados SAPR11/VIVA3 no DF final:\\n%s\", grouped.head(20).to_string(index=False))\n        else:\n            logger.info(\"Nenhum dado SAPR11/VIVA3 no DF final.\")\n\n    # salvar resultados\n    out_parquet = processed_dir / f\"raw_{doc_name}.parquet\"\n    out_csv = processed_dir / f\"raw_{doc_name}.csv\"\n\n    try:\n        logger.info(\"Salvando Parquet: %s\", out_parquet)\n        final_df.to_parquet(out_parquet, index=False)\n        logger.info(\"Salvando CSV: %s\", out_csv)\n        final_df.to_csv(out_csv, index=False, sep=\";\", encoding=\"utf-8-sig\")\n        logger.info(\"Shape final salvo: %s\", final_df.shape)\n    except Exception as exc:\n        logger.exception(\"Falha ao salvar arquivos: %s\", exc)\n        return {\"status\": \"save_error\", \"error\": str(exc)}\n\n    # limpeza final\n    del df_chunks, consolidated_df, final_df\n    gc.collect()\n\n    return {\n        \"status\": \"ok\",\n        \"files_count\": len(files),\n        \"rows_read\": total_rows_read,\n        \"rows_after_filter\": total_rows_after_filter,\n        \"saved_parquet\": str(out_parquet),\n        \"saved_csv\": str(out_csv),\n    }\n\n\nif __name__ == \"__main__\":\n    results = {}\n    for name, pattern in DOC_PATTERNS_BROAD.items():\n        results[name] = parse_and_consolidate_final(name, pattern)\n        gc.collect()\n\n    logger.info(\"Processo de parsing (v9.0 - Busca Consolidada) conclu√≠do!\")\n    logger.info(\"Consulte logs acima para mensagens '---> [DIAGN√ìSTICO SAPR11 v9.0]'.\")\n    logger.info(\"Resumo por documento: %s\", results)\n"
  },
  {
   "cell_type": "markdown",
   "id": "59c1a75f",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### **üîß cv_processor ‚Äî** Transforma√ß√£o para formato WIDE (fundamentals_wide)\n",
    "\n",
    "**1. Descri√ß√£o curta**  \n",
    "Transforma os arquivos processados (`processed/raw_dre.parquet`, `raw_bpa.parquet`, `raw_bpp.parquet`) em tabelas *wide* prontos para an√°lise fundamentalista. Mapeia c√≥digos de conta para nomes leg√≠veis (plano de contas), filtra pelo √öLTIMO exerc√≠cio, pivota (long ‚Üí wide) e faz o merge final entre DRE, BPA e BPP.\n",
    "\n",
    "**2. Principais responsabilidades**\n",
    "- Ler `processed/raw_{doc}.parquet` para cada doc type (dre, bpa, bpp)\n",
    "- Filtrar linhas por CD_CONTA usando`MAPA_CONTAS_*\n",
    "- Manter apenas ORDEM_EXERC == '√öLTIMO' quando dispon√≠vel\n",
    "- Converter VL_CONTA para num√©rico e remover nulos\n",
    "- Pivot (index: CNPJ_CIA, DENOM_CIA, DT_FIM_EXERC; columns: nome leg√≠vel da CONTA)\n",
    "- Merge outer entre DRE / BPA / BPP em ['CNPJ_CIA','DENOM_CIA','DT_FIM_EXERC']\n",
    "- Salvar data/cvm/final/fundamentals_wide.parquet (+ CSV ;)\n",
    "\n",
    "**3. Entradas**\n",
    "- data/cvm/processed/raw_dre.parquet\n",
    "- data/cvm/processed/raw_bpa.parquet\n",
    "- data/cvm/processed/raw_bpp.parquet\n",
    "\n",
    "**4. Sa√≠das**\n",
    "- data/cvm/final/fundamentals_wide.parquet\n",
    "- data/cvm/final/fundamentals_wide.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1fea78",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport logging\nfrom typing import Dict, List, Optional\nimport gc # Para garbage collection\n\n# -----------------------\n# Configura√ß√£o (ATUALIZADA)\n# -----------------------\nBASE_DIR = Path(\"..\") / \"data\" / \"cvm\"\nPROCESSED_DIR = BASE_DIR / \"processed\"\nFINAL_DIR = BASE_DIR / \"final\"\nFINAL_DIR.mkdir(parents=True, exist_ok=True)\n\n# Mapa de contas PRINCIPAIS (agregadas)\nMAPA_CONTAS_DRE_MAIN = {\n    \"3.01\": \"Receita L√≠quida\",\n    \"3.02\": \"Custo dos Bens e/ou Servi√ßos Vendidos\",\n    \"3.03\": \"Lucro Bruto\",\n    \"3.05\": \"EBIT\",\n    \"3.07\": \"EBT\",\n    \"3.11\": \"Lucro L√≠quido Consolidado\",\n}\n\nMAPA_CONTAS_BPA_MAIN = {\n    \"1\": \"Ativo Total\",\n    \"1.01\": \"Ativo Circulante\",\n    \"1.02\": \"Ativo N√£o Circulante\",\n}\n\nMAPA_CONTAS_BPP_MAIN = {\n    \"2\": \"Passivo Total\",\n    \"2.01\": \"Passivo Circulante\",\n    \"2.02\": \"Passivo N√£o Circulante\",\n    \"2.03\": \"Patrim√¥nio L√≠quido Consolidado\",\n}\n\n# --- NOVO: Mapa de contas DETALHADAS (para colunas espec√≠ficas) ---\n# Estes c√≥digos s√£o comuns, mas podem precisar de ajustes finos\n# Verifique o plano de contas da CVM para mais detalhes se necess√°rio\nMAPA_CONTAS_DETALHADAS_BPA = {\n    \"1.01.01\": \"Caixa e Equivalentes\", # Caixa e Equivalentes de Caixa\n}\n\nMAPA_CONTAS_DETALHADAS_BPP = {\n    \"2.01.04\": \"D√≠vida Curto Prazo\", # Empr√©stimos e Financiamentos CP\n    \"2.02.01\": \"D√≠vida Longo Prazo\", # Empr√©stimos e Financiamentos LP\n}\n# --- FIM NOVO ---\n\n# Estrutura unificada dos mapas\nMAPA_CONTAS_GERAL = {\n    \"dre\": {\"main\": MAPA_CONTAS_DRE_MAIN, \"detailed\": None}, # DRE n√£o tem detalhado por enquanto\n    \"bpa\": {\"main\": MAPA_CONTAS_BPA_MAIN, \"detailed\": MAPA_CONTAS_DETALHADAS_BPA},\n    \"bpp\": {\"main\": MAPA_CONTAS_BPP_MAIN, \"detailed\": MAPA_CONTAS_DETALHADAS_BPP},\n}\n\n# Colunas chave esperadas\nINDEX_COLS = [\"CNPJ_CIA\", \"DENOM_CIA\", \"DT_FIM_EXERC\"]\n\n# -----------------------\n# Logging\n# -----------------------\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\nlogger = logging.getLogger(\"cvm_transform\")\n\n# -----------------------\n# Fun√ß√µes utilit√°rias\n# -----------------------\ndef _read_processed_parquet(path: Path) -> Optional[pd.DataFrame]:\n    \"\"\"L√™ um parquet com tratamento de erro.\"\"\"\n    try:\n        df = pd.read_parquet(path)\n        logger.info(\"Lido parquet: %s (shape=%s)\", path.name, df.shape)\n        return df\n    except Exception as exc:\n        logger.warning(\"Falha ao ler parquet %s: %s\", path.name, exc)\n        return None\n\n# --- ATUALIZADO: Fun√ß√£o process_and_pivot_file ---\ndef process_and_pivot_file(\n    doc_name: str,\n    main_account_map: Dict[str, str],\n    detailed_account_map: Optional[Dict[str, str]] = None, # Aceita mapa detalhado opcional\n    processed_dir: Path = PROCESSED_DIR,\n) -> Optional[pd.DataFrame]:\n    \"\"\"\n    Carrega raw parquet, filtra contas (principais E detalhadas),\n    mant√©m '√öLTIMO' exerc√≠cio e pivota para wide.\n    \"\"\"\n    input_file = processed_dir / f\"raw_{doc_name}.parquet\"\n    logger.info(\"Processando %s -> %s\", doc_name.upper(), input_file.name)\n\n    if not input_file.exists():\n        logger.warning(\"Arquivo n√£o encontrado: %s. Pulando %s.\", input_file.name, doc_name)\n        return None\n\n    df = _read_processed_parquet(input_file)\n    if df is None or df.empty: return None\n\n    # Validar colunas essenciais\n    essential_cols = [\"CD_CONTA\", \"VL_CONTA\", \"DT_FIM_EXERC\", \"ORDEM_EXERC\"] + INDEX_COLS\n    missing_essentials = [c for c in essential_cols if c not in df.columns]\n    if missing_essentials:\n        logger.error(\"Colunas essenciais ausentes em %s: %s. Pulando.\", input_file.name, missing_essentials)\n        return None\n\n    # --- L√ìGICA ATUALIZADA: Combinar contas principais e detalhadas ---\n    all_accounts_to_keep = list(main_account_map.keys())\n    if detailed_account_map:\n        all_accounts_to_keep.extend(list(detailed_account_map.keys()))\n        logger.info(f\"Contas detalhadas a serem extra√≠das: {list(detailed_account_map.keys())}\")\n\n    logger.info(f\"Total de c√≥digos de conta a serem filtrados: {len(all_accounts_to_keep)}\")\n    df = df[df[\"CD_CONTA\"].isin(all_accounts_to_keep)].copy()\n    logger.info(\"Ap√≥s filtrar contas (principais + detalhadas): shape=%s\", df.shape)\n    if df.empty:\n        logger.info(\"Nenhuma conta de interesse encontrada. Pulando.\")\n        return None\n    # --- FIM L√ìGICA ATUALIZADA ---\n\n    # Converter data e filtrar por ORDEM_EXERC == '√öLTIMO'\n    df[\"DT_FIM_EXERC\"] = pd.to_datetime(df[\"DT_FIM_EXERC\"], errors=\"coerce\")\n    df = df.dropna(subset=[\"DT_FIM_EXERC\"])\n    df = df[df[\"ORDEM_EXERC\"] == \"√öLTIMO\"].copy()\n    logger.info(\"Ap√≥s filtrar ORDEM_EXERC == '√öLTIMO': shape=%s\", df.shape)\n    if df.empty:\n        logger.info(\"Nenhuma linha '√öLTIMO' encontrada. Pulando.\")\n        return None\n\n    # --- L√ìGICA ATUALIZADA: Mapear nome da conta (prioriza detalhado) ---\n    # Primeiro tenta mapear com o mapa detalhado, depois com o principal\n    map_detalhado = detailed_account_map if detailed_account_map else {}\n    df[\"CONTA\"] = df[\"CD_CONTA\"].map(map_detalhado).fillna(df[\"CD_CONTA\"].map(main_account_map))\n    # Remove contas que n√£o foram mapeadas por nenhum dos mapas (se houver)\n    df = df.dropna(subset=[\"CONTA\"])\n    logger.info(\"Contas mapeadas. Exemplo de nomes: %s\", df[\"CONTA\"].unique()[:5])\n    if df.empty:\n        logger.info(\"Nenhuma conta mapeada resultou em nome v√°lido. Pulando.\")\n        return None\n    # --- FIM L√ìGICA ATUALIZADA ---\n\n    # Garantir VL_CONTA num√©rico e remover ausentes\n    df[\"VL_CONTA\"] = pd.to_numeric(df[\"VL_CONTA\"], errors=\"coerce\")\n    df = df.dropna(subset=[\"VL_CONTA\"])\n    if df.empty:\n        logger.info(\"Sem valores num√©ricos para VL_CONTA. Pulando.\")\n        return None\n\n    # Pivot (long -> wide)\n    try:\n        logger.info(\"Pivotando (long -> wide)...\")\n        # Usar fill_value=0 pode ser √∫til se algumas contas n√£o aparecem em todos os trimestres\n        df_wide = df.pivot_table(\n            index=INDEX_COLS,\n            columns=\"CONTA\", # Usar√° os nomes mapeados (incluindo os detalhados)\n            values=\"VL_CONTA\",\n            aggfunc=\"sum\",\n            fill_value=0 # Preenche com 0 contas ausentes naquele per√≠odo/empresa\n        )\n        df_wide = df_wide.reset_index()\n        df_wide.columns.name = None # Limpa o nome do √≠ndice das colunas\n        logger.info(\"Pivot conclu√≠do: shape=%s\", df_wide.shape)\n        logger.info(\"Colunas geradas pelo pivot: %s\", df_wide.columns.tolist())\n    except Exception as exc:\n        logger.exception(\"Erro ao pivotar %s: %s\", input_file.name, exc)\n        return None\n\n    del df\n    gc.collect()\n    return df_wide\n\n# -----------------------\n# Fun√ß√£o para juntar todos os DF wide (dre,bpa,bpp) - SEM ALTERA√á√ÉO\n# -----------------------\ndef merge_fundamentals(dfs_wide: Dict[str, pd.DataFrame]) -> Optional[pd.DataFrame]:\n    \"\"\" Junta os DataFrames wide (DRE, BPA, BPP) \"\"\"\n    if not dfs_wide:\n        logger.warning(\"Nenhum DataFrame wide fornecido para merge.\")\n        return None\n    valid_dfs = {k: v for k, v in dfs_wide.items() if v is not None and not v.empty}\n    if not valid_dfs:\n        logger.warning(\"Nenhum DataFrame wide V√ÅLIDO fornecido para merge.\")\n        return None\n\n    keys = list(valid_dfs.keys())\n    base = valid_dfs[keys[0]].copy()\n    logger.info(\"Usando %s como base para merge (shape=%s)\", keys[0], base.shape)\n\n    for k in keys[1:]:\n        logger.info(\"Mesclando com %s (shape=%s)\", k, valid_dfs[k].shape)\n        # Verifica colunas duplicadas (exceto as de √≠ndice) antes do merge\n        cols_to_merge = valid_dfs[k].columns.difference(base.columns).tolist() + INDEX_COLS\n        base = pd.merge(base, valid_dfs[k][cols_to_merge], on=INDEX_COLS, how=\"outer\")\n        logger.info(\"Shape ap√≥s merge com %s: %s\", k, base.shape)\n\n    base = base.sort_values(by=[\"CNPJ_CIA\", \"DT_FIM_EXERC\"]).reset_index(drop=True)\n    logger.info(\"Merge finalizado: shape=%s\", base.shape)\n    logger.info(\"Colunas finais: %s\", base.columns.tolist())\n    return base\n\n\ndef save_final(df: pd.DataFrame, final_dir: Path = FINAL_DIR, fname: str = \"fundamentals_wide.parquet\") -> Dict[str, str]:\n    \"\"\" Salva o DataFrame final em parquet e CSV \"\"\"\n    final_dir.mkdir(parents=True, exist_ok=True)\n    out_parquet = final_dir / fname\n    out_csv = final_dir / str(fname).replace(\".parquet\", \".csv\")\n\n    try:\n        logger.info(\"Salvando parquet final em: %s\", out_parquet)\n        df.to_parquet(out_parquet, index=False)\n        logger.info(\"Salvando CSV final em: %s\", out_csv)\n        df.to_csv(out_csv, index=False, sep=\";\", encoding=\"utf-8-sig\")\n        return {\"parquet\": str(out_parquet), \"csv\": str(out_csv)}\n    except Exception as exc:\n        logger.exception(\"Erro ao salvar arquivo final: %s\", exc)\n        raise\n\n# -----------------------\n# Execu√ß√£o principal (ATUALIZADA)\n# -----------------------\nif __name__ == \"__main__\":\n    logger.info(\"Iniciando transforma√ß√£o para formato WIDE (com contas detalhadas)...\")\n\n    dfs_wide = {}\n    # --- L√ìGICA ATUALIZADA: Iterar sobre a nova estrutura de mapas ---\n    for doc_type, maps_dict in MAPA_CONTAS_GERAL.items():\n        main_map = maps_dict.get(\"main\")\n        detailed_map = maps_dict.get(\"detailed\") # Pode ser None\n\n        if main_map: # S√≥ processa se houver um mapa principal\n            logger.info(\"-\" * 20)\n            df_wide = process_and_pivot_file(doc_type, main_map, detailed_map) # Passa ambos os mapas\n            if df_wide is not None and not df_wide.empty:\n                dfs_wide[doc_type] = df_wide\n            else:\n                logger.warning(\"Processamento de %s n√£o gerou DataFrame v√°lido.\", doc_type)\n        else:\n             logger.warning(\"Mapa principal n√£o definido para %s. Pulando.\", doc_type)\n    # --- FIM L√ìGICA ATUALIZADA ---\n\n    if not dfs_wide:\n        logger.error(\"Nenhum dataframe produzido. Encerrando sem salvar.\")\n    else:\n        logger.info(\"-\" * 20)\n        logger.info(\"Iniciando merge dos DataFrames DRE, BPA, BPP...\")\n        final_df = merge_fundamentals(dfs_wide)\n        if final_df is None or final_df.empty:\n            logger.error(\"DataFrame final vazio ap√≥s merge. Encerrando.\")\n        else:\n            save_paths = save_final(final_df)\n            logger.info(\"-\" * 20)\n            logger.info(\"Processamento conclu√≠do com sucesso!\")\n            logger.info(\"Arquivos salvos: %s\", save_paths)\n            logger.info(\"Shape final mestre: %s\", final_df.shape)\n            logger.info(\"Colunas finais geradas: %s\", final_df.columns.tolist())\n            logger.info(\"Amostra do resultado final:\\n%s\",\n                        final_df.head()[INDEX_COLS + list(final_df.columns.difference(INDEX_COLS))].to_string(index=False)) # Reordena colunas para amostra\n"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}