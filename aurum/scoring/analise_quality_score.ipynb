{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15f319e1",
   "metadata": {},
   "source": [
    "### **DocumentaÃ§Ã£o:** Script de Amostragem e AnÃ¡lise (AurumDataSampler)\n",
    "\n",
    "#### **1. Objetivo**\n",
    "\n",
    "Este Ã© um **script utilitÃ¡rio** de AnÃ¡lise ExploratÃ³ria de Dados (EDA). Seu principal objetivo Ã© ajudar o desenvolvedor a entender o ecossistema de dados do projeto Aurum, que estÃ¡ em constante evoluÃ§Ã£o.\n",
    "\n",
    "Ele **nÃ£o coleta** novos dados. Em vez disso, ele **varre** o diretÃ³rio `data/` em busca de todos os arquivos de dados `.parquet` jÃ¡ processados e, para cada um, executa as seguintes aÃ§Ãµes:\n",
    "1.  **Descobre:** Lista todos os datasets `.parquet` disponÃ­veis.\n",
    "2.  **Amostra:** Carrega uma amostra pequena e inteligente dos dados (usando amostragem estratificada por data, se possÃ­vel).\n",
    "3.  **Analisa:** Gera um relatÃ³rio bÃ¡sico de qualidade dos dados (contagem de linhas, colunas, tipos de dados, valores nulos, duplicatas e estatÃ­sticas numÃ©ricas).\n",
    "4.  **Reporta:** Salva as amostras em arquivos `.csv` fÃ¡ceis de visualizar e cria um relatÃ³rio consolidado em `.json`.\n",
    "\n",
    "Este script Ã© fundamental para validar rapidamente a saÃ­da de cada etapa do pipeline (coleta de notÃ­cias, processamento de fundamentos, etc.).\n",
    "\n",
    "#### **2. ConfiguraÃ§Ã£o (Input)**\n",
    "\n",
    "O script foi projetado para funcionar sem configuraÃ§Ã£o manual. Ele assume a seguinte estrutura de diretÃ³rio:\n",
    "\n",
    "* **data/ (DiretÃ³rio Raiz):** O script inicia sua busca aqui.\n",
    "* **.parquet** (Arquivos Alvo):** Ele procurarÃ¡ recursivamente em **todos** os subdiretÃ³rios dentro de data/ por qualquer arquivo que termine com a extensÃ£o .parquet.\n",
    "\n",
    "## 3. SaÃ­da (Output)\n",
    "\n",
    "Ao executar main(), o script cria um novo diretÃ³rio: data/samples/. Este diretÃ³rio conterÃ¡:\n",
    "\n",
    "1.  **Amostras em CSV (`*.sample.csv`):**\n",
    "    * `raw_news_data_sample.csv`\n",
    "    * `fundamentals_wide_sample.csv`\n",
    "    * `aurum_quality_scores_complete_sample.csv`\n",
    "    * *(...e um `.csv` para cada `.parquet` encontrado)*\n",
    "\n",
    "2.  **RelatÃ³rio Consolidado (`aurum_sampling_report.json`):**\n",
    "    * Um Ãºnico arquivo JSON que armazena os metadados e a anÃ¡lise de qualidade de todos os datasets processados. Este arquivo Ã© ideal para monitoramento programÃ¡tico do \"data health\" (saÃºde dos dados) do projeto.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2336353b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "import json\n",
    "\n",
    "# ConfiguraÃ§Ã£o de logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class AurumDataSampler:\n",
    "    \"\"\"\n",
    "    Classe para amostragem e anÃ¡lise exploratÃ³ria dos dados do Aurum\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str = \"data\"):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.sample_size = 100  # Tamanho padrÃ£o da amostra\n",
    "        \n",
    "    def list_available_datasets(self) -> Dict[str, Path]:\n",
    "        \"\"\"\n",
    "        Lista todos os datasets Parquet disponÃ­veis\n",
    "        \"\"\"\n",
    "        datasets = {}\n",
    "        \n",
    "        # Procurar recursivamente por arquivos .parquet\n",
    "        for parquet_file in self.base_path.rglob(\"*.parquet\"):\n",
    "            relative_path = parquet_file.relative_to(self.base_path)\n",
    "            datasets[str(relative_path)] = parquet_file\n",
    "            \n",
    "        return datasets\n",
    "    \n",
    "    def load_dataset_sample(self, dataset_path: Path, sample_size: int = None, \n",
    "                          random_state: int = 42) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Carrega uma amostra representativa do dataset\n",
    "        \"\"\"\n",
    "        if sample_size is None:\n",
    "            sample_size = self.sample_size\n",
    "            \n",
    "        logger.info(f\"ğŸ“Š Carregando amostra de {dataset_path}...\")\n",
    "        \n",
    "        try:\n",
    "            # Ler o dataset completo para obter metadados\n",
    "            full_df = pd.read_parquet(dataset_path)\n",
    "            total_rows = len(full_df)\n",
    "            \n",
    "            logger.info(f\"   ğŸ“ˆ Dataset completo: {total_rows} linhas, {len(full_df.columns)} colunas\")\n",
    "            \n",
    "            # EstratÃ©gias de amostragem baseadas no tamanho do dataset\n",
    "            if total_rows <= sample_size:\n",
    "                # Dataset pequeno - retornar tudo\n",
    "                sample_df = full_df.copy()\n",
    "                logger.info(\"   ğŸ” Dataset pequeno - retornando dados completos\")\n",
    "            else:\n",
    "                # Dataset grande - amostragem estratificada quando possÃ­vel\n",
    "                sample_df = self._stratified_sampling(full_df, sample_size, random_state)\n",
    "                \n",
    "            return sample_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erro ao carregar {dataset_path}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _stratified_sampling(self, df: pd.DataFrame, sample_size: int, random_state: int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Amostragem estratificada tentando manter proporÃ§Ãµes de categorias importantes\n",
    "        \"\"\"\n",
    "        sample_df = df.copy()\n",
    "        \n",
    "        # Tentar estratificar por ano se a coluna existir\n",
    "        year_columns = [col for col in sample_df.columns if 'ANO' in col or 'YEAR' in col or 'DATA' in col]\n",
    "        \n",
    "        if year_columns:\n",
    "            try:\n",
    "                year_col = year_columns[0]\n",
    "                # Amostragem estratificada por ano\n",
    "                stratified_sample = sample_df.groupby(year_col, group_keys=False).apply(\n",
    "                    lambda x: x.sample(n=min(len(x), max(1, sample_size // sample_df[year_col].nunique())), \n",
    "                                      random_state=random_state)\n",
    "                )\n",
    "                \n",
    "                # Se a amostra estratificada for muito pequena, completar com amostra aleatÃ³ria\n",
    "                if len(stratified_sample) < sample_size:\n",
    "                    remaining = sample_size - len(stratified_sample)\n",
    "                    additional_sample = sample_df.drop(stratified_sample.index).sample(\n",
    "                        n=remaining, random_state=random_state)\n",
    "                    stratified_sample = pd.concat([stratified_sample, additional_sample])\n",
    "                \n",
    "                logger.info(f\"   ğŸ¯ Amostragem estratificada por {year_col}: {len(stratified_sample)} linhas\")\n",
    "                return stratified_sample\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"   âš ï¸ Amostragem estratificada falhou: {e}\")\n",
    "        \n",
    "        # Fallback: amostra aleatÃ³ria simples\n",
    "        simple_sample = sample_df.sample(n=min(sample_size, len(sample_df)), random_state=random_state)\n",
    "        logger.info(f\"   ğŸ² Amostra aleatÃ³ria simples: {len(simple_sample)} linhas\")\n",
    "        return simple_sample\n",
    "    \n",
    "    def generate_basic_report(self, dataset_path: Path, sample_size: int = 50) -> Dict:\n",
    "        \"\"\"\n",
    "        Gera relatÃ³rio bÃ¡sico do dataset (sem dados complexos para JSON)\n",
    "        \"\"\"\n",
    "        # Carregar amostra\n",
    "        sample_df = self.load_dataset_sample(dataset_path, sample_size)\n",
    "        \n",
    "        if sample_df.empty:\n",
    "            return {\n",
    "                'dataset_name': dataset_path.name,\n",
    "                'dataset_path': str(dataset_path),\n",
    "                'error': 'Falha ao carregar dataset'\n",
    "            }\n",
    "        \n",
    "        # EstatÃ­sticas bÃ¡sicas\n",
    "        basic_stats = {\n",
    "            'total_rows_original': len(pd.read_parquet(dataset_path)),\n",
    "            'sample_size': len(sample_df),\n",
    "            'columns_count': len(sample_df.columns),\n",
    "            'memory_usage_mb': round(sample_df.memory_usage(deep=True).sum() / 1024**2, 2),\n",
    "            'columns_list': list(sample_df.columns),\n",
    "            'dtypes': {col: str(dtype) for col, dtype in sample_df.dtypes.items()}\n",
    "        }\n",
    "        \n",
    "        # InformaÃ§Ãµes de data\n",
    "        date_info = self._get_date_info(sample_df)\n",
    "        \n",
    "        # Qualidade bÃ¡sica dos dados\n",
    "        quality_info = self._get_basic_quality_info(sample_df)\n",
    "        \n",
    "        report = {\n",
    "            'dataset_name': dataset_path.name,\n",
    "            'dataset_path': str(dataset_path),\n",
    "            'basic_stats': basic_stats,\n",
    "            'date_info': date_info,\n",
    "            'quality_info': quality_info\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def _get_date_info(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Extrai informaÃ§Ãµes de data de forma segura\"\"\"\n",
    "        date_info = {}\n",
    "        date_columns = df.select_dtypes(include=['datetime64']).columns\n",
    "        \n",
    "        for col in date_columns:\n",
    "            try:\n",
    "                if col in df.columns and not df[col].isna().all():\n",
    "                    date_info[col] = {\n",
    "                        'min': str(df[col].min()),\n",
    "                        'max': str(df[col].max()),\n",
    "                        'null_count': int(df[col].isna().sum()),\n",
    "                        'null_percentage': round((df[col].isna().sum() / len(df)) * 100, 2)\n",
    "                    }\n",
    "            except Exception as e:\n",
    "                date_info[col] = {'error': f'Erro ao processar: {str(e)}'}\n",
    "        \n",
    "        return date_info\n",
    "    \n",
    "    def _get_basic_quality_info(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Analisa qualidade bÃ¡sica dos dados\"\"\"\n",
    "        quality_info = {\n",
    "            'total_rows': len(df),\n",
    "            'complete_cases': len(df.dropna()),\n",
    "            'duplicate_rows': int(df.duplicated().sum()),\n",
    "            'columns_quality': {}\n",
    "        }\n",
    "        \n",
    "        for column in df.columns:\n",
    "            try:\n",
    "                col_data = df[column]\n",
    "                col_info = {\n",
    "                    'dtype': str(col_data.dtype),\n",
    "                    'null_count': int(col_data.isna().sum()),\n",
    "                    'null_percentage': round((col_data.isna().sum() / len(df)) * 100, 2),\n",
    "                    'unique_count': int(col_data.nunique())\n",
    "                }\n",
    "                \n",
    "                # EstatÃ­sticas para colunas numÃ©ricas\n",
    "                if pd.api.types.is_numeric_dtype(col_data):\n",
    "                    col_info.update({\n",
    "                        'mean': float(col_data.mean()) if not col_data.isna().all() else None,\n",
    "                        'std': float(col_data.std()) if not col_data.isna().all() else None,\n",
    "                        'min': float(col_data.min()) if not col_data.isna().all() else None,\n",
    "                        'max': float(col_data.max()) if not col_data.isna().all() else None\n",
    "                    })\n",
    "                \n",
    "                quality_info['columns_quality'][column] = col_info\n",
    "                \n",
    "            except Exception as e:\n",
    "                quality_info['columns_quality'][column] = {'error': f'Erro na anÃ¡lise: {str(e)}'}\n",
    "        \n",
    "        return quality_info\n",
    "    \n",
    "    def create_simple_sampling_report(self, output_dir: str = \"data/samples\") -> Dict:\n",
    "        \"\"\"\n",
    "        Cria relatÃ³rio simplificado de amostragem para todos os datasets\n",
    "        \"\"\"\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        datasets = self.list_available_datasets()\n",
    "        reports = {}\n",
    "        \n",
    "        logger.info(\"ğŸ” INICIANDO AMOSTRAGEM DOS DATASETS AURUM\")\n",
    "        logger.info(f\"ğŸ“ Encontrados {len(datasets)} datasets:\")\n",
    "        \n",
    "        for name, path in datasets.items():\n",
    "            logger.info(f\"   ğŸ“Š {name}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ¯ RELATÃ“RIO DE AMOSTRAGEM - AURUM DATA SAMPLES\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for dataset_name, dataset_path in datasets.items():\n",
    "            print(f\"\\nğŸ“ DATASET: {dataset_name}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Gerar relatÃ³rio bÃ¡sico\n",
    "            report = self.generate_basic_report(dataset_path)\n",
    "            reports[dataset_name] = report\n",
    "            \n",
    "            # Carregar amostra para salvar como CSV\n",
    "            sample_df = self.load_dataset_sample(dataset_path, 50)\n",
    "            \n",
    "            if not sample_df.empty:\n",
    "                # Salvar amostra como CSV\n",
    "                sample_csv_path = output_path / f\"{dataset_path.stem}_sample.csv\"\n",
    "                sample_df.to_csv(sample_csv_path, index=False, encoding='utf-8')\n",
    "                \n",
    "                # Mostrar resumo no console\n",
    "                basic_stats = report['basic_stats']\n",
    "                print(f\"ğŸ“ˆ EstatÃ­sticas:\")\n",
    "                print(f\"   â€¢ Linhas totais: {basic_stats['total_rows_original']:,}\")\n",
    "                print(f\"   â€¢ Amostra: {basic_stats['sample_size']} linhas\")\n",
    "                print(f\"   â€¢ Colunas: {basic_stats['columns_count']}\")\n",
    "                print(f\"   â€¢ Uso de memÃ³ria: {basic_stats['memory_usage_mb']:.2f} MB\")\n",
    "                \n",
    "                # Mostrar primeiras linhas\n",
    "                print(f\"\\nğŸ‘€ Primeiras 3 linhas da amostra:\")\n",
    "                print(sample_df.head(3).to_string(index=False))\n",
    "                \n",
    "                print(f\"\\nğŸ’¾ Amostra salva em: {sample_csv_path}\")\n",
    "            else:\n",
    "                print(\"âŒ NÃ£o foi possÃ­vel carregar amostra deste dataset\")\n",
    "        \n",
    "        # Salvar relatÃ³rio consolidado (agora seguro para JSON)\n",
    "        report_path = output_path / \"aurum_sampling_report.json\"\n",
    "        \n",
    "        try:\n",
    "            with open(report_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(reports, f, indent=2, ensure_ascii=False, default=str)\n",
    "            logger.info(f\"ğŸ“‹ RelatÃ³rio consolidado salvo em: {report_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Erro ao salvar relatÃ³rio JSON: {e}\")\n",
    "        \n",
    "        return reports\n",
    "\n",
    "    def quick_preview(self, dataset_pattern: str = None, sample_size: int = 10):\n",
    "        \"\"\"\n",
    "        VisualizaÃ§Ã£o rÃ¡pida dos datasets\n",
    "        \"\"\"\n",
    "        datasets = self.list_available_datasets()\n",
    "        \n",
    "        print(\"ğŸš€ VISUALIZAÃ‡ÃƒO RÃPIDA DOS DATASETS AURUM\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for name, path in datasets.items():\n",
    "            if dataset_pattern and dataset_pattern.lower() not in name.lower():\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nğŸ“Š {name}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            sample_df = self.load_dataset_sample(path, sample_size)\n",
    "            if not sample_df.empty:\n",
    "                print(f\"ğŸ“‹ Shape: {sample_df.shape}\")\n",
    "                print(f\"ğŸ¯ Amostra de {len(sample_df)} linhas:\")\n",
    "                print(sample_df.to_string(index=False))\n",
    "            else:\n",
    "                print(\"âŒ NÃ£o foi possÃ­vel carregar amostra\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# FunÃ§Ãµes de uso rÃ¡pido\n",
    "def quick_sample(dataset_pattern: str, sample_size: int = 15) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    FunÃ§Ã£o rÃ¡pida para obter amostra de datasets que correspondam ao padrÃ£o\n",
    "    \"\"\"\n",
    "    sampler = AurumDataSampler()\n",
    "    datasets = sampler.list_available_datasets()\n",
    "    \n",
    "    matching_datasets = []\n",
    "    for name, path in datasets.items():\n",
    "        if dataset_pattern.lower() in name.lower():\n",
    "            matching_datasets.append((name, path))\n",
    "    \n",
    "    if not matching_datasets:\n",
    "        available = \"\\n\".join(datasets.keys())\n",
    "        print(f\"âŒ Nenhum dataset encontrado com '{dataset_pattern}'.\")\n",
    "        print(f\"ğŸ“ Datasets disponÃ­veis:\\n{available}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if len(matching_datasets) == 1:\n",
    "        name, path = matching_datasets[0]\n",
    "        sample = sampler.load_dataset_sample(path, sample_size)\n",
    "        print(f\"ğŸ¯ Amostra de {name} ({len(sample)} linhas):\")\n",
    "        print(sample.to_string(index=False))\n",
    "        return sample\n",
    "    else:\n",
    "        print(f\"ğŸ” MÃºltiplos datasets encontrados com '{dataset_pattern}':\")\n",
    "        for i, (name, path) in enumerate(matching_datasets, 1):\n",
    "            print(f\"   {i}. {name}\")\n",
    "        \n",
    "        choice = input(\"ğŸ‘‰ Escolha o nÃºmero do dataset (ou Enter para o primeiro): \").strip()\n",
    "        try:\n",
    "            choice_idx = int(choice) - 1 if choice else 0\n",
    "            name, path = matching_datasets[choice_idx]\n",
    "            sample = sampler.load_dataset_sample(path, sample_size)\n",
    "            print(f\"ğŸ¯ Amostra de {name} ({len(sample)} linhas):\")\n",
    "            print(sample.to_string(index=False))\n",
    "            return sample\n",
    "        except (ValueError, IndexError):\n",
    "            print(\"âŒ Escolha invÃ¡lida.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "def list_datasets():\n",
    "    \"\"\"Lista todos os datasets disponÃ­veis\"\"\"\n",
    "    sampler = AurumDataSampler()\n",
    "    datasets = sampler.list_available_datasets()\n",
    "    \n",
    "    print(\"ğŸ“ DATASETS DISPONÃVEIS NO AURUM:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, (name, path) in enumerate(datasets.items(), 1):\n",
    "        try:\n",
    "            # Tentar obter informaÃ§Ãµes bÃ¡sicas\n",
    "            df_sample = pd.read_parquet(path, nrows=1)\n",
    "            print(f\"{i:2d}. {name}\")\n",
    "            print(f\"    ğŸ“Š Colunas: {len(df_sample.columns)}, Estilo: {df_sample.shape[1]}xN\")\n",
    "            print(f\"    ğŸ“ {path}\")\n",
    "        except:\n",
    "            print(f\"{i:2d}. {name} (âŒ erro ao carregar)\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# ExecuÃ§Ã£o principal\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Executa amostragem completa de todos os datasets\n",
    "    \"\"\"\n",
    "    sampler = AurumDataSampler()\n",
    "    reports = sampler.create_simple_sampling_report()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… AMOSTRAGEM CONCLUÃDA!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Resumo final\n",
    "    total_datasets = len(reports)\n",
    "    successful_samples = sum(1 for report in reports.values() if 'error' not in report)\n",
    "    \n",
    "    print(f\"ğŸ“Š Resumo Final:\")\n",
    "    print(f\"   â€¢ Datasets processados: {total_datasets}\")\n",
    "    print(f\"   â€¢ Amostras geradas com sucesso: {successful_samples}\")\n",
    "    print(f\"   â€¢ Local das amostras: data/samples/\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ COMANDOS RÃPIDOS:\")\n",
    "    print(f\"   â€¢ list_datasets() - Lista todos os datasets\")\n",
    "    print(f\"   â€¢ quick_sample('quality') - Amostra de datasets de qualidade\")\n",
    "    print(f\"   â€¢ quick_sample('news') - Amostra de datasets de notÃ­cias\")\n",
    "    print(f\"   â€¢ quick_sample('fundamentals') - Amostra de dados fundamentalistas\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Exemplos de uso apÃ³s execuÃ§Ã£o\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸš€ EXEMPLOS DE USO RÃPIDO - EXECUTE APÃ“S O SCRIPT:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "# Listar todos os datasets\n",
    "list_datasets()\n",
    "\n",
    "# Amostra rÃ¡pida de datasets de qualidade\n",
    "quick_sample('quality', 10)\n",
    "\n",
    "# Amostra de dados fundamentalistas  \n",
    "quick_sample('fundamental', 15)\n",
    "\n",
    "# Amostra de notÃ­cias\n",
    "quick_sample('news', 8)\n",
    "\n",
    "# VisualizaÃ§Ã£o rÃ¡pida de tudo\n",
    "sampler = AurumDataSampler()\n",
    "sampler.quick_preview()\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
