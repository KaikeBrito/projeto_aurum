{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7659e3a",
   "metadata": {},
   "source": [
    "### **Documenta√ß√£o:** Calculadora de Quality Score (AurumQualityScoreCalculator)\n",
    "\n",
    "#### **1. Objetivo**\n",
    "\n",
    "Este script √© um dos pilares centrais do **Pilar 1 (Qualidade Financeira)** do projeto Aurum. Sua responsabilidade √© transformar os dados fundamentalistas brutos (extra√≠dos da CVM e formatados no `fundamentals_wide.parquet`) em m√©tricas de performance acion√°veis.\n",
    "\n",
    "O script executa um pipeline completo que:\n",
    "1.  Carrega os dados brutos da CVM.\n",
    "2.  Calcula os valores **TTM (Trailing Twelve Months / √öltimos 12 Meses)**, corrigindo a natureza \"Acumulada no Ano\" (YTD) dos dados da CVM.\n",
    "3.  Calcula um conjunto abrangente de **ratios financeiros** (Rentabilidade, Margens, Alavancagem, etc.) usando os dados TTM.\n",
    "4.  Calcula um **Aurum Quality Score** inicial, baseado em um *ranking percentile cross-sectional* (por data) desses ratios.\n",
    "5.  Salva os resultados hist√≥ricos e os mais recentes.\n",
    "\n",
    "#### **2. Configura√ß√£o (Input)**\n",
    "\n",
    "O script depende de um √∫nico arquivo de entrada, que deve ser gerado pelo pipeline de processamento da CVM:\n",
    "\n",
    "* **`data/cvm/final/fundamentals_wide.parquet**: Um arquivo Parquet contendo os dados de Balan√ßo Patrimonial (BP) e Demonstra√ß√£o do Resultado (DRE) em formato \"wide\" (uma linha por empresa/data, m√∫ltiplas colunas de m√©tricas).\n",
    "    * **Importante:** O script assume que os dados da DRE (ex: `Receita L√≠quida`, `Lucro Bruto`) est√£o no formato **YTD (Acumulado no Ano)**, que √© o padr√£o da CVM.\n",
    "\n",
    "#### 3. Pipeline de Execu√ß√£o (Passo a Passo)\n",
    "\n",
    "A classe **AurumQualityScoreCalculator** gerencia todo o fluxo de trabalho:\n",
    "\n",
    "##### **Passo 1:** Carregar e Preparar Dados (`load_and_prepare_data`)\n",
    "\n",
    "* Carrega o arquivo `fundamentals_wide.parquet`.\n",
    "* Chama _clean_fundamentals_data para:\n",
    "    * Converter DT_FIM_EXERC para datetime.\n",
    "    * Remover quaisquer linhas duplicadas por CNPJ_CIA e DT_FIM_EXERC.\n",
    "    * **Ordenar** os dados por CNPJ_CIA e DT_FIM_EXERC, o que √© **essencial** para o c√°lculo de TTM.\n",
    "\n",
    "##### **Passo 2:** C√°lculo de TTM (`calculate_ttm_data`)\n",
    "\n",
    "Este √© o passo mais cr√≠tico do script. Ele converte os dados de fluxo (DRE) de YTD para TTM.\n",
    "\n",
    "* **L√≥gica:** O script agrupa por `CNPJ_CIA` e aplica a fun√ß√£o `rolling_sum_group`.\n",
    "* **Desacumula√ß√£o (YTD -> Trimestral):** Para cada coluna de DRE, ele calcula o valor trimestral fazendo:\n",
    "    quarterly = group[col] - group[col].shift(1).fillna(0)\n",
    "* **Anualiza√ß√£o (Trimestral -> TTM):** Em seguida, ele calcula a soma m√≥vel dos √∫ltimos 4 valores trimestrais:\n",
    "    group[f'{col}_ttm'] = quarterly.rolling(window=4, min_periods=4).sum()\n",
    "* O resultado √© salvo em self.ratios_df, pronto para o c√°lculo dos ratios.\n",
    "\n",
    "##### Passo 3: C√°lculo de Ratios Financeiros (`calculate_financial_ratios`)\n",
    "\n",
    "* Usando o self.ratios_df (que agora cont√©m as colunas `_ttm`), este m√©todo calcula os principais indicadores financeiros.\n",
    "* **Categorias:**\n",
    "    * **Contas de D√≠vida: D√≠vida Bruta, D√≠vida L√≠quida, Capital Investido**.\n",
    "    * **Rentabilidade (com TTM):** ROE, ROA, ROIC.\n",
    "    * **Margens (com TTM):** MARGEM_EBIT, MARGEM_LIQUIDA, MARGEM_BRUTA.\n",
    "    * **Alavancagem:** ALAVANCAGEM (Passivo/Ativo), DIVIDA_PL, DIVIDA_LIQ_EBIT.\n",
    "    * **Liquidez:** LIQUIDEZ_CORRENTE.\n",
    "    * **Efici√™ncia (com TTM):** GIRO_ATIVO.\n",
    "* Usa safe_divide para evitar erros de divis√£o por zero.\n",
    "\n",
    "##### Passo 3.1: Tratamento de Outliers (`_handle_ratio_outliers`)\n",
    "\n",
    "* Ap√≥s o c√°lculo, os ratios s√£o limpos.\n",
    "* Valores `infinitos` s√£o substitu√≠dos por `NaN`.\n",
    "* Os dados s√£o \"winsorizados\": valores extremos s√£o \"clipados\" (limitados) aos **percentis 1% (inferior) e 99% (superior)**. Isso torna o scoring subsequente mais robusto.\n",
    "\n",
    "##### Passo 4: C√°lculo do Score de Qualidade (`calculate_quality_scores`)\n",
    "\n",
    "Este m√©todo implementa a **Metodologia de Ranking Cross-Sectional**.\n",
    "\n",
    "* **Configura√ß√£o:** Define as m√©tricas, pesos e a dire√ß√£o (ex: `ROIC` -> maior √© melhor, `DIVIDA_LIQ_EBIT` -> menor √© melhor).\n",
    "* **Ranking por Data:** O script agrupa os dados por `DT_FIM_EXERC` (`grouped_by_date`).\n",
    "* Para cada m√©trica, ele calcula o **ranking percentile** de cada empresa *dentro daquele per√≠odo*:\n",
    "    `scores_df[score_col] = grouped_by_date[metric].rank(pct=True) * 100`\n",
    "* **Score Composto:** O `aurum_quality_score` √© calculado como a soma ponderada desses rankings (percentis de 0 a 100). Valores `NaN` s√£o preenchidos com a mediana (50) para n√£o penalizar empresas excessivamente.\n",
    "* **Classifica√ß√£o Final:** O script calcula `quality_quintile` e `quality_grade` (A-E) com base no ranking *final* do `aurum_quality_score` de cada per√≠odo.\n",
    "\n",
    "##### Passo 5 e 6: Salvar Resultados (`get_latest_scores` e `save_results`)\n",
    "\n",
    "* O script gera e salva quatro arquivos distintos no diret√≥rio `data/aurum_scores_output/`.\n",
    "\n",
    "#### 6. Fluxograma\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    %% --- INPUT ---\n",
    "    Input[(\"üìÇ <b>Input</b><br/>fundamentals_wide.parquet\")]\n",
    "    \n",
    "    %% --- PREPARA√á√ÉO ---\n",
    "    subgraph Prep [\"1. Prepara√ß√£o\"]\n",
    "        Clean[\"<b>Limpeza & Ordena√ß√£o</b><br/>Sort by: CNPJ + Data\"]\n",
    "    end\n",
    "    \n",
    "    %% --- TTM ---\n",
    "    subgraph TTM [\"2. C√°lculo TTM\"]\n",
    "        Desac[\"<b>Desacumula√ß√£o</b><br/>YTD ‚Üí Trimestral\"]\n",
    "        Roll[\"<b>Soma M√≥vel (Rolling 4)</b><br/>Trimestral ‚Üí TTM (12 Meses)\"]\n",
    "    end\n",
    "    \n",
    "    %% --- RATIOS ---\n",
    "    subgraph Ratios [\"3. Ratios Financeiros\"]\n",
    "        CalcRat[\"<b>C√°lculo de Indicadores</b><br/>ROIC, ROE, D√≠vida L√≠quida...\"]\n",
    "        Outliers[\"<b>Tratamento de Outliers</b><br/>Winsorization (Clip 1% - 99%)\"]\n",
    "    end\n",
    "    \n",
    "    %% --- SCORING ---\n",
    "    subgraph Score [\"4. Scoring Cross-Sectional\"]\n",
    "        Group[\"<b>Agrupamento por Data</b><br/>(Compara√ß√£o Temporal Justa)\"]\n",
    "        Rank[\"<b>Ranking Percentile</b><br/>0 a 100 para cada m√©trica\"]\n",
    "        Weight[\"<b>M√©dia Ponderada</b><br/>Aplica√ß√£o dos Pesos (ex: ROIC 25%)\"]\n",
    "        Grade[\"<b>Classifica√ß√£o Final</b><br/>Quintis e Notas (A-E)\"]\n",
    "    end\n",
    "    \n",
    "    %% --- OUTPUTS ---\n",
    "    subgraph Outputs [\"5. Sa√≠da\"]\n",
    "        Hist[(\"üíæ <b>Hist√≥rico Completo</b><br/>.parquet / .csv\")]\n",
    "        Latest[(\"üíæ <b>√öltimo Per√≠odo</b><br/>.parquet / .csv\")]\n",
    "        Stats[(\"üìÑ <b>Relat√≥rio Estat√≠stico</b><br/>.txt\")]\n",
    "    end\n",
    "\n",
    "    %% --- LIGA√á√ïES ---\n",
    "    Input --> Clean\n",
    "    Clean --> Desac --> Roll\n",
    "    Roll --> CalcRat --> Outliers\n",
    "    Outliers --> Group --> Rank --> Weight --> Grade\n",
    "    Grade --> Hist & Latest & Stats\n",
    "```\n",
    "#### 7. Sa√≠da (Output)\n",
    "\n",
    "A execu√ß√£o do script gera os seguintes arquivos em `data/aurum_scores_output/`:\n",
    "\n",
    "1.  **`aurum_quality_scores_complete.parquet`**:\n",
    "    * O arquivo mais importante. Cont√©m o **hist√≥rico completo** de todas as empresas, datas, ratios calculados e scores. Este ser√° o input para o `AurumScoringSystem` avan√ßado e para o *backtesting*.\n",
    "\n",
    "2.  **`aurum_quality_scores_latest.parquet`**:\n",
    "    * Um arquivo de conveni√™ncia que cont√©m apenas o **√∫ltimo registro (data mais recente)** de score/ratios para cada empresa.\n",
    "\n",
    "3.  **`aurum_quality_scores_latest.csv`**:\n",
    "    * Vers√£o CSV do arquivo acima, para f√°cil visualiza√ß√£o em planilhas.\n",
    "\n",
    "4.  **`aurum_scores_statistics.txt`**:\n",
    "    * Um relat√≥rio de texto (`.txt`) leg√≠vel, mostrando as estat√≠sticas do √∫ltimo per√≠odo (distribui√ß√£o de notas, Top 10 empresas) para uma verifica√ß√£o r√°pida.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b00a779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 17:50:20,381 - INFO - üöÄ INICIANDO PIPELINE DO AURUM QUALITY SCORE (Vers√£o Final com ROIC Correto)\n",
      "2025-12-17 17:50:20,400 - INFO - üì• 1. CARREGANDO NOVO fundamentals_wide.parquet...\n",
      "2025-12-17 17:50:20,730 - INFO - ‚úÖ Dados carregados: (21559, 19)\n",
      "2025-12-17 17:50:20,732 - INFO - Colunas encontradas: ['CNPJ_CIA', 'DENOM_CIA', 'DT_FIM_EXERC', 'Custo dos Bens e/ou Servi√ßos Vendidos', 'EBIT', 'EBT', 'Lucro Bruto', 'Lucro L√≠quido Consolidado', 'Receita L√≠quida', 'Ativo Circulante', 'Ativo N√£o Circulante', 'Ativo Total', 'Caixa e Equivalentes', 'D√≠vida Curto Prazo', 'D√≠vida Longo Prazo', 'Passivo Circulante', 'Passivo N√£o Circulante', 'Passivo Total', 'Patrim√¥nio L√≠quido Consolidado']\n",
      "2025-12-17 17:50:20,740 - INFO - ‚úÖ Novas colunas (Caixa, D√≠vida CP, D√≠vida LP) encontradas!\n",
      "2025-12-17 17:50:20,861 - INFO - ‚è≥ 2. CALCULANDO TTM...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 729/729 [00:03<00:00, 196.08it/s]\n",
      "2025-12-17 17:50:24,651 - INFO - ‚úÖ TTM calculado.\n",
      "2025-12-17 17:50:24,653 - INFO - üßÆ 3. CALCULANDO RATIOS FINANCEIROS (COM TTM)...\n",
      "2025-12-17 17:50:24,666 - INFO - ‚úÖ D√≠vida Bruta, Capital Investido e D√≠vida L√≠quida calculados.\n",
      "2025-12-17 17:50:24,668 - INFO - üìà Calculando RENTABILIDADE...\n",
      "2025-12-17 17:50:24,679 - INFO - ‚úÖ ROIC (correto) calculado. M√©dia: 98.7966\n",
      "2025-12-17 17:50:24,689 - INFO - ‚öñÔ∏è Calculando ALAVANCAGEM...\n",
      "2025-12-17 17:50:24,696 - INFO - ‚úÖ Ratios de alavancagem (incluindo DIVIDA_PL, DIVIDA_LIQ_EBIT) calculados.\n",
      "2025-12-17 17:50:24,796 - INFO - üéØ RATIOS CALCULADOS e limpos.\n",
      "2025-12-17 17:50:24,799 - INFO - Colunas FINAIS de Ratios: ['CNPJ_CIA', 'DENOM_CIA', 'DT_FIM_EXERC', 'Custo dos Bens e/ou Servi√ßos Vendidos', 'EBIT', 'EBT', 'Lucro Bruto', 'Lucro L√≠quido Consolidado', 'Receita L√≠quida', 'Ativo Circulante', 'Ativo N√£o Circulante', 'Ativo Total', 'Caixa e Equivalentes', 'D√≠vida Curto Prazo', 'D√≠vida Longo Prazo', 'Passivo Circulante', 'Passivo N√£o Circulante', 'Passivo Total', 'Patrim√¥nio L√≠quido Consolidado', 'Receita L√≠quida_ttm', 'Custo dos Bens e/ou Servi√ßos Vendidos_ttm', 'Lucro Bruto_ttm', 'EBIT_ttm', 'EBT_ttm', 'Lucro L√≠quido Consolidado_ttm', 'D√≠vida Bruta', 'Capital Investido', 'D√≠vida L√≠quida', 'ROE', 'ROA', 'ROIC', 'MARGEM_EBIT', 'MARGEM_LIQUIDA', 'MARGEM_BRUTA', 'ALAVANCAGEM', 'DIVIDA_PL', 'DIVIDA_LIQ_EBIT', 'LIQUIDEZ_CORRENTE', 'GIRO_ATIVO']\n",
      "2025-12-17 17:50:24,800 - INFO - üéØ 4. CALCULANDO SCORES DE QUALIDADE...\n",
      "2025-12-17 17:50:24,815 - INFO - Configura√ß√£o de m√©tricas para score: {'ROIC': 0.25, 'ROE': 0.15, 'MARGEM_EBIT': 0.15, 'MARGEM_LIQUIDA': 0.1, 'DIVIDA_LIQ_EBIT': 0.15, 'LIQUIDEZ_CORRENTE': 0.1, 'GIRO_ATIVO': 0.1}\n",
      "2025-12-17 17:50:24,832 - INFO -   ‚úÖ Score ROIC calculado.\n",
      "2025-12-17 17:50:24,841 - INFO -   ‚úÖ Score ROE calculado.\n",
      "2025-12-17 17:50:24,856 - INFO -   ‚úÖ Score MARGEM_EBIT calculado.\n",
      "2025-12-17 17:50:24,866 - INFO -   ‚úÖ Score MARGEM_LIQUIDA calculado.\n",
      "2025-12-17 17:50:24,877 - INFO -   ‚úÖ Score DIVIDA_LIQ_EBIT calculado.\n",
      "2025-12-17 17:50:24,884 - INFO -   ‚úÖ Score LIQUIDEZ_CORRENTE calculado.\n",
      "2025-12-17 17:50:24,894 - INFO -   ‚úÖ Score GIRO_ATIVO calculado.\n",
      "2025-12-17 17:50:24,895 - INFO - ‚öñÔ∏è Calculando score composto...\n",
      "2025-12-17 17:50:24,909 - INFO - üèÜ Classificando empresas...\n",
      "2025-12-17 17:50:24,932 - INFO - Colunas FINAIS ANTES de salvar: ['CNPJ_CIA', 'DENOM_CIA', 'DT_FIM_EXERC', 'Custo dos Bens e/ou Servi√ßos Vendidos', 'EBIT', 'EBT', 'Lucro Bruto', 'Lucro L√≠quido Consolidado', 'Receita L√≠quida', 'Ativo Circulante', 'Ativo N√£o Circulante', 'Ativo Total', 'Caixa e Equivalentes', 'D√≠vida Curto Prazo', 'D√≠vida Longo Prazo', 'Passivo Circulante', 'Passivo N√£o Circulante', 'Passivo Total', 'Patrim√¥nio L√≠quido Consolidado', 'Receita L√≠quida_ttm', 'Custo dos Bens e/ou Servi√ßos Vendidos_ttm', 'Lucro Bruto_ttm', 'EBIT_ttm', 'EBT_ttm', 'Lucro L√≠quido Consolidado_ttm', 'D√≠vida Bruta', 'Capital Investido', 'D√≠vida L√≠quida', 'ROE', 'ROA', 'ROIC', 'MARGEM_EBIT', 'MARGEM_LIQUIDA', 'MARGEM_BRUTA', 'ALAVANCAGEM', 'DIVIDA_PL', 'DIVIDA_LIQ_EBIT', 'LIQUIDEZ_CORRENTE', 'GIRO_ATIVO', 'score_ROIC', 'score_ROE', 'score_MARGEM_EBIT', 'score_MARGEM_LIQUIDA', 'score_DIVIDA_LIQ_EBIT', 'score_LIQUIDEZ_CORRENTE', 'score_GIRO_ATIVO', 'aurum_quality_score', 'final_rank', 'quality_quintile', 'quality_grade']\n",
      "2025-12-17 17:50:24,934 - INFO - Salvando DataFrame com colunas: ['CNPJ_CIA', 'DENOM_CIA', 'DT_FIM_EXERC', 'Custo dos Bens e/ou Servi√ßos Vendidos', 'EBIT', 'EBT', 'Lucro Bruto', 'Lucro L√≠quido Consolidado', 'Receita L√≠quida', 'Ativo Circulante', 'Ativo N√£o Circulante', 'Ativo Total', 'Caixa e Equivalentes', 'D√≠vida Curto Prazo', 'D√≠vida Longo Prazo', 'Passivo Circulante', 'Passivo N√£o Circulante', 'Passivo Total', 'Patrim√¥nio L√≠quido Consolidado', 'Receita L√≠quida_ttm', 'Custo dos Bens e/ou Servi√ßos Vendidos_ttm', 'Lucro Bruto_ttm', 'EBIT_ttm', 'EBT_ttm', 'Lucro L√≠quido Consolidado_ttm', 'D√≠vida Bruta', 'Capital Investido', 'D√≠vida L√≠quida', 'ROE', 'ROA', 'ROIC', 'MARGEM_EBIT', 'MARGEM_LIQUIDA', 'MARGEM_BRUTA', 'ALAVANCAGEM', 'DIVIDA_PL', 'DIVIDA_LIQ_EBIT', 'LIQUIDEZ_CORRENTE', 'GIRO_ATIVO', 'score_ROIC', 'score_ROE', 'score_MARGEM_EBIT', 'score_MARGEM_LIQUIDA', 'score_DIVIDA_LIQ_EBIT', 'score_LIQUIDEZ_CORRENTE', 'score_GIRO_ATIVO', 'aurum_quality_score', 'final_rank', 'quality_quintile', 'quality_grade']\n",
      "2025-12-17 17:50:27,495 - INFO - üíæ Scores completos (hist√≥rico) salvos em:\n",
      "2025-12-17 17:50:27,496 - INFO -    -> ..\\data\\aurum_scores\\aurum_quality_scores_complete.parquet\n",
      "2025-12-17 17:50:27,498 - INFO -    -> ..\\data\\aurum_scores\\aurum_quality_scores_complete.csv\n",
      "2025-12-17 17:50:27,622 - INFO - üíæ Scores mais recentes salvos: ..\\data\\aurum_scores\\aurum_quality_scores_latest.parquet e ..\\data\\aurum_scores\\aurum_quality_scores_latest.csv\n",
      "2025-12-17 17:50:27,641 - INFO - üíæ Estat√≠sticas salvas: ..\\data\\aurum_scores\\aurum_scores_statistics.txt\n",
      "2025-12-17 17:50:27,643 - INFO - üíæ Resultados (com ROIC correto) salvos em: {'complete_parquet': WindowsPath('../data/aurum_scores/aurum_quality_scores_complete.parquet'), 'complete_csv': WindowsPath('../data/aurum_scores/aurum_quality_scores_complete.csv'), 'latest_parquet': WindowsPath('../data/aurum_scores/aurum_quality_scores_latest.parquet'), 'statistics': WindowsPath('../data/aurum_scores/aurum_scores_statistics.txt')}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéâ AURUM QUALITY SCORE - RESULTADOS FINAIS (√öLTIMO PER√çODO)\n",
      "============================================================\n",
      "\n",
      "üìä TOTAL DE EMPRESAS: 729\n",
      "üìà SCORE M√âDIO: 49.27\n",
      "\n",
      "üìã DISTRIBUI√á√ÉO DAS NOTAS:\n",
      "   Nota A: 191 empresas\n",
      "   Nota B:  90 empresas\n",
      "   Nota C:  83 empresas\n",
      "   Nota D: 147 empresas\n",
      "   Nota E: 218 empresas\n",
      "\n",
      "ü•á TOP 10 EMPRESAS:\n",
      "    1. CAMIL ALIMENTOS S.A.                100.00 (Nota A)\n",
      "    2. MINUPAR PARTICIPACOES S.A.           90.07 (Nota A)\n",
      "    3. 521 PARTICIPACOES S.A. - EM LIQUIDA  89.17 (Nota A)\n",
      "    4. STEIN SP II PARTICIPA√á√ïES S.A.       87.37 (Nota A)\n",
      "    5. COMERCIAL QUINTELLA COM EXP SA EM L  84.67 (Nota A)\n",
      "    6. SONDOTECNICA ENGENHARIA SOLOS S.A.   83.36 (Nota A)\n",
      "    7. EPR INFRAESTRUTURA PR S.A.           83.19 (Nota A)\n",
      "    8. M√âLIUZ S.A.                          82.29 (Nota A)\n",
      "    9. RIVA INCORPORADORA S.A               79.77 (Nota A)\n",
      "   10. AURA ALMAS MINERA√á√ÉO S.A.            79.68 (Nota A)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "tqdm.pandas()\n",
    "\n",
    "class AurumQualityScoreCalculator:\n",
    "    \"\"\" Calcula TTM, ratios financeiros (incluindo ROIC correto) e scores \"\"\"\n",
    "\n",
    "    def __init__(self, fundamentals_path: str):\n",
    "        self.fundamentals_path = Path(fundamentals_path)\n",
    "        self.fundamentals_df = None\n",
    "        self.ratios_df = None\n",
    "        self.scores_df = None\n",
    "        self.dre_cols = [\n",
    "            'Receita L√≠quida', 'Custo dos Bens e/ou Servi√ßos Vendidos',\n",
    "            'Lucro Bruto', 'EBIT', 'EBT', 'Lucro L√≠quido Consolidado'\n",
    "        ]\n",
    "\n",
    "    def load_and_prepare_data(self) -> pd.DataFrame:\n",
    "        logger.info(\"üì• 1. CARREGANDO NOVO fundamentals_wide.parquet...\")\n",
    "        try:\n",
    "            self.fundamentals_df = pd.read_parquet(self.fundamentals_path)\n",
    "            logger.info(f\"‚úÖ Dados carregados: {self.fundamentals_df.shape}\")\n",
    "            logger.info(f\"Colunas encontradas: {self.fundamentals_df.columns.tolist()}\")\n",
    "            new_cols = ['Caixa e Equivalentes', 'D√≠vida Curto Prazo', 'D√≠vida Longo Prazo']\n",
    "            missing_new = [col for col in new_cols if col not in self.fundamentals_df.columns]\n",
    "            if missing_new:\n",
    "                 logger.error(f\"‚ùå ERRO: Novas colunas {missing_new} N√ÉO encontradas no input!\")\n",
    "                 raise ValueError(f\"Novas colunas faltando: {missing_new}\")\n",
    "            else:\n",
    "                 logger.info(\"‚úÖ Novas colunas (Caixa, D√≠vida CP, D√≠vida LP) encontradas!\")\n",
    "\n",
    "            self.fundamentals_df = self._clean_fundamentals_data(self.fundamentals_df)\n",
    "            return self.fundamentals_df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erro ao carregar dados: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _clean_fundamentals_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df_clean = df.copy()\n",
    "        df_clean['DT_FIM_EXERC'] = pd.to_datetime(df_clean['DT_FIM_EXERC'], errors='coerce')\n",
    "        df_clean = df_clean.dropna(subset=['DT_FIM_EXERC'])\n",
    "        df_clean = df_clean.drop_duplicates(subset=['CNPJ_CIA', 'DT_FIM_EXERC'])\n",
    "        df_clean = df_clean.sort_values(['CNPJ_CIA', 'DT_FIM_EXERC'])\n",
    "        return df_clean\n",
    "\n",
    "    def calculate_ttm_data(self) -> pd.DataFrame:\n",
    "        if self.fundamentals_df is None: raise ValueError(\"Dados n√£o carregados.\")\n",
    "        logger.info(\"‚è≥ 2. CALCULANDO TTM...\")\n",
    "        df_ttm = self.fundamentals_df.copy()\n",
    "        def rolling_sum_group(group):\n",
    "            for col in self.dre_cols:\n",
    "                if col in group.columns:\n",
    "                    quarterly = group[col] - group[col].shift(1).fillna(0)\n",
    "                    group[f'{col}_ttm'] = quarterly.rolling(window=4, min_periods=4).sum()\n",
    "                else: group[f'{col}_ttm'] = np.nan\n",
    "            return group\n",
    "        grouped = df_ttm.groupby('CNPJ_CIA', group_keys=False)\n",
    "        self.ratios_df = grouped.progress_apply(rolling_sum_group)\n",
    "        logger.info(f\"‚úÖ TTM calculado.\")\n",
    "        return self.ratios_df\n",
    "\n",
    "    def calculate_financial_ratios(self) -> pd.DataFrame:\n",
    "        \"\"\" PASSO 3: Calcula ratios (com ROIC correto e ratios de d√≠vida) \"\"\"\n",
    "        if self.ratios_df is None: raise ValueError(\"Dados TTM n√£o calculados.\")\n",
    "        logger.info(\"üßÆ 3. CALCULANDO RATIOS FINANCEIROS (COM TTM)...\")\n",
    "        df = self.ratios_df.copy()\n",
    "\n",
    "        def get_col(df, col_name): return df.get(col_name, np.nan)\n",
    "        def safe_divide(num, den): return np.where(den == 0, np.nan, num / den)\n",
    "\n",
    "        divida_cp = get_col(df, 'D√≠vida Curto Prazo').fillna(0) \n",
    "        divida_lp = get_col(df, 'D√≠vida Longo Prazo').fillna(0)\n",
    "        caixa = get_col(df, 'Caixa e Equivalentes').fillna(0)   \n",
    "        pl = get_col(df, 'Patrim√¥nio L√≠quido Consolidado').fillna(0)\n",
    "        \n",
    "        df['D√≠vida Bruta'] = divida_cp + divida_lp\n",
    "        df['Capital Investido'] = df['D√≠vida Bruta'] + pl \n",
    "        df['D√≠vida L√≠quida'] = df['D√≠vida Bruta'] - caixa \n",
    "        logger.info(\"‚úÖ D√≠vida Bruta, Capital Investido e D√≠vida L√≠quida calculados.\")\n",
    "\n",
    "        logger.info(\"üìà Calculando RENTABILIDADE...\")\n",
    "        df['ROE'] = safe_divide(get_col(df, 'Lucro L√≠quido Consolidado_ttm'), pl)\n",
    "        df['ROA'] = safe_divide(get_col(df, 'Lucro L√≠quido Consolidado_ttm'), get_col(df, 'Ativo Total'))\n",
    "        df['ROIC'] = safe_divide(get_col(df, 'EBIT_ttm'), df['Capital Investido'])\n",
    "        logger.info(f\"‚úÖ ROIC (correto) calculado. M√©dia: {df['ROIC'].mean():.4f}\")\n",
    "\n",
    "        df['MARGEM_EBIT'] = safe_divide(get_col(df, 'EBIT_ttm'), get_col(df, 'Receita L√≠quida_ttm'))\n",
    "        df['MARGEM_LIQUIDA'] = safe_divide(get_col(df, 'Lucro L√≠quido Consolidado_ttm'), get_col(df, 'Receita L√≠quida_ttm'))\n",
    "        df['MARGEM_BRUTA'] = safe_divide(get_col(df, 'Lucro Bruto_ttm'), get_col(df, 'Receita L√≠quida_ttm'))\n",
    "\n",
    "        logger.info(\"‚öñÔ∏è Calculando ALAVANCAGEM...\")\n",
    "        df['ALAVANCAGEM'] = safe_divide(get_col(df, 'Passivo Total'), get_col(df, 'Ativo Total'))\n",
    "        df['DIVIDA_PL'] = safe_divide(df['D√≠vida Bruta'], pl) \n",
    "        df['DIVIDA_LIQ_EBIT'] = safe_divide(df['D√≠vida L√≠quida'], get_col(df, 'EBIT_ttm')) \n",
    "        logger.info(\"‚úÖ Ratios de alavancagem (incluindo DIVIDA_PL, DIVIDA_LIQ_EBIT) calculados.\")\n",
    "\n",
    "        df['LIQUIDEZ_CORRENTE'] = safe_divide(get_col(df, 'Ativo Circulante'), get_col(df, 'Passivo Circulante'))\n",
    "\n",
    "        df['GIRO_ATIVO'] = safe_divide(get_col(df, 'Receita L√≠quida_ttm'), get_col(df, 'Ativo Total'))\n",
    "\n",
    "        self.ratios_df = self._handle_ratio_outliers(df)\n",
    "        logger.info(\"üéØ RATIOS CALCULADOS e limpos.\")\n",
    "        logger.info(f\"Colunas FINAIS de Ratios: {self.ratios_df.columns.tolist()}\")\n",
    "        return self.ratios_df\n",
    "\n",
    "    def _handle_ratio_outliers(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df_clean = df.copy()\n",
    "        ratio_columns = [\n",
    "            'ROE', 'ROA', 'ROIC', 'MARGEM_EBIT', 'MARGEM_LIQUIDA', 'MARGEM_BRUTA',\n",
    "            'ALAVANCAGEM', 'DIVIDA_PL', 'DIVIDA_LIQ_EBIT', 'LIQUIDEZ_CORRENTE', 'GIRO_ATIVO'\n",
    "        ]\n",
    "        for col in ratio_columns:\n",
    "            if col in df_clean.columns:\n",
    "                df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "                if df_clean[col].notna().sum() > 0:\n",
    "                    lower = df_clean[col].quantile(0.01)\n",
    "                    upper = df_clean[col].quantile(0.99)\n",
    "                    df_clean[col] = df_clean[col].clip(lower=lower, upper=upper)\n",
    "        return df_clean\n",
    "\n",
    "    def calculate_quality_scores(self) -> pd.DataFrame:\n",
    "        \"\"\" PASSO 4: Calcula scores individuais e score composto \"\"\"\n",
    "        if self.ratios_df is None: raise ValueError(\"Ratios n√£o calculados.\")\n",
    "        logger.info(\"üéØ 4. CALCULANDO SCORES DE QUALIDADE...\")\n",
    "        scores_df = self.ratios_df.copy()\n",
    "        grouped_by_date = scores_df.groupby('DT_FIM_EXERC')\n",
    "\n",
    "        metrics_config = {\n",
    "            'ROIC': {'direction': 1, 'weight': 0.25}, \n",
    "            'ROE': {'direction': 1, 'weight': 0.15},\n",
    "            'MARGEM_EBIT': {'direction': 1, 'weight': 0.15},\n",
    "            'MARGEM_LIQUIDA': {'direction': 1, 'weight': 0.10},\n",
    "            'DIVIDA_LIQ_EBIT': {'direction': -1, 'weight': 0.15}, \n",
    "            'LIQUIDEZ_CORRENTE': {'direction': 1, 'weight': 0.10},\n",
    "            'GIRO_ATIVO': {'direction': 1, 'weight': 0.10},\n",
    "            # 'ALAVANCAGEM': {'direction': -1, 'weight': 0.0}, # Pode remover ou ajustar peso\n",
    "        }\n",
    "        total_w = sum(c['weight'] for c in metrics_config.values())\n",
    "        if abs(total_w - 1.0) > 0.01:\n",
    "             logger.warning(f\"Soma dos pesos √© {total_w:.2f}. Ajustando proporcionalmente...\")\n",
    "             for k in metrics_config: metrics_config[k]['weight'] /= total_w\n",
    "\n",
    "        logger.info(f\"Configura√ß√£o de m√©tricas para score: { {k: v['weight'] for k, v in metrics_config.items()} }\")\n",
    "\n",
    "        for metric, config in metrics_config.items():\n",
    "            if metric in scores_df.columns and scores_df[metric].notna().any():\n",
    "                score_col = f'score_{metric}'\n",
    "                if config['direction'] == 1:\n",
    "                    scores_df[score_col] = grouped_by_date[metric].rank(pct=True) * 100\n",
    "                else:\n",
    "                    scores_df[score_col] = grouped_by_date[metric].rank(ascending=False, pct=True) * 100\n",
    "                logger.info(f\"  ‚úÖ Score {metric} calculado.\")\n",
    "            else:\n",
    "                 logger.warning(f\"M√©trica '{metric}' n√£o encontrada ou sem dados para score.\")\n",
    "\n",
    "        logger.info(\"‚öñÔ∏è Calculando score composto...\")\n",
    "        scores_df['aurum_quality_score'] = 0.0\n",
    "        total_applied_weight = 0.0\n",
    "        for metric, config in metrics_config.items():\n",
    "            score_col = f'score_{metric}'\n",
    "            if score_col in scores_df.columns:\n",
    "                scores_df['aurum_quality_score'] += scores_df[score_col].fillna(50) * config['weight']\n",
    "                total_applied_weight += config['weight']\n",
    "\n",
    "        if total_applied_weight > 0:\n",
    "            scores_df['aurum_quality_score'] /= total_applied_weight\n",
    "\n",
    "        logger.info(\"üèÜ Classificando empresas...\")\n",
    "        try:\n",
    "             valid_scores = scores_df['aurum_quality_score'].dropna()\n",
    "             if not valid_scores.empty:\n",
    "                  ranks = valid_scores.rank(ascending=False, pct=True)\n",
    "                  scores_df['final_rank'] = ranks\n",
    "                  try:\n",
    "                       scores_df['quality_quintile'] = pd.qcut(scores_df['final_rank'].dropna(), 5, labels=[f'{i}¬∫ Quintil' for i in range(1, 6)])\n",
    "                  except ValueError: scores_df['quality_quintile'] = pd.cut(scores_df['final_rank'].dropna(), 5, labels=False)\n",
    "                  scores_df['quality_grade'] = pd.cut(scores_df['final_rank'].dropna(), bins=[0, 0.2, 0.4, 0.6, 0.8, 1.0], labels=['A', 'B', 'C', 'D', 'E'], right=True, include_lowest=True)\n",
    "             else: scores_df[['final_rank', 'quality_quintile', 'quality_grade']] = np.nan\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Erro ao calcular ranks/grades: {e}\")\n",
    "             scores_df[['final_rank', 'quality_quintile', 'quality_grade']] = np.nan\n",
    "        \n",
    "        self.scores_df = scores_df\n",
    "        logger.info(f\"Colunas FINAIS ANTES de salvar: {self.scores_df.columns.tolist()}\")\n",
    "        return scores_df\n",
    "\n",
    "    def get_latest_scores(self) -> pd.DataFrame:\n",
    "        if self.scores_df is None: raise ValueError(\"Scores n√£o calculados.\")\n",
    "        latest_scores = self.scores_df.sort_values('DT_FIM_EXERC').groupby('CNPJ_CIA').last().reset_index()\n",
    "        return latest_scores\n",
    "\n",
    "    def save_results(self, output_dir: str = \"../data/aurum_scores\"):\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if self.scores_df is not None:\n",
    "            logger.info(f\"Salvando DataFrame com colunas: {self.scores_df.columns.tolist()}\")\n",
    "            \n",
    "            if 'ROIC' not in self.scores_df.columns: \n",
    "                logger.error(\"ERRO FATAL: Coluna 'ROIC' AUSENTE antes de salvar!\")\n",
    "\n",
    "            scores_path = output_path / \"aurum_quality_scores_complete.parquet\"\n",
    "            self.scores_df.to_parquet(scores_path, index=False)\n",
    "\n",
    "            scores_csv_path = output_path / \"aurum_quality_scores_complete.csv\"\n",
    "            self.scores_df.to_csv(scores_csv_path, index=False, sep=';', float_format='%.4f', encoding='utf-8-sig')\n",
    "\n",
    "            logger.info(f\"üíæ Scores completos (hist√≥rico) salvos em:\")\n",
    "            logger.info(f\"   -> {scores_path}\")\n",
    "            logger.info(f\"   -> {scores_csv_path}\")\n",
    "\n",
    "        latest_scores = self.get_latest_scores()\n",
    "        \n",
    "        latest_path = output_path / \"aurum_quality_scores_latest.parquet\"\n",
    "        latest_csv_path = output_path / \"aurum_quality_scores_latest.csv\"\n",
    "        \n",
    "        latest_scores.to_parquet(latest_path, index=False)\n",
    "        latest_scores.to_csv(latest_csv_path, index=False, sep=';', encoding='utf-8-sig')\n",
    "        \n",
    "        logger.info(f\"üíæ Scores mais recentes salvos: {latest_path} e {latest_csv_path}\")\n",
    "\n",
    "        # Salvar Estat√≠sticas\n",
    "        stats_path = output_path / \"aurum_scores_statistics.txt\"\n",
    "        self._save_statistics(latest_scores, stats_path)\n",
    "        logger.info(f\"üíæ Estat√≠sticas salvas: {stats_path}\")\n",
    "\n",
    "        return {\n",
    "            'complete_parquet': scores_path,\n",
    "            'complete_csv': scores_csv_path,\n",
    "            'latest_parquet': latest_path,\n",
    "            'statistics': stats_path\n",
    "        }\n",
    "\n",
    "    def _save_statistics(self, scores_df: pd.DataFrame, stats_path: Path):\n",
    "        with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"AURUM QUALITY SCORE - ESTAT√çSTICAS (√öLTIMO PER√çODO)\\n\" + \"=\" * 50 + \"\\n\\n\")\n",
    "            f.write(f\"Total de empresas: {len(scores_df)}\\n\")\n",
    "            max_date = scores_df['DT_FIM_EXERC'].max()\n",
    "            f.write(f\"Per√≠odo mais recente: {max_date if pd.notna(max_date) else 'N/A'}\\n\\n\")\n",
    "            if 'aurum_quality_score' in scores_df.columns:\n",
    "                 f.write(\"DISTRIBUI√á√ÉO DOS SCORES:\\n\" + f\"  M√©dia: {scores_df['aurum_quality_score'].mean():.2f}\\n\" +\n",
    "                         f\"  Mediana: {scores_df['aurum_quality_score'].median():.2f}\\n\" + f\"  M√≠nimo: {scores_df['aurum_quality_score'].min():.2f}\\n\" +\n",
    "                         f\"  M√°ximo: {scores_df['aurum_quality_score'].max():.2f}\\n\\n\")\n",
    "            else: f.write(\"DISTRIBUI√á√ÉO DOS SCORES: N/A\\n\\n\")\n",
    "            if 'quality_grade' in scores_df.columns:\n",
    "                 f.write(\"DISTRIBUI√á√ÉO POR NOTAS:\\n\")\n",
    "                 grade_counts = scores_df['quality_grade'].value_counts().sort_index(ascending=True)\n",
    "                 for grade, count in grade_counts.items(): f.write(f\"  Nota {grade}: {count} empresas\\n\")\n",
    "            else: f.write(\"DISTRIBUI√á√ÉO POR NOTAS: N/A\\n\")\n",
    "            if 'aurum_quality_score' in scores_df.columns:\n",
    "                 f.write(\"\\nTOP 10 EMPRESAS:\\n\")\n",
    "                 top_10 = scores_df.nlargest(10, 'aurum_quality_score')[['DENOM_CIA', 'aurum_quality_score', 'quality_grade']]\n",
    "                 for i, (_, row) in enumerate(top_10.iterrows(), 1):\n",
    "                      grade = row.get('quality_grade', 'N/A')\n",
    "                      f.write(f\"  {i:2d}. {str(row['DENOM_CIA'])[:35]:35} {row['aurum_quality_score']:6.2f} (Nota {grade})\\n\")\n",
    "            else: f.write(\"\\nTOP 10 EMPRESAS: N/A\\n\")\n",
    "\n",
    "\n",
    "# ==================== EXECU√á√ÉO PRINCIPAL ====================\n",
    "def main():\n",
    "    logger.info(\"üöÄ INICIANDO PIPELINE DO AURUM QUALITY SCORE (Vers√£o Final com ROIC Correto)\")\n",
    "    try:\n",
    "        calculator = AurumQualityScoreCalculator(\n",
    "            fundamentals_path=\"../data/cvm/final/fundamentals_wide.parquet\"\n",
    "        )\n",
    "        calculator.load_and_prepare_data()\n",
    "        calculator.calculate_ttm_data()\n",
    "        calculator.calculate_financial_ratios() # Calcular ROIC correto e ratios de d√≠vida\n",
    "        calculator.calculate_quality_scores()   # Usar ROIC correto e ratios de d√≠vida\n",
    "        output_files = calculator.save_results(output_dir=\"../data/aurum_scores\") # Salvar no diret√≥rio correto\n",
    "        logger.info(f\"üíæ Resultados (com ROIC correto) salvos em: {output_files}\")\n",
    "\n",
    "        latest_scores = calculator.get_latest_scores()\n",
    "        # ... (impress√£o dos resultados igual) ...\n",
    "        print(\"\\n\" + \"=\"*60 + \"\\nüéâ AURUM QUALITY SCORE - RESULTADOS FINAIS (√öLTIMO PER√çODO)\\n\" + \"=\"*60)\n",
    "        print(f\"\\nüìä TOTAL DE EMPRESAS: {len(latest_scores)}\")\n",
    "        if 'aurum_quality_score' in latest_scores.columns: print(f\"üìà SCORE M√âDIO: {latest_scores['aurum_quality_score'].mean():.2f}\")\n",
    "        print(f\"\\nüìã DISTRIBUI√á√ÉO DAS NOTAS:\")\n",
    "        if 'quality_grade' in latest_scores.columns:\n",
    "             grade_dist = latest_scores['quality_grade'].value_counts().sort_index(ascending=True)\n",
    "             for grade, count in grade_dist.items(): print(f\"   Nota {grade}: {count:3d} empresas\")\n",
    "        print(f\"\\nü•á TOP 10 EMPRESAS:\")\n",
    "        if 'aurum_quality_score' in latest_scores.columns:\n",
    "             top_10 = latest_scores.nlargest(10, 'aurum_quality_score')[['DENOM_CIA', 'aurum_quality_score', 'quality_grade']]\n",
    "             for i, (_, row) in enumerate(top_10.iterrows(), 1):\n",
    "                  grade = row.get('quality_grade', 'N/A')\n",
    "                  print(f\"   {i:2d}. {str(row['DENOM_CIA'])[:35]:35} {row['aurum_quality_score']:6.2f} (Nota {grade})\")\n",
    "\n",
    "        return calculator\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå ERRO NO PIPELINE: {e}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    aurum_calculator = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a29a0b",
   "metadata": {},
   "source": [
    "\n",
    "### üß¨ Script: Aurum Data Unifier (Merge Engine)\n",
    "\n",
    "> **Fase:** Engenharia de Dados / ETL\n",
    "> **Arquivo Alvo:** `aurum_master_features.parquet`\n",
    "\n",
    "Este script √© o **Motor de Integra√ß√£o** do sistema Aurum. Ele √© respons√°vel por fundir tr√™s fontes de dados heterog√™neas (Pre√ßos, Fundamentos e Metadados) em um √∫nico DataFrame temporalmente coerente (\"Master Features\").\n",
    "\n",
    "Sua principal fun√ß√£o √© resolver o problema do **\"Look-Ahead Bias\"** (vi√©s de olhar para o futuro), aplicando um `LAG` (atraso) de 3 meses nas datas de balan√ßo para simular a disponibilidade real da informa√ß√£o para o investidor.\n",
    "\n",
    "### üéØ Objetivos\n",
    "\n",
    "1. **Sincroniza√ß√£o Temporal:** Alinhar pre√ßos di√°rios/mensais com fundamentos trimestrais.\n",
    "2. **Mapeamento de Identificadores:** Conectar `CNPJ` (usado nos balan√ßos da CVM) com `Tickers` (usados na bolsa).\n",
    "3. **C√°lculo de Volatilidade:** Gerar a m√©trica de risco (`StdDev` de 63 dias) para uso no scoring.\n",
    "4. **Consolida√ß√£o:** Gerar o arquivo final que alimentar√° o Backtest e o Scoring Engine.\n",
    "\n",
    "### üèóÔ∏è Arquitetura de Dados (Data Flow)\n",
    "\n",
    "O processo segue um fluxo linear de carregamento, transforma√ß√£o e fus√£o (`Merge As-Of`).\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    %% --- FONTES ---\n",
    "    subgraph Sources [\"1. Fontes de Dados\"]\n",
    "        Prices[(\"<b>Pre√ßos Limpos</b><br/>all_histories_cleaned.parquet\")]\n",
    "        Fund[(\"<b>Scores Fundamentais</b><br/>aurum_quality_scores_complete.parquet\")]\n",
    "        Map[(\"<b>Mapa Ticker-CNPJ</b><br/>mapa_ticker_cnpj.parquet\")]\n",
    "    end\n",
    "\n",
    "    %% --- VOLATILIDADE ---\n",
    "    subgraph Volatility [\"2. C√°lculo de Risco\"]\n",
    "        CalcVol[\"<b>C√°lculo de Volatilidade</b><br/>Janela M√≥vel 63 dias\"]\n",
    "        Resample[\"<b>Agrega√ß√£o Mensal</b><br/>Resample('M').last()\"]\n",
    "    end\n",
    "\n",
    "    %% --- PREPARA√á√ÉO ---\n",
    "    subgraph Prep [\"3. Prepara√ß√£o & Lag\"]\n",
    "        Melt[\"<b>Melt Pre√ßos</b><br/>Wide -> Long Format\"]\n",
    "        Lag[\"<b>Aplica√ß√£o de Lag (3 Meses)</b><br/>Date_Balan√ßo + 3M = Date_Dispon√≠vel\"]\n",
    "        MapJoin[\"<b>Join CNPJ-Ticker</b><br/>V√≠nculo da empresa com c√≥digo de bolsa\"]\n",
    "    end\n",
    "\n",
    "    %% --- FUS√ÉO ---\n",
    "    subgraph Merge [\"4. Fus√£o Temporal (As-Of)\"]\n",
    "        AsOf{{\"<b>Merge As-Of</b>\"}}\n",
    "        Note[\"Alinha o pre√ßo de HOJE<br/>com o balan√ßo MAIS RECENTE dispon√≠vel\"]\n",
    "        VolJoin[\"<b>Join Volatilidade</b><br/>Left Join por Ticker/Data\"]\n",
    "    end\n",
    "\n",
    "    %% --- OUTPUT ---\n",
    "    subgraph Output [\"5. Sa√≠da Final\"]\n",
    "        Clean[\"<b>Limpeza Final</b><br/>Remove linhas sem Score/Pre√ßo\"]\n",
    "        Master[(\"<b>AURUM MASTER FEATURES</b><br/>Dataset Unificado\")]\n",
    "    end\n",
    "\n",
    "    %% --- LIGA√á√ïES ---\n",
    "    Prices --> CalcVol --> Resample\n",
    "    Prices --> Melt --> AsOf\n",
    "    Fund --> Lag --> MapJoin\n",
    "    Map --> MapJoin --> AsOf\n",
    "    Resample --> VolJoin\n",
    "    \n",
    "    AsOf --> VolJoin --> Clean --> Master\n",
    "\n",
    "```\n",
    "\n",
    "### ‚öôÔ∏è Detalhes da Implementa√ß√£o\n",
    "\n",
    "#### 1. C√°lculo de Volatilidade (`calcular_volatilidade_mensal`)\n",
    "\n",
    "* **Input:** Pre√ßos di√°rios limpos.\n",
    "* **L√≥gica:**\n",
    "* Calcula retorno di√°rio (`pct_change`).\n",
    "* Calcula desvio padr√£o m√≥vel de 63 dias (aprox. 1 trimestre √∫til).\n",
    "* Agrega o √∫ltimo valor de cada m√™s (`resample('M').last()`).\n",
    "\n",
    "\n",
    "* **Por que?** A volatilidade √© um componente chave do Score Aurum (peso 10%), penalizando ativos inst√°veis.\n",
    "\n",
    "#### 2. Tratamento de Fundamentos & Lag\n",
    "\n",
    "* **Input:** `aurum_quality_scores_complete.parquet` (Dados da CVM).\n",
    "* **O Problema:** Um balan√ßo do 4¬∫ Trimestre (31/Dez) s√≥ √© divulgado em Mar√ßo/Abril. Se usarmos a data 31/Dez no backtest, estamos \"roubando\", pois a informa√ß√£o n√£o existia publicamente naquele dia.\n",
    "* **A Solu√ß√£o (Lag):**\n",
    "```python\n",
    "df_fund['date_disponivel'] = df_fund['date_balanco'] + DateOffset(months=3)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "O sistema s√≥ permite que o rob√¥ \"veja\" o balan√ßo 3 meses ap√≥s o fechamento do trimestre.\n",
    "\n",
    "#### 3. Mapeamento CNPJ -> Ticker\n",
    "\n",
    "* A CVM usa `CNPJ` como chave prim√°ria. O Yahoo Finance usa `Ticker`.\n",
    "* O script carrega um dicion√°rio (`mapa_ticker_cnpj_automatizado.parquet`) para traduzir os dados.\n",
    "* **Limpeza:** Remove caracteres especiais de CNPJ (`.`, `/`, `-`) para garantir o *match*.\n",
    "\n",
    "#### 4. Merge As-Of (O Cora√ß√£o do Script)\n",
    "\n",
    "* **Comando:** `pd.merge_asof(..., direction='backward')`\n",
    "* **Como funciona:** Para cada data de pre√ßo (ex: 15/Jan/2024), ele procura a data de balan√ßo dispon√≠vel **mais pr√≥xima no passado** (ex: 30/Set/2023 + 3 meses lag = 30/Dez/2023).\n",
    "* Isso garante que, em qualquer dia da simula√ß√£o, temos o dado fundamentalista mais recente e v√°lido.\n",
    "\n",
    "### üìÇ Outputs (Sa√≠das)\n",
    "\n",
    "O script gera dois arquivos id√™nticos em formatos diferentes:\n",
    "\n",
    "1. **`../data/aurum_master_features.parquet`**:\n",
    "* Arquivo bin√°rio otimizado para leitura r√°pida no Pandas/Backtrader.\n",
    "* Cont√©m: `date`, `ticker`, `Adj Close`, `aurum_quality_score`, `ROE`, `VOLATILIDADE`, etc.\n",
    "\n",
    "\n",
    "2. **`../data/aurum_master_features.csv`**:\n",
    "* Vers√£o em texto para auditoria visual e debug r√°pido no Excel.\n",
    "\n",
    "\n",
    "\n",
    "### üöÄ Como Executar\n",
    "\n",
    "Este script depende da execu√ß√£o pr√©via dos scripts de **Coleta de Pre√ßos** e **C√°lculo de Scores Fundamentais**.\n",
    "\n",
    "```bash\n",
    "python src/step_04_unificar_master.py\n",
    "\n",
    "```\n",
    "\n",
    "### üìã Verifica√ß√£o de Sucesso\n",
    "\n",
    "Se o script rodar corretamente, voc√™ ver√° no log:\n",
    "\n",
    "```text\n",
    "‚úÖ‚úÖ‚úÖ ARQUIVO MESTRE GERADO: ../data/aurum_master_features.parquet ‚úÖ‚úÖ‚úÖ\n",
    "\n",
    "```\n",
    "\n",
    "E uma amostra final dos dados combinados:\n",
    "\n",
    "```text\n",
    "             date    ticker date_disponivel  Adj Close  aurum_quality_score      ROE\n",
    "...  2024-12-31  WEGE3.SA      2024-09-30      35.40                65.40  0.2510\n",
    "\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb24355d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 17:44:09,894 - INFO - Iniciando Unifica√ß√£o (Passo 1/2): C√°lculo da Volatilidade...\n",
      "C:\\Users\\kaike\\AppData\\Local\\Temp\\ipykernel_22668\\4189223198.py:40: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  df_vol_mensal = df_prices_daily.set_index('date').groupby('ticker').resample('M').last()['VOLATILIDADE'].reset_index()\n",
      "C:\\Users\\kaike\\AppData\\Local\\Temp\\ipykernel_22668\\4189223198.py:40: FutureWarning: DataFrameGroupBy.resample operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_vol_mensal = df_prices_daily.set_index('date').groupby('ticker').resample('M').last()['VOLATILIDADE'].reset_index()\n",
      "2025-12-17 17:44:11,734 - INFO - ‚úÖ Volatilidade mensal calculada.\n",
      "2025-12-17 17:44:11,738 - INFO - Iniciando Unifica√ß√£o (Passo 2/2): Jun√ß√£o do DataFrame Mestre (SEM SENTIMENTO)...\n",
      "2025-12-17 17:44:11,785 - INFO - Base de pre√ßos (wide) carregada.\n",
      "2025-12-17 17:44:11,903 - INFO - ‚è≥ Aplicando LAG de 3 meses nas datas de balan√ßo...\n",
      "2025-12-17 17:44:11,909 - INFO - üìÇ Carregando mapeamento de: ../data/dados_mapeamento/mapa_ticker_cnpj_automatizado.parquet\n",
      "2025-12-17 17:44:11,937 - INFO - ‚úÖ Mapeamento carregado e limpo: 97 tickers.\n",
      "2025-12-17 17:44:11,986 - INFO - Fundamentos mapeados: 4158 registros.\n",
      "2025-12-17 17:44:12,019 - INFO - Merge 'as-of' com LAG conclu√≠do.\n",
      "2025-12-17 17:44:12,094 - INFO - Limpeza final: 16041 linhas removidas (sem score fundamentalista).\n",
      "2025-12-17 17:44:12,217 - INFO - ‚úÖ‚úÖ‚úÖ ARQUIVO MESTRE GERADO: ..\\data\\aurum_master_features.parquet ‚úÖ‚úÖ‚úÖ\n",
      "2025-12-17 17:44:14,164 - INFO - ‚úÖ‚úÖ‚úÖ ARQUIVO MESTRE GERADO (CSV): ..\\data\\aurum_master_features.csv ‚úÖ‚úÖ‚úÖ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Amostra Final ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "ticker",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "date_disponivel",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "Adj Close",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "aurum_quality_score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "ROE",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "d01a6329-466c-4917-ac7a-943309b11bd0",
       "rows": [
        [
         "34914",
         "2025-12-31 00:00:00",
         "SANB11.SA",
         "2025-12-30 00:00:00",
         "32.029998779296875",
         "32.49376075731498",
         "0.0"
        ],
        [
         "34915",
         "2025-12-31 00:00:00",
         "BRKM5.SA",
         "2025-12-30 00:00:00",
         "7.860000133514404",
         "40.404761904761905",
         "-0.021430822565395524"
        ],
        [
         "34917",
         "2025-12-31 00:00:00",
         "BRAV3.SA",
         "2025-12-30 00:00:00",
         "13.430000305175781",
         "50.768215720022944",
         "0.15160840500414885"
        ],
        [
         "34918",
         "2025-12-31 00:00:00",
         "CYRE3.SA",
         "2025-12-30 00:00:00",
         "32.7400016784668",
         "33.32716580608147",
         "0.004176503507550167"
        ],
        [
         "34919",
         "2025-12-31 00:00:00",
         "YDUQ3.SA",
         "2012-06-30 00:00:00",
         "12.359999656677246",
         "58.418262188356024",
         "0.07609890109890109"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ticker</th>\n",
       "      <th>date_disponivel</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>aurum_quality_score</th>\n",
       "      <th>ROE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34914</th>\n",
       "      <td>2025-12-31</td>\n",
       "      <td>SANB11.SA</td>\n",
       "      <td>2025-12-30</td>\n",
       "      <td>32.029999</td>\n",
       "      <td>32.493761</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34915</th>\n",
       "      <td>2025-12-31</td>\n",
       "      <td>BRKM5.SA</td>\n",
       "      <td>2025-12-30</td>\n",
       "      <td>7.860000</td>\n",
       "      <td>40.404762</td>\n",
       "      <td>-0.021431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34917</th>\n",
       "      <td>2025-12-31</td>\n",
       "      <td>BRAV3.SA</td>\n",
       "      <td>2025-12-30</td>\n",
       "      <td>13.430000</td>\n",
       "      <td>50.768216</td>\n",
       "      <td>0.151608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34918</th>\n",
       "      <td>2025-12-31</td>\n",
       "      <td>CYRE3.SA</td>\n",
       "      <td>2025-12-30</td>\n",
       "      <td>32.740002</td>\n",
       "      <td>33.327166</td>\n",
       "      <td>0.004177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34919</th>\n",
       "      <td>2025-12-31</td>\n",
       "      <td>YDUQ3.SA</td>\n",
       "      <td>2012-06-30</td>\n",
       "      <td>12.360000</td>\n",
       "      <td>58.418262</td>\n",
       "      <td>0.076099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date     ticker date_disponivel  Adj Close  aurum_quality_score  \\\n",
       "34914 2025-12-31  SANB11.SA      2025-12-30  32.029999            32.493761   \n",
       "34915 2025-12-31   BRKM5.SA      2025-12-30   7.860000            40.404762   \n",
       "34917 2025-12-31   BRAV3.SA      2025-12-30  13.430000            50.768216   \n",
       "34918 2025-12-31   CYRE3.SA      2025-12-30  32.740002            33.327166   \n",
       "34919 2025-12-31   YDUQ3.SA      2012-06-30  12.360000            58.418262   \n",
       "\n",
       "            ROE  \n",
       "34914  0.000000  \n",
       "34915 -0.021431  \n",
       "34917  0.151608  \n",
       "34918  0.004177  \n",
       "34919  0.076099  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "PATH_PRECOS_LIMPOS = \"../data/historical/all_histories_cleaned.parquet\"\n",
    "PATH_PRECOS_WIDE = \"../data/historical/prices_close_wide.parquet\"\n",
    "PATH_FUNDAMENTOS = \"../data/aurum_scores/aurum_quality_scores_complete.parquet\"\n",
    "PATH_DE_PARA = \"../data/dados_mapeamento/mapa_ticker_cnpj_automatizado.parquet\"\n",
    "# SENTIMENTO: Desativado temporariamente\n",
    "# PATH_SENTIMENTO = \"../data/news/news_with_sentiment.parquet\" \n",
    "\n",
    "OUTPUT_DIR = \"../data\"\n",
    "OUTPUT_FILENAME = \"aurum_master_features.parquet\"\n",
    "output_path = Path(OUTPUT_DIR) / OUTPUT_FILENAME\n",
    "\n",
    "def calcular_volatilidade_mensal():\n",
    "    logger.info(\"Iniciando Unifica√ß√£o (Passo 1/2): C√°lculo da Volatilidade...\")\n",
    "    try: \n",
    "        df_prices_daily = pd.read_parquet(PATH_PRECOS_LIMPOS)\n",
    "    except FileNotFoundError: \n",
    "        logger.error(f\"Arquivo n√£o encontrado: {PATH_PRECOS_LIMPOS}\")\n",
    "        return None\n",
    "        \n",
    "    df_prices_daily['date'] = pd.to_datetime(df_prices_daily['date'])\n",
    "    df_prices_daily['returns'] = df_prices_daily.groupby('ticker')['Adj Close'].pct_change()\n",
    "    \n",
    "    df_prices_daily['VOLATILIDADE'] = df_prices_daily.groupby('ticker')['returns'].rolling(window=63, min_periods=30).std().reset_index(0, drop=True)\n",
    "    \n",
    "    df_vol_mensal = df_prices_daily.set_index('date').groupby('ticker').resample('M').last()['VOLATILIDADE'].reset_index()\n",
    "    \n",
    "    logger.info(f\"‚úÖ Volatilidade mensal calculada.\")\n",
    "    return df_vol_mensal\n",
    "\n",
    "def carregar_e_limpar_mapeamento():\n",
    "    \"\"\" Carrega e prepara o novo arquivo de mapeamento \"\"\"\n",
    "    logger.info(f\"üìÇ Carregando mapeamento de: {PATH_DE_PARA}\")\n",
    "    try:\n",
    "        df_map = pd.read_parquet(PATH_DE_PARA)\n",
    "        \n",
    "        cols_necessarias = ['ticker', 'CNPJ']\n",
    "        if not all(col in df_map.columns for col in cols_necessarias):\n",
    "            logger.error(f\"Colunas {cols_necessarias} n√£o encontradas no mapeamento. Colunas atuais: {df_map.columns}\")\n",
    "            return None\n",
    "            \n",
    "        df_map = df_map[cols_necessarias].copy()\n",
    "        \n",
    "        df_map = df_map.rename(columns={'CNPJ': 'CNPJ_CIA'})\n",
    "        \n",
    "        df_map['CNPJ_CIA'] = df_map['CNPJ_CIA'].astype(str).str.replace(r'[.\\-/]', '', regex=True)\n",
    "        \n",
    "        df_map['ticker'] = df_map['ticker'].apply(lambda x: f\"{x}.SA\" if not str(x).endswith(\".SA\") else x)\n",
    "        \n",
    "        df_map = df_map.drop_duplicates(subset=['ticker'])\n",
    "        \n",
    "        logger.info(f\"‚úÖ Mapeamento carregado e limpo: {len(df_map)} tickers.\")\n",
    "        return df_map\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Arquivo de mapeamento n√£o encontrado: {PATH_DE_PARA}\")\n",
    "        return None\n",
    "\n",
    "def unificar_dataframe_mestre(df_vol_mensal):\n",
    "    logger.info(\"Iniciando Unifica√ß√£o (Passo 2/2): Jun√ß√£o do DataFrame Mestre (SEM SENTIMENTO)...\")\n",
    "\n",
    "    try:\n",
    "        df_close_wide = pd.read_parquet(PATH_PRECOS_WIDE)\n",
    "        df_base_mensal = df_close_wide.melt(ignore_index=False, var_name='ticker', value_name='Adj Close').reset_index()\n",
    "        df_base_mensal = df_base_mensal.rename(columns={'index': 'date'})\n",
    "        logger.info(f\"Base de pre√ßos (wide) carregada.\")\n",
    "    except FileNotFoundError: \n",
    "        logger.error(f\"Arquivo n√£o encontrado: {PATH_PRECOS_WIDE}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df_fund = pd.read_parquet(PATH_FUNDAMENTOS)\n",
    "        df_fund = df_fund.rename(columns={'DT_FIM_EXERC': 'date_balanco'}) \n",
    "        df_fund['date_balanco'] = pd.to_datetime(df_fund['date_balanco'])\n",
    "        \n",
    "        logger.info(\"‚è≥ Aplicando LAG de 3 meses nas datas de balan√ßo...\")\n",
    "        df_fund['date_disponivel'] = df_fund['date_balanco'] + DateOffset(months=3)\n",
    "        \n",
    "    except FileNotFoundError: \n",
    "        logger.error(f\"Arquivo n√£o encontrado: {PATH_FUNDAMENTOS}\")\n",
    "        return\n",
    "\n",
    "    df_mapping = carregar_e_limpar_mapeamento()\n",
    "    if df_mapping is None: return\n",
    "\n",
    "    df_fund['CNPJ_CIA'] = df_fund['CNPJ_CIA'].astype(str).str.replace(r'[.\\-/]', '', regex=True)\n",
    "    \n",
    "    df_fund_com_ticker = pd.merge(df_fund, df_mapping, on='CNPJ_CIA', how='inner')\n",
    "    \n",
    "    if df_fund_com_ticker.empty:\n",
    "        logger.error(\"‚ùå O Merge entre Fundamentos e Tickers retornou vazio! Verifique os CNPJs.\")\n",
    "        return\n",
    "        \n",
    "    logger.info(f\"Fundamentos mapeados: {len(df_fund_com_ticker)} registros.\")\n",
    "\n",
    "    df_master = df_base_mensal.sort_values(by='date')\n",
    "    df_fund_com_ticker = df_fund_com_ticker.sort_values(by='date_disponivel') \n",
    "\n",
    "    df_master = pd.merge_asof(\n",
    "        df_master, \n",
    "        df_fund_com_ticker, \n",
    "        left_on='date', \n",
    "        right_on='date_disponivel', \n",
    "        by='ticker', \n",
    "        direction='backward' \n",
    "    )\n",
    "    \n",
    "    logger.info(\"Merge 'as-of' com LAG conclu√≠do.\")\n",
    "\n",
    "    if df_vol_mensal is not None:\n",
    "        df_master = pd.merge(df_master, df_vol_mensal, on=['date', 'ticker'], how='left')\n",
    "        df_master['VOLATILIDADE'] = df_master.groupby('ticker')['VOLATILIDADE'].ffill()\n",
    "\n",
    "    key_metrics = ['aurum_quality_score', 'ROE', 'ROIC'] \n",
    "    \n",
    "    df_master_clean = df_master.dropna(subset=key_metrics)\n",
    "    \n",
    "    removed = len(df_master) - len(df_master_clean)\n",
    "    logger.info(f\"Limpeza final: {removed} linhas removidas (sem score fundamentalista).\")\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    df_master_clean.to_parquet(output_path, index=False)\n",
    "    logger.info(f\"‚úÖ‚úÖ‚úÖ ARQUIVO MESTRE GERADO: {output_path} ‚úÖ‚úÖ‚úÖ\")\n",
    "\n",
    "    output_csv_path = Path(OUTPUT_DIR) / \"aurum_master_features.csv\"\n",
    "    df_master_clean.to_csv(output_csv_path, index=False, sep=';', float_format='%.4f', encoding='utf-8-sig')\n",
    "    logger.info(f\"‚úÖ‚úÖ‚úÖ ARQUIVO MESTRE GERADO (CSV): {output_csv_path} ‚úÖ‚úÖ‚úÖ\")\n",
    "\n",
    "    cols_view = ['date', 'ticker', 'date_disponivel', 'Adj Close', 'aurum_quality_score', 'ROE']\n",
    "    print(\"\\n--- Amostra Final ---\")\n",
    "    try:\n",
    "        display(df_master_clean[cols_view].tail(5))\n",
    "    except NameError:\n",
    "        print(df_master_clean[cols_view].tail(5).to_string())\n",
    "\n",
    "    return df_master_clean\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_vol = calcular_volatilidade_mensal()\n",
    "    \n",
    "    if df_vol is not None:\n",
    "        unificar_dataframe_mestre(df_vol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb59e5f",
   "metadata": {},
   "source": [
    "\n",
    "### üß† Script: Sentiment Fusion Engine (Merge V6)\n",
    "\n",
    "> **Fase:** Engenharia de Dados / Enriquecimento\n",
    "> **Arquivo Alvo:** `aurum_master_features_final.parquet`\n",
    "\n",
    "Este script √© respons√°vel por **enriquecer** o dataset financeiro mestre com dados de Intelig√™ncia Artificial (Sentimento). Ele unifica m√∫ltiplas fontes de not√≠cias (Google News e MarketAux), padroniza os scores de sentimento gerados pelo RoBERTa e os agrega em uma vis√£o mensal para alinhar com a frequ√™ncia dos balan√ßos e pre√ßos.\n",
    "\n",
    "### üéØ Objetivos\n",
    "\n",
    "1. **Unifica√ß√£o de Fontes:** Combinar feeds de not√≠cias d√≠spares (Google = Recente, MarketAux = Hist√≥rico) em um √∫nico fluxo de informa√ß√£o.\n",
    "2. **Padroniza√ß√£o:** Garantir que datas e scores sigam o mesmo formato, independentemente da origem.\n",
    "3. **Agrega√ß√£o Temporal:** Transformar not√≠cias di√°rias esparsas em um indicador mensal robusto (`SENTIMENT_SCORE` m√©dio e `NEWS_VOLUME`).\n",
    "4. **Integra√ß√£o Final:** Cruzar esses indicadores com o arquivo mestre financeiro (Pre√ßo + Fundamentos).\n",
    "\n",
    "### üèóÔ∏è Arquitetura de Dados (Data Flow)\n",
    "\n",
    "O processo segue um fluxo de \"Funil\": coleta de v√°rias pontas, agrega√ß√£o no meio e fus√£o final.\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    %% --- FONTES ---\n",
    "    subgraph Sources [\"1. Fontes de Dados\"]\n",
    "        Fin[(\"<b>Financeiro Mestre</b><br/>aurum_master_features.parquet\")]\n",
    "        GNews[(\"<b>Google News</b><br/>news_with_sentiment.parquet\")]\n",
    "        MktAux[(\"<b>MarketAux (Hist√≥rico)</b><br/>Pasta de Parquets\")]\n",
    "    end\n",
    "\n",
    "    %% --- PREPARA√á√ÉO DE NOT√çCIAS ---\n",
    "    subgraph NewsPrep [\"2. Padroniza√ß√£o de Not√≠cias\"]\n",
    "        NormG[\"<b>Normalizar Google</b><br/>Rename Cols -> Ticker, Date, Score\"]\n",
    "        NormM[\"<b>Normalizar MarketAux</b><br/>Loop em Arquivos -> Concat\"]\n",
    "        Union[\"<b>Uni√£o (Concat)</b><br/>Google + MarketAux\"]\n",
    "    end\n",
    "\n",
    "    %% --- AGREGA√á√ÉO ---\n",
    "    subgraph Agg [\"3. Agrega√ß√£o Mensal\"]\n",
    "        GroupBy[\"<b>Agrupamento</b><br/>Por Ticker e M√™s\"]\n",
    "        Metrics[\"<b>C√°lculo de KPI</b><br/>M√©dia (Sentimento)<br/>Contagem (Volume)\"]\n",
    "    end\n",
    "\n",
    "    %% --- FUS√ÉO ---\n",
    "    subgraph Merge [\"4. Fus√£o Final\"]\n",
    "        LeftJoin{{\"<b>Left Join</b>\"}}\n",
    "        Note[\"Preserva todas as linhas financeiras.<br/>Se n√£o houver not√≠cia, preenche com 0.\"]\n",
    "        FillNA[\"<b>Tratamento de Nulos</b><br/>Sentimento = 0 (Neutro)\"]\n",
    "    end\n",
    "\n",
    "    %% --- OUTPUT ---\n",
    "    subgraph Output [\"5. Sa√≠da\"]\n",
    "        Final[(\"<b>DATASET FINAL V6</b><br/>aurum_master_features_final.parquet\")]\n",
    "    end\n",
    "\n",
    "    %% --- LIGA√á√ïES ---\n",
    "    GNews --> NormG\n",
    "    MktAux --> NormM\n",
    "    NormG & NormM --> Union\n",
    "    Union --> GroupBy --> Metrics\n",
    "    \n",
    "    Fin --> LeftJoin\n",
    "    Metrics --> LeftJoin\n",
    "    LeftJoin --> FillNA --> Final\n",
    "\n",
    "```\n",
    "\n",
    "### ‚öôÔ∏è Detalhes da Implementa√ß√£o\n",
    "\n",
    "#### 1. Carregamento Flex√≠vel\n",
    "\n",
    "* **Google News:** Arquivo √∫nico, geralmente contendo dados recentes.\n",
    "* **MarketAux:** Diret√≥rio com m√∫ltiplos arquivos `.parquet` particionados. O script itera sobre todos eles (`glob`), garantindo que nenhum dado hist√≥rico seja perdido.\n",
    "\n",
    "#### 2. Agrega√ß√£o Mensal (O \"Fator Sentimento\")\n",
    "\n",
    "Como o pre√ßo e os fundamentos j√° est√£o alinhados mensalmente, n√£o podemos usar not√≠cias di√°rias diretamente (daria erro de granularidade).\n",
    "O script cria uma chave `periodo` (Ano-M√™s) e calcula:\n",
    "\n",
    "* **`SENTIMENT_SCORE`:** A m√©dia de todos os scores de not√≠cias daquele m√™s.\n",
    "* **`NEWS_VOLUME`:** Quantas not√≠cias sa√≠ram sobre a empresa.\n",
    "* **`SENTIMENT_VOL`:** (Opcional) O desvio padr√£o, para medir se as opini√µes est√£o divididas.\n",
    "\n",
    "#### 3. Tratamento de Aus√™ncia (Neutralidade)\n",
    "\n",
    "Nem toda empresa tem not√≠cia todo m√™s.\n",
    "\n",
    "* **L√≥gica:** `fillna(0.0)`\n",
    "* **Interpreta√ß√£o:** \"No News is Good News\" (ou pelo menos, n√£o √© bad news). A aus√™ncia de not√≠cias √© tratada como **Sentimento Neutro**, garantindo que o algoritmo de Scoring n√£o quebre ao encontrar um valor nulo.\n",
    "\n",
    "#### üìÇ Outputs (Sa√≠das)\n",
    "\n",
    "O arquivo gerado (`../data/aurum_master_features_final.parquet`) √© o **Artefato Final** da engenharia de dados. Ele cont√©m:\n",
    "\n",
    "| Coluna | Descri√ß√£o |\n",
    "| --- | --- |\n",
    "| `date` | Data de refer√™ncia (fim do m√™s). |\n",
    "| `ticker` | C√≥digo do ativo (ex: PETR4.SA). |\n",
    "| `Adj Close` | Pre√ßo ajustado. |\n",
    "| `ROE`, `D√≠vida...` | Indicadores fundamentalistas (com Lag). |\n",
    "| **`SENTIMENT_SCORE`** | **Novo:** Nota de qualidade das not√≠cias (-1 a 1). |\n",
    "| **`NEWS_VOLUME`** | **Novo:** Quantidade de not√≠cias no m√™s. |\n",
    "\n",
    "#### üöÄ Como Executar\n",
    "\n",
    "Este script deve ser rodado **ap√≥s** a unifica√ß√£o financeira (Step 04) e o processamento de NLP.\n",
    "\n",
    "```bash\n",
    "python src/step_06_merge_final_sentiment.py\n",
    "\n",
    "```\n",
    "\n",
    "#### üìã Verifica√ß√£o de Sucesso\n",
    "\n",
    "No final da execu√ß√£o, o script exibe uma amostra dos meses com maior volume de not√≠cias (\"Buzz\"). Verifique se os dados fazem sentido:\n",
    "\n",
    "```text\n",
    "üîé Top 5 Meses com Mais Not√≠cias:\n",
    "            date    ticker  SENTIMENT_SCORE  NEWS_VOLUME\n",
    " 2023-01-31  AMER3.SA        -0.8500        150.0   <-- Ex: Crise Americanas\n",
    " 2021-02-28  PETR4.SA        -0.4200         85.0\n",
    "\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40e3b584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ü¶Å INICIANDO MERGE FINAL (V6 - FONTES CORRETAS) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-20 02:40:15,747 - üìÇ Lendo 97 arquivos do MarketAux...\n",
      "2025-12-20 02:40:16,583 - üìä Linhas -> Google: 2766 | MarketAux: 204\n",
      "2025-12-20 02:40:16,588 - üîÑ Calculando 'Fator Sentimento' Mensal...\n",
      "2025-12-20 02:40:16,614 - ‚úÖ Base Agregada: 287 linhas (Ticker x M√™s).\n",
      "2025-12-20 02:40:16,650 - üîó Realizando Merge (Left Join)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "üèÜ DATASET V4 PRONTO: ../data/aurum_master_features_final.parquet\n",
      "========================================\n",
      "\n",
      "üîé Top 5 Meses com Mais Not√≠cias:\n",
      "            date    ticker  SENTIMENT_SCORE  NEWS_VOLUME\n",
      "12561 2022-05-31  PETR4.SA        -0.111604         15.0\n",
      "12486 2022-05-01  PETR4.SA        -0.111604         15.0\n",
      "12982 2022-08-31  BBAS3.SA        -0.281047          9.0\n",
      "12299 2022-03-31  PETR4.SA         0.105945          9.0\n",
      "12219 2022-03-01  PETR4.SA         0.105945          9.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# --- CONFIGURA√á√ÉO DE CAMINHOS (AJUSTADO CONFORME SEU PEDIDO) ---\n",
    "# 1. Dados Financeiros (Pre√ßo + Fundamentos)\n",
    "PATH_MESTRE_ANTIGO = \"../data/aurum_master_features.parquet\"\n",
    "\n",
    "# 2. Fonte Google News (Arquivo √önico)\n",
    "PATH_GOOGLE_NEWS = \"../data/news/news_with_sentiment.parquet\" \n",
    "\n",
    "# 3. Fonte MarketAux (Pasta com Parquets processados pelo RoBERTa)\n",
    "DIR_MARKETAUX = \"../data/news/processed/marketaux_roberta\"\n",
    "\n",
    "# 4. Sa√≠da Final\n",
    "OUTPUT_FILE = \"../data/aurum_master_features_final.parquet\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "def carregar_google():\n",
    "    \"\"\"Carrega e padroniza o Google News\"\"\"\n",
    "    if not os.path.exists(PATH_GOOGLE_NEWS):\n",
    "        logging.warning(f\"‚ö†Ô∏è Google News n√£o encontrado em: {PATH_GOOGLE_NEWS}\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    try:\n",
    "        df = pd.read_parquet(PATH_GOOGLE_NEWS)\n",
    "        # Padroniza colunas\n",
    "        # Seu arquivo Google tem: ticker_query, title, sentiment_weighted (ou numeric_sentiment), published_date\n",
    "        cols_map = {\n",
    "            'ticker_query': 'ticker',\n",
    "            'published_date': 'date',\n",
    "            'sentiment_weighted': 'score'\n",
    "        }\n",
    "        # Fallback se n√£o tiver o weighted\n",
    "        if 'sentiment_weighted' not in df.columns and 'numeric_sentiment' in df.columns:\n",
    "            cols_map['numeric_sentiment'] = 'score'\n",
    "            \n",
    "        df = df.rename(columns=cols_map)\n",
    "        df['source'] = 'Google'\n",
    "        \n",
    "        # Garante data limpa\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce', utc=True).dt.tz_localize(None)\n",
    "        \n",
    "        # Filtra colunas vitais\n",
    "        cols = ['date', 'ticker', 'score', 'title', 'source']\n",
    "        return df[[c for c in cols if c in df.columns]]\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erro ao ler Google News: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def carregar_marketaux():\n",
    "    \"\"\"Carrega e padroniza o MarketAux (M√∫ltiplos arquivos)\"\"\"\n",
    "    if not os.path.exists(DIR_MARKETAUX):\n",
    "        logging.warning(f\"‚ö†Ô∏è Pasta MarketAux n√£o encontrada: {DIR_MARKETAUX}\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    arquivos = list(Path(DIR_MARKETAUX).glob(\"*.parquet\"))\n",
    "    logging.info(f\"üìÇ Lendo {len(arquivos)} arquivos do MarketAux...\")\n",
    "    \n",
    "    dfs = []\n",
    "    for f in arquivos:\n",
    "        try:\n",
    "            df = pd.read_parquet(f)\n",
    "            # Padroniza\n",
    "            if 'sentiment' in df.columns:\n",
    "                df = df.rename(columns={'sentiment': 'score'})\n",
    "            \n",
    "            df['source'] = 'MarketAux'\n",
    "            df['date'] = pd.to_datetime(df['date'], errors='coerce', utc=True).dt.tz_localize(None)\n",
    "            \n",
    "            cols = ['date', 'ticker', 'score', 'title', 'source']\n",
    "            # Garante que existem (Title pode n√£o existir em alguns processados antigos)\n",
    "            cols_validas = [c for c in cols if c in df.columns]\n",
    "            dfs.append(df[cols_validas])\n",
    "        except: pass\n",
    "        \n",
    "    if not dfs: return pd.DataFrame()\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def main():\n",
    "    print(\"--- ü¶Å INICIANDO MERGE FINAL (V6 - FONTES CORRETAS) ---\")\n",
    "    \n",
    "    # 1. Carrega as duas fontes\n",
    "    df_google = carregar_google()\n",
    "    df_mkt = carregar_marketaux()\n",
    "    \n",
    "    logging.info(f\"üìä Linhas -> Google: {len(df_google)} | MarketAux: {len(df_mkt)}\")\n",
    "    \n",
    "    if df_google.empty and df_mkt.empty:\n",
    "        logging.error(\"‚ùå Nenhuma not√≠cia carregada. Abortando.\")\n",
    "        return\n",
    "\n",
    "    # 2. Unifica\n",
    "    df_news = pd.concat([df_google, df_mkt], ignore_index=True)\n",
    "    \n",
    "    # 3. Agrega√ß√£o Mensal (M√©dia do Score no M√™s)\n",
    "    logging.info(\"üîÑ Calculando 'Fator Sentimento' Mensal...\")\n",
    "    \n",
    "    # Cria chave M√™s\n",
    "    df_news['periodo'] = df_news['date'].dt.to_period('M')\n",
    "    \n",
    "    df_agregado = df_news.groupby(['ticker', 'periodo']).agg(\n",
    "        SENTIMENT_SCORE=('score', 'mean'),      # Qualidade (Positivo/Negativo)\n",
    "        NEWS_VOLUME=('score', 'count'),         # Quantidade (Buzz)\n",
    "        SENTIMENT_VOL=('score', 'std')          # Volatilidade de opini√£o (Pol√™mica)\n",
    "    ).reset_index()\n",
    "    \n",
    "    logging.info(f\"‚úÖ Base Agregada: {len(df_agregado)} linhas (Ticker x M√™s).\")\n",
    "\n",
    "    # 4. Cruzamento com Financeiro\n",
    "    if not os.path.exists(PATH_MESTRE_ANTIGO):\n",
    "        logging.error(\"‚ùå Arquivo Mestre Financeiro n√£o encontrado.\")\n",
    "        return\n",
    "\n",
    "    df_mestre = pd.read_parquet(PATH_MESTRE_ANTIGO)\n",
    "    df_mestre['date'] = pd.to_datetime(df_mestre['date'])\n",
    "    df_mestre['periodo'] = df_mestre['date'].dt.to_period('M')\n",
    "    \n",
    "    logging.info(\"üîó Realizando Merge (Left Join)...\")\n",
    "    \n",
    "    df_final = pd.merge(\n",
    "        df_mestre,\n",
    "        df_agregado[['ticker', 'periodo', 'SENTIMENT_SCORE', 'NEWS_VOLUME']],\n",
    "        on=['ticker', 'periodo'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 5. Tratamento de Nulos\n",
    "    # Se n√£o teve not√≠cia, Sentimento = 0 (Neutro)\n",
    "    df_final['SENTIMENT_SCORE'] = df_final['SENTIMENT_SCORE'].fillna(0.0)\n",
    "    df_final['NEWS_VOLUME'] = df_final['NEWS_VOLUME'].fillna(0.0)\n",
    "    \n",
    "    # Limpeza\n",
    "    df_final = df_final.drop(columns=['periodo'])\n",
    "    \n",
    "    # 6. Salva\n",
    "    df_final.to_parquet(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"üèÜ DATASET V4 PRONTO: {OUTPUT_FILE}\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # Amostra dos dados com not√≠cias\n",
    "    print(\"\\nüîé Top 5 Meses com Mais Not√≠cias:\")\n",
    "    print(df_final.sort_values('NEWS_VOLUME', ascending=False)[['date', 'ticker', 'SENTIMENT_SCORE', 'NEWS_VOLUME']].head(5).to_string())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea775ab",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### üßÆ Script: Aurum Scoring Engine (Final)\n",
    "\n",
    "> **Fase:** Intelig√™ncia / Modelagem\n",
    "> **Arquivo Alvo:** `aurum_scored_history.parquet`\n",
    "\n",
    "Este script √© o **Cora√ß√£o Quantitativo** do sistema Aurum. Ele aplica um modelo multifatorial para transformar m√©tricas financeiras brutas (que possuem escalas diferentes, como `%` e `R$`) em uma nota padronizada de 0 a 100, integrando tamb√©m os dados de Sentimento (IA) e Risco (Volatilidade).\n",
    "\n",
    "### üéØ Objetivos\n",
    "\n",
    "1. **Normaliza√ß√£o Estat√≠stica:** Tornar compar√°veis m√©tricas de naturezas distintas (ex: comparar ROE com Sentimento).\n",
    "2. **Pondera√ß√£o de Fatores:** Aplicar a \"Tese de Investimento\" atribuindo pesos espec√≠ficos para Rentabilidade, Solv√™ncia e Risco.\n",
    "3. **Classifica√ß√£o:** Gerar o **Aurum Quality Score** e atribuir notas (Grades A-E) para facilitar a sele√ß√£o de ativos.\n",
    "\n",
    "### üèóÔ∏è Arquitetura L√≥gica (Algoritmo)\n",
    "\n",
    "O motor utiliza uma fun√ß√£o **Sigmoide sobre Z-Score** para suavizar outliers e garantir uma distribui√ß√£o justa de notas.\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    %% --- INPUT ---\n",
    "    Input[(\"<b>Input: Master Features</b><br/>aurum_master_features_final.parquet\")]\n",
    "\n",
    "    %% --- CONFIGURA√á√ÉO ---\n",
    "    subgraph Config [\"1. Defini√ß√£o de Pesos (Tese)\"]\n",
    "        Rent[\"<b>Rentabilidade (45%)</b><br/>ROE, ROA, Margens\"]\n",
    "        Solv[\"<b>Solv√™ncia (30%)</b><br/>D√≠vida/PL, Liquidez\"]\n",
    "        Risk[\"<b>Risco & IA (25%)</b><br/>Sentimento (NLP) + Volatilidade\"]\n",
    "    end\n",
    "\n",
    "    %% --- NORMALIZA√á√ÉO (LOOP) ---\n",
    "    subgraph Norm [\"2. Pipeline de Normaliza√ß√£o (Por M√©trica)\"]\n",
    "        Winsor[\"<b>Winsorization</b><br/>Clip Outliers (1% - 99%)\"]\n",
    "        ZScore[\"<b>Z-Score</b><br/>Desvio da M√©dia\"]\n",
    "        Sigmoid[\"<b>Fun√ß√£o Sigmoide</b><br/>Transforma√ß√£o para 0-1\"]\n",
    "        Scale[\"<b>Escala Final</b><br/>0 a 100\"]\n",
    "    end\n",
    "\n",
    "    %% --- AGREGA√á√ÉO ---\n",
    "    subgraph Agg [\"3. C√°lculo Final\"]\n",
    "        Weighted[\"<b>M√©dia Ponderada</b><br/>Soma (Nota * Peso)\"]\n",
    "        Grade[\"<b>Classifica√ß√£o (Grade)</b><br/>A (>80), B (>60)...\"]\n",
    "    end\n",
    "\n",
    "    %% --- OUTPUTS ---\n",
    "    subgraph Output [\"4. Sa√≠da\"]\n",
    "        Hist[(\"<b>Hist√≥rico Completo</b><br/>aurum_scored_history.parquet\")]\n",
    "        Snapshot[(\"<b>Carteira Atual (Snapshot)</b><br/>aurum_scored_latest_portfolio.csv\")]\n",
    "    end\n",
    "\n",
    "    %% --- LIGA√á√ïES ---\n",
    "    Input --> Rent & Solv & Risk\n",
    "    Rent & Solv & Risk --> Winsor\n",
    "    Winsor --> ZScore --> Sigmoid --> Scale\n",
    "    Scale --> Weighted --> Grade\n",
    "    Grade --> Hist & Snapshot\n",
    "\n",
    "```\n",
    "\n",
    "### ‚öôÔ∏è Detalhes da Implementa√ß√£o\n",
    "\n",
    "#### 1. M√©tricas e Pesos\n",
    "\n",
    "O sistema avalia 3 pilares fundamentais:\n",
    "\n",
    "* **Rentabilidade (45%):** Efici√™ncia em gerar lucro (`ROE`, `ROIC`, `Margens`).\n",
    "* **Solv√™ncia (30%):** Sa√∫de financeira (`D√≠vida/PL`, `Liquidez`).\n",
    "* **Qualidade/Risco (25%):** O diferencial do Aurum (`Sentimento` positivo e `Volatilidade` baixa).\n",
    "\n",
    "#### 2. O Segredo Matem√°tico (Sigmoide)\n",
    "\n",
    "Em vez de usar um ranking simples (que ignora a magnitude da diferen√ßa), usamos a Sigmoide:\n",
    "\n",
    "* `Z = (Valor - M√©dia) / DesvioPadr√£o`\n",
    "* `Score = 1 / (1 + e^-Z)`\n",
    "* **Por que?** Isso evita que uma √∫nica empresa com ROE de 500% distor√ßa o ranking inteiro. A nota \"satura\" suavemente perto de 100.\n",
    "\n",
    "#### 3. Tratamento de Dire√ß√£o\n",
    "\n",
    "O script entende que para algumas m√©tricas, **menos √© mais**.\n",
    "\n",
    "* **Exemplo:** D√≠vida Alta.\n",
    "* O script inverte a nota automaticamente: `Score = 1 - Sigmoide`.\n",
    "\n",
    "### üìÇ Outputs (Sa√≠das)\n",
    "\n",
    "1. **`../data/aurum_final_scores/aurum_scored_history.parquet`**:\n",
    "* O arquivo \"Ouro\". Cont√©m todo o hist√≥rico de notas m√™s a m√™s. √â este arquivo que alimenta o Backtest.\n",
    "\n",
    "\n",
    "2. **`../data/aurum_final_scores/aurum_scored_latest_portfolio.csv`**:\n",
    "* Um relat√≥rio executivo mostrando a situa√ß√£o **HOJE**.\n",
    "* Ideal para abrir no Excel e decidir quais a√ß√µes comprar na segunda-feira.\n",
    "\n",
    "\n",
    "\n",
    "### üöÄ Como Executar\n",
    "\n",
    "Este √© o passo final antes do Backtest.\n",
    "\n",
    "```bash\n",
    "python src/step_08_scoring_engine_final.py\n",
    "\n",
    "```\n",
    "\n",
    "### üìã Verifica√ß√£o de Sucesso\n",
    "\n",
    "O script imprime o **Top 10 Atual** no console. Verifique se as empresas listadas s√£o coerentes (Blue Chips de qualidade):\n",
    "\n",
    "```text\n",
    "üèÜ TOP 10 A√á√ïES HOJE (Qualidade Aurum):\n",
    " ticker  aurum_quality_score quality_grade       ROE  SENTIMENT_SCORE\n",
    " WEGE3.SA            78.45             B   0.2450             0.8\n",
    " PSSA3.SA            76.12             B   0.1800             0.5\n",
    " ...\n",
    "\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf380bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 18:24:18,594 - INFO - Dados carregados: 18879 linhas de 94 empresas.\n",
      "2025-12-23 18:24:18,608 - INFO - üéØ Calculando scores individuais...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ü¶Å INICIANDO MOTOR DE SCORING AVAN√áADO ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-23 18:24:18,969 - INFO - ‚öñÔ∏è Calculando AURUM SCORE final...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "‚úÖ SCORING CONCLU√çDO!\n",
      "üìÅ Hist√≥rico: ..\\data\\aurum_final_scores\\aurum_scored_history.parquet\n",
      "üìÅ Snapshot (Excel): ..\\data\\aurum_final_scores\\aurum_scored_latest_portfolio.csv\n",
      "==================================================\n",
      "\n",
      "üèÜ TOP 10 A√á√ïES HOJE (Qualidade Aurum):\n",
      "   ticker  aurum_quality_score quality_grade      ROE  SENTIMENT_SCORE  VOLATILIDADE\n",
      " MULT3.SA            57.928712             C 2.201927              0.0      0.015106\n",
      " SLCE3.SA            57.340818             C 0.119173              0.0      0.012109\n",
      " CURY3.SA            57.197869             C 0.166912              0.0      0.015372\n",
      " MOTV3.SA            56.527477             C 0.110268              0.0      0.014928\n",
      " SUZB3.SA            56.399023             C 0.301152              0.0      0.012409\n",
      " ISAE4.SA            55.815113             C 0.158355              0.0      0.011797\n",
      " RAIL3.SA            55.372407             C 0.095788              0.0      0.022669\n",
      " EGIE3.SA            55.311749             C 0.158355              0.0      0.014555\n",
      " PCAR3.SA            53.892008             C 0.389937              0.0      0.027663\n",
      "IGTI11.SA            53.777022             C 0.038535              0.0      0.014662\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- CONFIGURA√á√ÉO ---\n",
    "INPUT_FILE = \"../data/aurum_master_features_final.parquet\"\n",
    "OUTPUT_DIR = \"../data/aurum_final_scores\"\n",
    "\n",
    "@dataclass\n",
    "class ScoringMetric:\n",
    "    name: str\n",
    "    weight: float\n",
    "    direction: int \n",
    "    description: str\n",
    "    min_value: float = None\n",
    "    max_value: float = None\n",
    "    ideal_range: Tuple[float, float] = None\n",
    "\n",
    "class AurumScoringSystem:\n",
    "    def __init__(self):\n",
    "        self.scoring_metrics = self._initialize_scoring_metrics()\n",
    "        self.quality_thresholds = {'A': 80, 'B': 60, 'C': 40, 'D': 20, 'E': 0}\n",
    "        \n",
    "    def _initialize_scoring_metrics(self) -> Dict[str, ScoringMetric]:\n",
    "        metrics_config = {\n",
    "            'ROE': ScoringMetric('ROE', 0.15, 1, 'Efici√™ncia do capital pr√≥prio'),\n",
    "            'ROA': ScoringMetric('ROA', 0.10, 1, 'Efici√™ncia dos ativos'),\n",
    "            'ROIC': ScoringMetric('ROIC', 0.10, 1, 'Retorno sobre capital investido'),\n",
    "            'MARGEM_EBIT': ScoringMetric('MARGEM_EBIT', 0.05, 1, 'Margem Operacional'),\n",
    "            'MARGEM_LIQUIDA': ScoringMetric('MARGEM_LIQUIDA', 0.05, 1, 'Margem L√≠quida'),\n",
    "\n",
    "            'ALAVANCAGEM': ScoringMetric('ALAVANCAGEM', 0.08, -1, 'Passivo/Ativo (Menor √© melhor)'),\n",
    "            'DIVIDA_PL': ScoringMetric('DIVIDA_PL', 0.08, -1, 'D√≠vida/PL (Menor √© melhor)'),\n",
    "            'LIQUIDEZ_CORRENTE': ScoringMetric('LIQUIDEZ_CORRENTE', 0.07, 1, 'Liquidez de curto prazo'),\n",
    "            'GIRO_ATIVO': ScoringMetric('GIRO_ATIVO', 0.07, 1, 'Efici√™ncia de vendas'),\n",
    "\n",
    "            'SENTIMENT_SCORE': ScoringMetric('SENTIMENT_SCORE', 0.15, 1, 'Sentimento de Not√≠cias (IA)'),\n",
    "            'VOLATILIDADE': ScoringMetric('VOLATILIDADE', 0.10, -1, 'Volatilidade do Pre√ßo (Menor √© melhor)')\n",
    "        }\n",
    "        return metrics_config\n",
    "    \n",
    "    def calculate_individual_scores(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        scores_df = df.copy()\n",
    "        \n",
    "        cols_to_numeric = [m for m in self.scoring_metrics.keys() if m in scores_df.columns]\n",
    "        for col in cols_to_numeric:\n",
    "            scores_df[col] = pd.to_numeric(scores_df[col], errors='coerce')\n",
    "\n",
    "        logger.info(\"üéØ Calculando scores individuais...\")\n",
    "        \n",
    "        valid_metrics = []\n",
    "        for metric_name, config in self.scoring_metrics.items():\n",
    "            if metric_name not in scores_df.columns:\n",
    "                logger.warning(f\"‚ö†Ô∏è M√©trica {metric_name} n√£o encontrada. Peso ser√° ignorado.\")\n",
    "                continue\n",
    "            \n",
    "            if scores_df[metric_name].abs().sum() == 0:\n",
    "                 # logger.warning(f\"‚ö†Ô∏è M√©trica {metric_name} est√° vazia/zerada. Ignorando.\")\n",
    "                 pass\n",
    "\n",
    "            series = scores_df[metric_name]\n",
    "            lower = series.quantile(0.01)\n",
    "            upper = series.quantile(0.99)\n",
    "            series_clipped = series.clip(lower, upper)\n",
    "            \n",
    "            mean = series_clipped.mean()\n",
    "            std = series_clipped.std()\n",
    "            if std == 0: std = 1\n",
    "            \n",
    "            z_score = (series_clipped - mean) / std\n",
    "            sigmoid_score = 1 / (1 + np.exp(-z_score)) \n",
    "            \n",
    "            if config.direction == -1:\n",
    "                sigmoid_score = 1 - sigmoid_score\n",
    "                \n",
    "            col_score = f'score_{metric_name}'\n",
    "            scores_df[col_score] = sigmoid_score * 100\n",
    "            \n",
    "            valid_metrics.append((col_score, config.weight))\n",
    "            \n",
    "        return scores_df, valid_metrics\n",
    "    \n",
    "    def calculate_final_score(self, df: pd.DataFrame, metric_weights: List[Tuple]) -> pd.DataFrame:\n",
    "        logger.info(\"‚öñÔ∏è Calculando AURUM SCORE final...\")\n",
    "        \n",
    "        total_weight = sum(w for _, w in metric_weights)\n",
    "        if total_weight == 0: return df\n",
    "        \n",
    "        df['aurum_quality_score'] = 0.0\n",
    "        \n",
    "        for col_score, weight in metric_weights:\n",
    "            adjusted_weight = weight / total_weight\n",
    "            df['aurum_quality_score'] += df[col_score].fillna(50) * adjusted_weight\n",
    "    \n",
    "        conditions = [\n",
    "            df['aurum_quality_score'] >= 80,\n",
    "            df['aurum_quality_score'] >= 60,\n",
    "            df['aurum_quality_score'] >= 40,\n",
    "            df['aurum_quality_score'] >= 20\n",
    "        ]\n",
    "        choices = ['A', 'B', 'C', 'D']\n",
    "        df['quality_grade'] = np.select(conditions, choices, default='E')\n",
    "        \n",
    "        return df\n",
    "\n",
    "def run_scoring_pipeline():\n",
    "    print(\"--- ü¶Å INICIANDO MOTOR DE SCORING AVAN√áADO ---\")\n",
    "    \n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        logger.error(f\"Arquivo n√£o encontrado: {INPUT_FILE}\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_parquet(INPUT_FILE)\n",
    "    logger.info(f\"Dados carregados: {len(df)} linhas de {len(df['ticker'].unique())} empresas.\")\n",
    "    \n",
    "    engine = AurumScoringSystem()\n",
    "    \n",
    "    df_scored, valid_metrics = engine.calculate_individual_scores(df)\n",
    "    \n",
    "    df_final = engine.calculate_final_score(df_scored, valid_metrics)\n",
    "    \n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    file_hist = Path(OUTPUT_DIR) / \"aurum_scored_history.parquet\"\n",
    "    df_final.to_parquet(file_hist, index=False)\n",
    "    \n",
    "    df_latest = df_final.sort_values('date').groupby('ticker').tail(1)\n",
    "    file_latest = Path(OUTPUT_DIR) / \"aurum_scored_latest_portfolio.csv\"\n",
    "    df_latest.to_csv(file_latest, sep=';', decimal=',', index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"‚úÖ SCORING CONCLU√çDO!\")\n",
    "    print(f\"üìÅ Hist√≥rico: {file_hist}\")\n",
    "    print(f\"üìÅ Snapshot (Excel): {file_latest}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"\\nüèÜ TOP 10 A√á√ïES HOJE (Qualidade Aurum):\")\n",
    "    cols_show = ['ticker', 'aurum_quality_score', 'quality_grade', 'ROE', 'SENTIMENT_SCORE', 'VOLATILIDADE']\n",
    "    top10 = df_latest.sort_values('aurum_quality_score', ascending=False).head(10)\n",
    "    print(top10[cols_show].to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_scoring_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
