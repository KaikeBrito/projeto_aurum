{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb779f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b3_iframe_selenium.py\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import logging\n",
    "import re\n",
    "import time\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Optional, Set\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "# Selenium imports are optional; import when needed to avoid heavy dependency at module import\n",
    "try:\n",
    "    from selenium import webdriver\n",
    "    from selenium.common.exceptions import (\n",
    "        ElementClickInterceptedException,\n",
    "        StaleElementReferenceException,\n",
    "        TimeoutException,\n",
    "    )\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.chrome.options import Options\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from webdriver_manager.chrome import ChromeDriverManager\n",
    "except Exception:  # pragma: no cover - selenium optional\n",
    "    webdriver = None\n",
    "\n",
    "\n",
    "\n",
    "#download_histories.py\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Optional\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "\n",
    "# analysis_historical_data.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# cvm_downloader.py\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "import sys\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Iterable, List, Tuple, Dict\n",
    "\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# cvm_parser.py\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "# news_crawler_rss.py\n",
    "import feedparser\n",
    "import urllib.parse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff819c4e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üß† Arquitetura de Coleta e Processamento ‚Äî Projeto Aurum\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[\"Web Scraping na p√°gina da B3<br/><b>Ticker</b>\"]\n",
    "\n",
    "    %% Yahoo Finance\n",
    "    A --> B[\"Web Scraping no Yahoo Finance<br/><b>Dados Hist√≥ricos</b>\"]\n",
    "    B --> C[\"<b>Backtests</b>\"]\n",
    "    C --> D[\"<b>Relat√≥rio de Desempenho</b>\"]\n",
    "\n",
    "    %% Google News\n",
    "    A --> E[\"Web Scraping na p√°gina do Google News<br/><b>Dados de Not√≠cias</b>\"]\n",
    "    E --> F[\"<b>Modelo de Sentimento</b>\"]\n",
    "    F --> G[\"<b>Score de Sentimento</b>\"]\n",
    "\n",
    "    %% CVM\n",
    "    A --> H[\"Baixar por uma URL gov.br<br/><b>Baixar as pastas CVM</b>\"]\n",
    "    H --> I[\"<b>Parser e Processador</b>\"]\n",
    "    I --> J[\"<b>Dados Fundamentalistas</b>\"]\n",
    "\n",
    "    %% Uni√£o\n",
    "    G --> K[\"<b>DataFrame Fundamentalistas e Score de Sentimento</b>\"]\n",
    "    J --> K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71993a44",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### b3_iframe_full_extractor.py\n",
    "\n",
    "### üìò Descri√ß√£o\n",
    "Script respons√°vel por **extrair automaticamente todos os tickers listados na B3** (como o √≠ndice IBRX100), mesmo quando o conte√∫do est√° dentro de **iframes com pagina√ß√£o din√¢mica**.  \n",
    "O c√≥digo realiza a extra√ß√£o utilizando **Selenium + BeautifulSoup + Pandas**, com fallback inteligente caso o download direto da tabela falhe.\n",
    "\n",
    "### ‚öôÔ∏è Funcionalidades Principais\n",
    "- Extra√ß√£o de tickers da p√°gina da B3 (`https://sistemaswebb3-listados.b3.com.br/indexPage/day/IBXX`)\n",
    "- Normaliza√ß√£o dos tickers (ex: `PETR4` ‚Üí `PETR4.SA`)\n",
    "- Identifica√ß√£o autom√°tica de colunas e cabe√ßalhos de c√≥digo\n",
    "- Suporte a pagina√ß√£o autom√°tica e op√ß√µes de \"itens por p√°gina\"\n",
    "- Modo **headless** (sem abrir navegador) ou **debug** (com logs detalhados)\n",
    "- Gera√ß√£o de arquivo CSV (`tickers_ibrx100_full.csv`) com todos os tickers coletados\n",
    "\n",
    "\n",
    "\n",
    "### üß© Diagrama (classe/entidade) ‚Äî sa√≠da do CSV\n",
    "```mermaid\n",
    "classDiagram\n",
    "  class tickers_csv {\n",
    "    + string Ticker\n",
    "    --\n",
    "    C√≥digo de negocia√ß√£o normalizado do ativo na B3\n",
    "    Formato: <nome do papel> + .SA (ex.: PETR4.SA)\n",
    "    Lista sem duplicatas; normaliza para mai√∫sculas\n",
    "  }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1165989b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 14:25:02,100 - INFO - Iniciando requisi√ß√£o √† API da B3 (IndexProxy)...\n",
      "2025-12-09 14:25:03,040 - INFO - API retornou 97 ativos.\n",
      "2025-12-09 14:25:03,052 - ERROR - Falha cr√≠tica no extrator da API B3: \"['codNeg'] not in index\"\n",
      "2025-12-09 14:25:03,164 - ERROR - Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kaike\\AppData\\Local\\Temp\\ipykernel_22100\\1422043272.py\", line 71, in fetch_ibrx100_from_b3_api\n",
      "    df_final = df[['codNeg', 'part']].copy()\n",
      "               ~~^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kaike\\projeto_aurum\\venv_aurum\\Lib\\site-packages\\pandas\\core\\frame.py\", line 4119, in __getitem__\n",
      "    indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\kaike\\projeto_aurum\\venv_aurum\\Lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 6212, in _get_indexer_strict\n",
      "    self._raise_if_missing(keyarr, indexer, axis_name)\n",
      "  File \"c:\\Users\\kaike\\projeto_aurum\\venv_aurum\\Lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 6264, in _raise_if_missing\n",
      "    raise KeyError(f\"{not_found} not in index\")\n",
      "KeyError: \"['codNeg'] not in index\"\n",
      "\n",
      "2025-12-09 14:25:03,168 - ERROR - N√£o foi poss√≠vel gerar a lista de tickers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tempo total: 1.07 segundos\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import base64\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import time\n",
    "import urllib3\n",
    "\n",
    "# --- Configura√ß√£o ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Diret√≥rio para salvar\n",
    "OUTPUT_DIR = Path(\".\") \n",
    "OUTPUT_FILENAME_CSV = \"tickers_ibrx100_full.csv\"\n",
    "OUTPUT_FILENAME_PARQUET = \"tickers_ibrx100_full.parquet\"\n",
    "\n",
    "def fetch_ibrx100_from_b3_api() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Consome diretamente a API JSON da B3 para obter a composi√ß√£o do IBRX-100.\n",
    "    \"\"\"\n",
    "    logger.info(\"Iniciando requisi√ß√£o √† API da B3 (IndexProxy)...\")\n",
    "    \n",
    "    try:\n",
    "        # IBXX √© o c√≥digo do IBRX-100\n",
    "        params = {\n",
    "            \"language\": \"pt-br\",\n",
    "            \"pageNumber\": 1,\n",
    "            \"pageSize\": 120, \n",
    "            \"index\": \"IBXX\", \n",
    "            \"segment\": \"1\"\n",
    "        }\n",
    "        \n",
    "        params_json = json.dumps(params)\n",
    "        params_b64 = base64.b64encode(params_json.encode(\"utf-8\")).decode(\"utf-8\")\n",
    "        \n",
    "        url = f\"https://sistemaswebb3-listados.b3.com.br/indexProxy/indexCall/GetPortfolioDay/{params_b64}\"\n",
    "        \n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "        \n",
    "        # Desativa warnings de SSL\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "        response = requests.get(url, headers=headers, timeout=15, verify=False)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            logger.error(f\"Erro na requisi√ß√£o: Status {response.status_code}\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        \n",
    "        if not results:\n",
    "            logger.warning(\"JSON retornado pela B3 est√° vazio na chave 'results'.\")\n",
    "            return None\n",
    "            \n",
    "        logger.info(f\"API retornou {len(results)} ativos.\")\n",
    "        \n",
    "        # Criar DataFrame Bruto\n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        # --- DEBUG: Imprimir colunas encontradas para ver o nome correto ---\n",
    "        logger.info(f\"Colunas encontradas no JSON: {df.columns.tolist()}\")\n",
    "        \n",
    "        # --- L√ìGICA DE CORRE√á√ÉO: Identificar a coluna do Ticker ---\n",
    "        # A B3 varia entre 'codNeg', 'cod', 'acronym', 'asset' dependendo do endpoint\n",
    "        coluna_ticker = None\n",
    "        possiveis_nomes = ['codNeg', 'cod', 'acronym', 'symbol', 'identifier']\n",
    "        \n",
    "        for col in possiveis_nomes:\n",
    "            if col in df.columns:\n",
    "                coluna_ticker = col\n",
    "                logger.info(f\"Coluna de ticker identificada como: '{col}'\")\n",
    "                break\n",
    "        \n",
    "        if not coluna_ticker:\n",
    "            logger.error(\"N√£o foi poss√≠vel identificar a coluna de Ticker no DataFrame.\")\n",
    "            logger.error(f\"Colunas dispon√≠veis: {df.columns.tolist()}\")\n",
    "            return None\n",
    "\n",
    "        # Selecionar e renomear\n",
    "        # Tamb√©m tentamos garantir a coluna de participa√ß√£o ('part' ou 'participation')\n",
    "        coluna_part = 'part' if 'part' in df.columns else None\n",
    "        \n",
    "        colunas_selecao = [coluna_ticker]\n",
    "        if coluna_part:\n",
    "            colunas_selecao.append(coluna_part)\n",
    "            \n",
    "        df_final = df[colunas_selecao].copy()\n",
    "        \n",
    "        # Renomear para o padr√£o do nosso sistema\n",
    "        rename_map = {coluna_ticker: 'ticker'}\n",
    "        if coluna_part:\n",
    "            rename_map[coluna_part] = 'participacao'\n",
    "            \n",
    "        df_final = df_final.rename(columns=rename_map)\n",
    "        \n",
    "        # 6. Normaliza√ß√£o para yfinance (.SA)\n",
    "        logger.info(\"Normalizando tickers (adicionando .SA)...\")\n",
    "        # Remove espa√ßos em branco que a B3 as vezes deixa\n",
    "        df_final['ticker'] = df_final['ticker'].str.strip()\n",
    "        df_final['Ticker_Yahoo'] = df_final['ticker'].apply(lambda x: f\"{x}.SA\")\n",
    "        \n",
    "        return df_final\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Falha cr√≠tica no extrator da API B3: {e}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "def save_data(df: pd.DataFrame):\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    path_csv = OUTPUT_DIR / OUTPUT_FILENAME_CSV\n",
    "    path_parquet = OUTPUT_DIR / OUTPUT_FILENAME_PARQUET\n",
    "    \n",
    "    df.to_csv(path_csv, index=False, encoding='utf-8-sig')\n",
    "    df.to_parquet(path_parquet, index=False)\n",
    "    \n",
    "    logger.info(f\"üíæ Arquivos salvos:\")\n",
    "    logger.info(f\"   -> {path_csv}\")\n",
    "    logger.info(f\"   -> {path_parquet}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df_result = fetch_ibrx100_from_b3_api()\n",
    "    \n",
    "    if df_result is not None and not df_result.empty:\n",
    "        print(\"\\n--- Amostra do IBRX-100 (API B3) ---\")\n",
    "        print(df_result.head())\n",
    "        save_data(df_result)\n",
    "    else:\n",
    "        logger.error(\"N√£o foi poss√≠vel gerar a lista de tickers.\")\n",
    "        \n",
    "    print(f\"\\nTempo total: {time.time() - start_time:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4057eb44",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### üóÇÔ∏è Hist√≥rico ‚Äî downloader & combinador (b3_hist_downloader.py)\n",
    "\n",
    "**Resumo r√°pido**  \n",
    "Script que baixa hist√≥ricos de pre√ßos (via `yfinance`), salva por ticker em **parquet** (e opcionalmente CSV), e permite combinar todos os parquets em um √∫nico arquivo *long-format*. Feito para rodar em batch com retry/backoff, pular tickers j√° salvos e gerar um resumo final.\n",
    "\n",
    "\n",
    "#### ‚ú® Principais responsabilidades\n",
    "- Baixar hist√≥ricos de pre√ßo por lotes (batch) com `yfinance`.  \n",
    "- Salvar cada ticker em `data/historical/{TICKER}.parquet` e `{TICKER}.csv`.  \n",
    "- Gerenciar retries (exponencial), fallback ticker-a-ticker e logs.  \n",
    "- Gerar CSV de resumo por execu√ß√£o (`download_summary_YYYYMMDDTHHMMSSZ.csv`).  \n",
    "- Recombinar todos os parquets em um √∫nico `all_histories.parquet` / `all_histories.csv`.\n",
    "\n",
    "\n",
    "#### üß© Fun√ß√µes principais (one-liners)\n",
    "- `ticker_exists_local(ticker)` ‚Üí verifica exist√™ncia de `{ticker}.parquet`.  \n",
    "- `save_history_df(ticker, df, save_csv=True)` ‚Üí salva parquet e CSV; garante coluna `date`.  \n",
    "- `download_batch(batch, start, threads)` ‚Üí tenta baixar um batch via `yfinance.download` com retries e fallback.  \n",
    "- `download_all_histories(tickers, start, force, save_summary, save_csv_per_ticker)` ‚Üí orquestra o download em batches, salva e retorna um `DataFrame` resumo.  \n",
    "- `combine_all_to_single_parquet(out_path, out_csv, tickers)` ‚Üí concatena todos os parquets em formato long e salva.\n",
    "\n",
    "\n",
    "#### ‚öôÔ∏è Par√¢metros principais (valores padr√£o)\n",
    "| Par√¢metro | Valor padr√£o |\n",
    "|---:|:---|\n",
    "| `HIST_DIR` | `data/historical/` |\n",
    "| `DEFAULT_START` | `\"2011-01-01\"` |\n",
    "| `BATCH_SIZE` | `15` |\n",
    "| `MAX_ATTEMPTS` | `4` |\n",
    "| `SLEEP_BETWEEN_BATCHES` | `1` (seg) |\n",
    "| `SLEEP_BETWEEN_TICKERS` | `0.2` (seg) |\n",
    "\n",
    "\n",
    "#### üìÇ Sa√≠das geradas\n",
    "- `data/historical/{TICKER}.parquet` ‚Äî parquet por ticker.  \n",
    "- `data/historical/{TICKER}.csv` ‚Äî  CSV por ticker.  \n",
    "- `data/historical/download_summary_{ts}.csv` ‚Äî resumo da execu√ß√£o.  \n",
    "- `data/historical/all_histories.parquet` & `all_histories.csv` ‚Äî concat final (long format).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging simples\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "# diret√≥rio onde os hist√≥ricos ser√£o salvos (compat√≠vel com seu script anterior)\n",
    "HIST_DIR = os.path.join(\"data\", \"historical\")\n",
    "os.makedirs(HIST_DIR, exist_ok=True)\n",
    "\n",
    "# par√¢metros de download\n",
    "DEFAULT_START = \"2011-01-01\"\n",
    "BATCH_SIZE = 15\n",
    "MAX_ATTEMPTS = 4          \n",
    "SLEEP_BETWEEN_BATCHES = 1  \n",
    "SLEEP_BETWEEN_TICKERS = 0.2\n",
    "\n",
    "def ticker_exists_local(ticker: str) -> bool:\n",
    "    \"\"\"Verifica se j√° existe parquet salvo para ticker (usa para pular downloads)\"\"\"\n",
    "    path = os.path.join(HIST_DIR, f\"{ticker}.parquet\")\n",
    "    return os.path.isfile(path)\n",
    "\n",
    "def save_history_df(ticker: str, df: pd.DataFrame, save_csv: bool = True):\n",
    "    \"\"\"Salva DataFrame em parquet e opcionalmente em CSV. Garante coluna 'date' se √≠ndice for DatetimeIndex.\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        raise ValueError(\"DataFrame nulo ou vazio\")\n",
    "    df = df.copy()\n",
    "    # garantir que a coluna de data exista como coluna\n",
    "    if isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index.name = \"date\"\n",
    "        df = df.reset_index()\n",
    "    # converter coluna date para string ISO ao salvar CSV (mant√©m compatibilidade)\n",
    "    out_parquet = os.path.join(HIST_DIR, f\"{ticker}.parquet\")\n",
    "    out_csv = os.path.join(HIST_DIR, f\"{ticker}.csv\")\n",
    "    try:\n",
    "        df.to_parquet(out_parquet, index=False)\n",
    "        logging.info(\"Saved %s rows for %s -> %s\", len(df), ticker, out_parquet)\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Erro salvando parquet para %s: %s\", ticker, e)\n",
    "        raise\n",
    "    if save_csv:\n",
    "        try:\n",
    "            # padronizar data para ISO antes de salvar CSV (se existir)\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')\n",
    "            df.to_csv(out_csv, index=False)\n",
    "            logging.info(\"Saved CSV for %s -> %s\", ticker, out_csv)\n",
    "        except Exception as e:\n",
    "            logging.exception(\"Erro salvando CSV para %s: %s\", ticker, e)\n",
    "            # n√£o raise ‚Äî parquet j√° salvo, apenas logamos o problema\n",
    "\n",
    "def download_batch(batch: List[str], start: str = DEFAULT_START, threads: bool = True) -> Dict[str, Optional[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Tenta baixar um batch de tickers via yfinance.download.\n",
    "    Retorna dict ticker -> DataFrame or None (se falhou).\n",
    "    \"\"\"\n",
    "    joined = \" \".join(batch)\n",
    "    attempt = 0\n",
    "    last_exc = None\n",
    "    while attempt < MAX_ATTEMPTS:\n",
    "        try:\n",
    "            logging.info(\"yfinance.download attempt %d for batch size %d\", attempt+1, len(batch))\n",
    "            data = yf.download(tickers=joined, start=start, progress=False, threads=threads, group_by='ticker', auto_adjust=False, actions=True)\n",
    "            result = {}\n",
    "            if isinstance(data, pd.DataFrame) and isinstance(data.columns, pd.MultiIndex):\n",
    "                for ticker in batch:\n",
    "                    if ticker in data.columns.get_level_values(0):\n",
    "                        df_t = data[ticker].copy()\n",
    "                        result[ticker] = df_t\n",
    "                    else:\n",
    "                        try:\n",
    "                            single = yf.download(ticker, start=start, progress=False, actions=True)\n",
    "                            result[ticker] = single if not single.empty else None\n",
    "                        except Exception:\n",
    "                            result[ticker] = None\n",
    "            else:\n",
    "                for ticker in batch:\n",
    "                    try:\n",
    "                        df_t = yf.download(ticker, start=start, progress=False, actions=True)\n",
    "                        result[ticker] = df_t if not df_t.empty else None\n",
    "                    except Exception:\n",
    "                        result[ticker] = None\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            last_exc = e\n",
    "            logging.warning(\"Erro no yfinance.download (attempt %d): %s\", attempt+1, str(e))\n",
    "            attempt += 1\n",
    "            time.sleep(2 ** attempt)  # backoff exponencial\n",
    "    logging.error(\"Todas tentativas falharam para batch (%s). √öltimo erro: %s\", joined, last_exc)\n",
    "    # fallback: tentar baixar ticker a ticker\n",
    "    result = {}\n",
    "    for ticker in batch:\n",
    "        try:\n",
    "            df_t = yf.download(ticker, start=start, progress=False, actions=True)\n",
    "            result[ticker] = df_t if not df_t.empty else None\n",
    "        except Exception as e:\n",
    "            logging.warning(\"Fallback individual falhou para %s: %s\", ticker, e)\n",
    "            result[ticker] = None\n",
    "    return result\n",
    "\n",
    "def download_all_histories(tickers: List[str], start: str = DEFAULT_START, force: bool = False, save_summary: bool = True, save_csv_per_ticker: bool = True):\n",
    "    \"\"\"\n",
    "    Processo principal: recebe lista de tickers (strings), baixa hist√≥ricos e salva parquet + csv por ticker.\n",
    "    - force: se True, re-baixa mesmo que arquivo exista.\n",
    "    - save_csv_per_ticker: se True salva um CSV para cada ticker (al√©m do parquet).\n",
    "    - retorna um DataFrame resumo com status por ticker.\n",
    "    \"\"\"\n",
    "    os.makedirs(HIST_DIR, exist_ok=True)\n",
    "    tickers = [t for t in tickers if isinstance(t, str) and t.strip()]\n",
    "    tickers = list(dict.fromkeys(tickers))\n",
    "    summary = []\n",
    "    for i in range(0, len(tickers), BATCH_SIZE):\n",
    "        batch = tickers[i:i+BATCH_SIZE]\n",
    "        to_download = [t for t in batch if force or not ticker_exists_local(t)]\n",
    "        if not to_download:\n",
    "            logging.info(\"Batch %d: todos j√° existem localmente ‚Äî pulando.\", i//BATCH_SIZE+1)\n",
    "            for t in batch:\n",
    "                summary.append({\n",
    "                    \"ticker\": t,\n",
    "                    \"status\": \"skipped_local\",\n",
    "                    \"rows\": None,\n",
    "                    \"saved_parquet\": os.path.join(HIST_DIR, f\"{t}.parquet\") if ticker_exists_local(t) else None,\n",
    "                    \"saved_csv\": os.path.join(HIST_DIR, f\"{t}.csv\") if os.path.exists(os.path.join(HIST_DIR, f\"{t}.csv\")) else None\n",
    "                })\n",
    "            continue\n",
    "\n",
    "        logging.info(\"Processando batch %d/%d (download %d/%d)\", i//BATCH_SIZE+1, (len(tickers)+BATCH_SIZE-1)//BATCH_SIZE, len(to_download), len(batch))\n",
    "        results = download_batch(to_download, start=start)\n",
    "        for t in batch:\n",
    "            df_t = results.get(t) if t in results else None\n",
    "            if df_t is None or (isinstance(df_t, pd.DataFrame) and df_t.empty):\n",
    "                logging.warning(\"Nenhum dado para %s em batch; tentativa isolada...\", t)\n",
    "                try:\n",
    "                    single = yf.download(t, start=start, progress=False, actions=True)\n",
    "                    df_t = single if not single.empty else None\n",
    "                except Exception:\n",
    "                    df_t = None\n",
    "            if df_t is None or df_t.empty:\n",
    "                logging.error(\"Falha obtendo dados para %s\", t)\n",
    "                summary.append({\"ticker\": t, \"status\": \"failed\", \"rows\": 0, \"saved_parquet\": None, \"saved_csv\": None})\n",
    "            else:\n",
    "                try:\n",
    "                    save_history_df(t, df_t, save_csv=save_csv_per_ticker)\n",
    "                    summary.append({\n",
    "                        \"ticker\": t,\n",
    "                        \"status\": \"ok\",\n",
    "                        \"rows\": len(df_t),\n",
    "                        \"saved_parquet\": os.path.join(HIST_DIR, f\"{t}.parquet\"),\n",
    "                        \"saved_csv\": os.path.join(HIST_DIR, f\"{t}.csv\") if save_csv_per_ticker else None\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    logging.exception(\"Erro salvando para %s: %s\", t, e)\n",
    "                    summary.append({\"ticker\": t, \"status\": \"save_error\", \"rows\": len(df_t) if isinstance(df_t, pd.DataFrame) else None, \"saved_parquet\": None, \"saved_csv\": None})\n",
    "            time.sleep(SLEEP_BETWEEN_TICKERS)\n",
    "        time.sleep(SLEEP_BETWEEN_BATCHES)\n",
    "\n",
    "    df_summary = pd.DataFrame(summary)\n",
    "    if save_summary:\n",
    "        ts = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "        summary_path = os.path.join(HIST_DIR, f\"download_summary_{ts}.csv\")\n",
    "        df_summary.to_csv(summary_path, index=False)\n",
    "        logging.info(\"Resumo salvo em %s\", summary_path)\n",
    "    return df_summary\n",
    "\n",
    "def combine_all_to_single_parquet(out_path: str = os.path.join(HIST_DIR, \"all_histories.parquet\"), out_csv: Optional[str] = os.path.join(HIST_DIR, \"all_histories.csv\"), tickers: Optional[List[str]] = None):\n",
    "    \"\"\"\n",
    "    L√™ todos os parquets em HIST_DIR (ou tickers list) e concatena em formato long:\n",
    "    columns: ['ticker','date', 'Open','High','Low','Close','Adj Close','Volume', 'Dividends','Stock Splits']\n",
    "    Salva em parquet e opcionalmente em csv.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    if tickers:\n",
    "        files = [os.path.join(HIST_DIR, f\"{t}.parquet\") for t in tickers if os.path.exists(os.path.join(HIST_DIR, f\"{t}.parquet\"))]\n",
    "    else:\n",
    "        files = [os.path.join(HIST_DIR, f) for f in os.listdir(HIST_DIR) if f.endswith(\".parquet\")]\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        try:\n",
    "            df = pd.read_parquet(f)\n",
    "            if 'date' in df.columns:\n",
    "                df['date'] = pd.to_datetime(df['date'])\n",
    "            fname = os.path.basename(f).replace(\".parquet\",\"\")\n",
    "            if 'ticker' not in df.columns:\n",
    "                df.insert(0, 'ticker', fname)\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            logging.warning(\"Erro lendo %s: %s\", f, e)\n",
    "    if not dfs:\n",
    "        raise RuntimeError(\"Nenhum parquet encontrado para combinar.\")\n",
    "    big = pd.concat(dfs, ignore_index=True, sort=False)\n",
    "    big.to_parquet(out_path, index=False)\n",
    "    logging.info(\"Combined saved to %s (rows=%d)\", out_path, len(big))\n",
    "    if out_csv:\n",
    "        try:\n",
    "            # converter date para formato iso ao salvar CSV\n",
    "            if 'date' in big.columns:\n",
    "                big['date'] = pd.to_datetime(big['date']).dt.strftime('%Y-%m-%d')\n",
    "            big.to_csv(out_csv, index=False)\n",
    "            logging.info(\"Combined CSV saved to %s\", out_csv)\n",
    "        except Exception as e:\n",
    "            logging.exception(\"Erro salvando combined CSV: %s\", e)\n",
    "    return big\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) carregue a lista de tickers a partir do arquivo que voc√™ j√° salvou\n",
    "    tickers_file = os.path.join(\"tickers_ibrx100_full.csv\")\n",
    "    if os.path.exists(tickers_file):\n",
    "        df = pd.read_csv(tickers_file)\n",
    "        if 'Ticker' in df.columns:\n",
    "            tickers = df['Ticker'].dropna().astype(str).tolist()\n",
    "        else:\n",
    "            tickers = df.iloc[:,0].dropna().astype(str).tolist()\n",
    "    else:\n",
    "        raise RuntimeError(f\"N√£o encontrou {tickers_file}. Coloque seu CSV de tickers no mesmo diret√≥rio ou edite este script.\")\n",
    "\n",
    "    # 2) op√ß√£o: validar/normalizar tickers (garantir sufixo .SA)\n",
    "    def normalize(t):\n",
    "        t = str(t).strip().upper()\n",
    "        if not t.endswith(\".SA\"):\n",
    "            t = t.replace(\".SA\",\"\") + \".SA\"\n",
    "        return t\n",
    "    tickers = [normalize(t) for t in tickers]\n",
    "    print(\"Tickers a baixar:\", len(tickers), tickers[:10])\n",
    "\n",
    "    # 3) executar (force=True re-baixa mesmo se j√° existir)\n",
    "    summary_df = download_all_histories(tickers, start=DEFAULT_START, force=False, save_summary=True, save_csv_per_ticker=True)\n",
    "    print(summary_df.head(50))\n",
    "\n",
    "    # 4) opcional: combinar tudo em um √∫nico parquet e CSV (pode ser grande)\n",
    "    combined = combine_all_to_single_parquet()\n",
    "    print(\"Combined rows:\", len(combined))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe3e402",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üìã Descri√ß√£o das colunas do dataset hist√≥rico\n",
    "\n",
    "| Coluna         | Tipo        | Breve descri√ß√£o |\n",
    "|---------------:|:-----------:|:----------------|\n",
    "| `date`         | `date`      | Data da observa√ß√£o no formato ISO (`YYYY-MM-DD`). Use como √≠ndice temporal para s√©ries. |\n",
    "| `Open`         | `float`     | Pre√ßo de abertura do preg√£o (primeira transa√ß√£o considerada naquele dia). |\n",
    "| `High`         | `float`     | Pre√ßo m√°ximo registrado durante o preg√£o. |\n",
    "| `Low`          | `float`     | Pre√ßo m√≠nimo registrado durante o preg√£o. |\n",
    "| `Close`        | `float`     | Pre√ßo de fechamento (√∫ltima transa√ß√£o do dia). N√£o considera ajustes por eventos corporativos. |\n",
    "| `Adj Close`    | `float`     | Pre√ßo de fechamento **ajustado** por splits e dividendos ‚Äî use para c√°lculo de retornos total/consistentes em s√©ries hist√≥ricas. |\n",
    "| `Volume`       | `int`       | Quantidade de a√ß√µes negociadas no dia (unidades). |\n",
    "| `Dividends`    | `float`     | Valor do dividendo pago por a√ß√£o na data (se houver). Geralmente em moeda local (ex.: BRL para B3). |\n",
    "| `Stock Splits` | `float`     | Fator de desdobramento/agrupamento (ex.: `2.0` ‚Üí split 2-por-1). Zero ou `0.0` quando n√£o houve evento. |\n",
    "\n",
    "**Notas r√°pidas**\n",
    "- `Adj Close` √© a coluna recomendada para backtests e c√°lculo de retornos cont√≠nuos (corrige pre√ßos para manter consist√™ncia ap√≥s eventos corporativos).  \n",
    "- Converta `date` para `datetime` e defina como √≠ndice para opera√ß√µes de s√©rie temporal (`df['date'] = pd.to_datetime(df['date']); df.set_index('date', inplace=True)`).  \n",
    "- Verifique a unidade/moeda conforme a fonte (normalmente moeda local do mercado, ex.: BRL para B3).  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d6f6bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iniciando An√°lise do Arquivo: data/historical/all_histories.csv ---\n",
      "\n",
      "[SUCESSO] Arquivo carregado. Total de 357154 linhas e 10 colunas.\n",
      "\n",
      "======================================================================\n",
      " 1. AMOSTRA DOS DADOS (PRIMEIRAS 5 LINHAS) \n",
      "======================================================================\n",
      "     ticker       date      Open      High       Low     Close  Adj Close     Volume  Dividends  Stock Splits\n",
      "0  ABEV3.SA 2011-01-03  8.632311  8.728203  8.630313  8.690246   4.694568   576145.0        0.0           0.0\n",
      "1  ABEV3.SA 2011-01-04  8.784141  8.784141  8.630313  8.692244   4.695646   328368.0        0.0           0.0\n",
      "2  ABEV3.SA 2011-01-05  8.672266  8.718215  8.448517  8.530425   4.608232   299836.0        0.0           0.0\n",
      "3  ABEV3.SA 2011-01-06  8.560392  8.590358  8.396576  8.450515   4.565063   731319.0        0.0           0.0\n",
      "4  ABEV3.SA 2011-01-07  8.450515  8.550403  8.368607  8.416553   4.546715  1090222.0        0.0           0.0\n",
      "\n",
      "======================================================================\n",
      " 2. INFORMA√á√ïES DO DATAFRAME (TIPOS DE COLUNA E NULOS) \n",
      "======================================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 357154 entries, 0 to 357153\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count   Dtype         \n",
      "---  ------        --------------   -----         \n",
      " 0   ticker        357154 non-null  object        \n",
      " 1   date          357154 non-null  datetime64[ns]\n",
      " 2   Open          285801 non-null  float64       \n",
      " 3   High          285801 non-null  float64       \n",
      " 4   Low           285801 non-null  float64       \n",
      " 5   Close         285801 non-null  float64       \n",
      " 6   Adj Close     285801 non-null  float64       \n",
      " 7   Volume        285801 non-null  float64       \n",
      " 8   Dividends     285801 non-null  float64       \n",
      " 9   Stock Splits  285801 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(8), object(1)\n",
      "memory usage: 27.2+ MB\n",
      "\n",
      "\n",
      "======================================================================\n",
      " 3. RESUMO DE VALORES NULOS POR COLUNA \n",
      "======================================================================\n",
      "Open            71353\n",
      "High            71353\n",
      "Low             71353\n",
      "Close           71353\n",
      "Adj Close       71353\n",
      "Volume          71353\n",
      "Dividends       71353\n",
      "Stock Splits    71353\n",
      "\n",
      "======================================================================\n",
      " 4. ESTAT√çSTICAS DESCRITIVAS (COLUNAS NUM√âRICAS) \n",
      "======================================================================\n",
      "                                date           Open           High            Low          Close      Adj Close        Volume      Dividends   Stock Splits\n",
      "count                         357154  285801.000000  285801.000000  285801.000000  285801.000000  285801.000000  2.858010e+05  285801.000000  285801.000000\n",
      "mean   2018-06-03 02:03:11.634980864      20.873310      21.200087      20.530067      20.862248      17.043737  7.036957e+06       0.003349       0.001203\n",
      "min              2011-01-03 00:00:00       0.139000       0.150000       0.137000       0.138000       0.137779  0.000000e+00       0.000000       0.000000\n",
      "25%              2014-09-19 00:00:00       8.946153       9.090000       8.780000       8.935662       5.960515  1.344300e+06       0.000000       0.000000\n",
      "50%              2018-05-29 12:00:00      15.350000      15.600000      15.097384      15.350727      11.014642  3.342554e+06       0.000000       0.000000\n",
      "75%              2022-02-17 00:00:00      24.535000      24.900000      24.150000      24.527271      19.452185  7.890200e+06       0.000000       0.000000\n",
      "max              2025-10-23 00:00:00    1025.463867    1037.195312    1007.751709    1032.824707    1028.702637  6.989506e+08      17.969149      50.000000\n",
      "std                              NaN      33.840339      34.388035      33.245081      33.834710      33.198218  1.156724e+07       0.077303       0.113737\n",
      "\n",
      "======================================================================\n",
      " 5. AN√ÅLISE DA COLUNA 'TICKER' \n",
      "======================================================================\n",
      "Total de tickers √∫nicos encontrados: 97\n",
      "Amostra de tickers: ['ABEV3.SA' 'ALOS3.SA' 'ANIM3.SA' 'ASAI3.SA' 'AURE3.SA' 'AZZA3.SA'\n",
      " 'BBAS3.SA' 'BBDC3.SA' 'BBDC4.SA' 'BBSE3.SA']...\n",
      "\n",
      "======================================================================\n",
      "--- AN√ÅLISE CONCLU√çDA ---\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "\n",
    "def print_header(title):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o auxiliar para imprimir um cabe√ßalho formatado no log.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\" {title.upper()} \")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "def analyze_dataframe(file_path):\n",
    "    \"\"\"\n",
    "    Carrega e analisa um DataFrame de hist√≥rico de a√ß√µes.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"--- Iniciando An√°lise do Arquivo: {file_path} ---\")\n",
    "\n",
    "    # --- 1. Verifica√ß√£o e Carregamento dos Dados ---\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print_header(\"[ERRO] Arquivo n√£o encontrado\")\n",
    "        print(f\"O arquivo no caminho '{file_path}' n√£o foi localizado.\")\n",
    "        print(\"Por favor, verifique se o caminho est√° correto e a pasta 'historical' existe.\")\n",
    "        print(\"=\" * 70)\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Tenta carregar o CSV. \n",
    "        # A coluna 'date' √© convertida para datetime no carregamento.\n",
    "        df = pd.read_csv(file_path, parse_dates=['date'])\n",
    "        print(f\"\\n[SUCESSO] Arquivo carregado. Total de {len(df)} linhas e {len(df.columns)} colunas.\")\n",
    "    except Exception as e:\n",
    "        print_header(\"[ERRO] Falha ao carregar o arquivo\")\n",
    "        print(f\"Ocorreu um erro ao tentar ler o arquivo CSV: {e}\")\n",
    "        print(\"=\" * 70)\n",
    "        return\n",
    "\n",
    "    # --- 2. Amostra dos Dados (Head) ---\n",
    "    print_header(\"1. Amostra dos Dados (Primeiras 5 Linhas)\")\n",
    "    # .to_string() formata o DataFrame como uma tabela de texto leg√≠vel\n",
    "    print(df.head().to_string())\n",
    "\n",
    "    # --- 3. Informa√ß√µes do DataFrame (Info) ---\n",
    "    print_header(\"2. Informa√ß√µes do DataFrame (Tipos de Coluna e Nulos)\")\n",
    "    # O df.info() imprime diretamente. Para captur√°-lo e format√°-lo,\n",
    "    # usamos um buffer de string.\n",
    "    buffer = io.StringIO()\n",
    "    df.info(buf=buffer)\n",
    "    info_str = buffer.getvalue()\n",
    "    print(info_str)\n",
    "\n",
    "    # --- 4. Contagem de Valores Nulos ---\n",
    "    print_header(\"3. Resumo de Valores Nulos por Coluna\")\n",
    "    null_counts = df.isnull().sum()\n",
    "    \n",
    "    if null_counts.sum() == 0:\n",
    "        print(\"√ìtimo! N√£o h√° valores nulos em nenhuma coluna.\")\n",
    "    else:\n",
    "        # Filtra para mostrar apenas colunas que *possuem* valores nulos\n",
    "        print(null_counts[null_counts > 0].to_string())\n",
    "\n",
    "    # --- 5. Estat√≠sticas Descritivas ---\n",
    "    print_header(\"4. Estat√≠sticas Descritivas (Colunas Num√©ricas)\")\n",
    "    # O 'include='np.number' garante que s√≥ analisar√° colunas num√©ricas\n",
    "    # .to_string() formata a sa√≠da para melhor visualiza√ß√£o no log\n",
    "    try:\n",
    "        print(df.describe().to_string())\n",
    "    except Exception as e:\n",
    "        print(f\"N√£o foi poss√≠vel calcular estat√≠sticas descritivas: {e}\")\n",
    "\n",
    "    # --- 6. An√°lise de Tickers ---\n",
    "    print_header(\"5. An√°lise da Coluna 'ticker'\")\n",
    "    if 'ticker' in df.columns:\n",
    "        unique_tickers = df['ticker'].unique()\n",
    "        num_unique_tickers = len(unique_tickers)\n",
    "        print(f\"Total de tickers √∫nicos encontrados: {num_unique_tickers}\")\n",
    "        \n",
    "        # Mostra uma amostra se houver muitos tickers\n",
    "        if num_unique_tickers > 10:\n",
    "            print(f\"Amostra de tickers: {unique_tickers[:10]}...\")\n",
    "        else:\n",
    "            print(f\"Tickers presentes: {unique_tickers}\")\n",
    "    else:\n",
    "        print(\"Coluna 'ticker' n√£o encontrada.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"--- AN√ÅLISE CONCLU√çDA ---\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# --- Ponto de Execu√ß√£o Principal ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Define o caminho do arquivo\n",
    "    path_do_arquivo = 'data/historical/all_histories.csv'\n",
    "    \n",
    "    # Executa a fun√ß√£o de an√°lise\n",
    "    analyze_dataframe(path_do_arquivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25772b37",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dd69e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Limpeza conclu√≠da. 79964 linhas de 'lookahead' removidas.\n",
      "     ticker       date      Open      High       Low     Close  Adj Close  \\\n",
      "0  ABEV3.SA 2011-01-03  8.632311  8.728203  8.630313  8.690246   4.694568   \n",
      "1  ABEV3.SA 2011-01-04  8.784141  8.784141  8.630313  8.692244   4.695646   \n",
      "2  ABEV3.SA 2011-01-05  8.672266  8.718215  8.448517  8.530425   4.608232   \n",
      "3  ABEV3.SA 2011-01-06  8.560392  8.590358  8.396576  8.450515   4.565063   \n",
      "4  ABEV3.SA 2011-01-07  8.450515  8.550403  8.368607  8.416553   4.546715   \n",
      "\n",
      "      Volume  Dividends  Stock Splits  \n",
      "0   576145.0        0.0           0.0  \n",
      "1   328368.0        0.0           0.0  \n",
      "2   299836.0        0.0           0.0  \n",
      "3   731319.0        0.0           0.0  \n",
      "4  1090222.0        0.0           0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os # Garante que 'os' est√° importado\n",
    "\n",
    "# Configura√ß√£o de logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 1. Carregar os Dados ---\n",
    "try:\n",
    "    # Tenta carregar o Parquet primeiro, √© mais r√°pido\n",
    "    df_prices_raw = pd.read_parquet(\"data/historical/all_histories.parquet\")\n",
    "    logger.info(f\"Dados de pre√ßo brutos carregados do PARQUET: {df_prices_raw.shape[0]} linhas\")\n",
    "except FileNotFoundError:\n",
    "    logger.warning(\"Arquivo all_histories.parquet n√£o encontrado! Tentando carregar o CSV...\")\n",
    "    try:\n",
    "        # Carrega o CSV como fallback\n",
    "        df_prices_raw = pd.read_csv(\"data/historical/all_histories.csv\", parse_dates=['date'])\n",
    "        logger.info(f\"Dados de pre√ßo brutos carregados do CSV: {df_prices_raw.shape[0]} linhas\")\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(\"Nenhum arquivo de hist√≥rico (parquet ou csv) encontrado. Execute o download primeiro.\")\n",
    "        raise e\n",
    "except Exception as e:\n",
    "    logger.error(f\"Erro ao carregar dados: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- 2. Limpeza Cr√≠tica (Remover NaN de pr√©-IPO) ---\n",
    "# Remove todas as linhas onde a a√ß√£o ainda n√£o existia (Adj Close ou Volume s√£o NaN)\n",
    "df_prices_clean = df_prices_raw.dropna(subset=['Adj Close', 'Volume'])\n",
    "\n",
    "# --- 3. Limpeza Opcional (Remover dias sem negocia√ß√£o) ---\n",
    "df_prices_clean = df_prices_clean[df_prices_clean['Volume'] > 0]\n",
    "\n",
    "linhas_removidas = len(df_prices_raw) - len(df_prices_clean)\n",
    "logger.info(f\"Dados de pre√ßo limpos: {df_prices_clean.shape[0]} linhas (removidas {linhas_removidas} linhas com NaN ou Volume 0)\")\n",
    "\n",
    "# --- 4. Salvar os Arquivos Limpos ---\n",
    "# Garante que o diret√≥rio existe\n",
    "output_dir = \"data/historical\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "parquet_path = os.path.join(output_dir, \"all_histories_cleaned.parquet\")\n",
    "csv_path = os.path.join(output_dir, \"all_histories_cleaned.csv\")\n",
    "\n",
    "try:\n",
    "    # Salvar em Parquet (preferencial para o pr√≥ximo passo)\n",
    "    df_prices_clean.to_parquet(parquet_path, index=False)\n",
    "    logger.info(f\"‚úÖ Arquivo limpo salvo em (Parquet): {parquet_path}\")\n",
    "\n",
    "    # Salvar em CSV (para sua verifica√ß√£o)\n",
    "    # date_format garante que a data seja salva em formato leg√≠vel\n",
    "    df_prices_clean.to_csv(csv_path, index=False, date_format='%Y-%m-%d')\n",
    "    logger.info(f\"‚úÖ Arquivo limpo salvo em (CSV): {csv_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Erro ao salvar arquivos limpos: {e}\")\n",
    "\n",
    "print(f\"\\nLimpeza conclu√≠da. {linhas_removidas} linhas de 'lookahead' removidas.\")\n",
    "print(df_prices_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b29e687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Amostra de Fechamento (Wide) ---\n",
      "ticker      ABEV3.SA   ALOS3.SA  ANIM3.SA  ASAI3.SA  AURE3.SA   AZZA3.SA  \\\n",
      "date                                                                       \n",
      "2025-08-31     12.35  23.764782      3.37     10.52     10.86  34.709999   \n",
      "2025-09-01     12.35  23.764782      3.37     10.52     10.86  34.709999   \n",
      "2025-09-30     12.09  25.721401      3.43      9.51     10.27  30.150000   \n",
      "2025-10-01     12.09  25.721401      3.43      9.51     10.27  30.150000   \n",
      "2025-10-31     12.12  24.480000      3.22      8.15     11.70  27.540001   \n",
      "\n",
      "ticker       BBAS3.SA   BBDC3.SA   BBDC4.SA   BBSE3.SA  ...   TOTS3.SA  \\\n",
      "date                                                    ...              \n",
      "2025-08-31  21.389999  14.151272  16.512844  32.820000  ...  42.957096   \n",
      "2025-09-01  21.389999  14.151272  16.512844  32.820000  ...  42.957096   \n",
      "2025-09-30  22.090000  15.212403  17.670698  33.259998  ...  45.930000   \n",
      "2025-10-01  22.090000  15.212403  17.670698  33.259998  ...  45.930000   \n",
      "2025-10-31  20.670000  15.320000  17.950001  31.950001  ...  41.939999   \n",
      "\n",
      "ticker       UGPA3.SA  USIM5.SA   VALE3.SA  VAMO3.SA   VBBR3.SA   VIVA3.SA  \\\n",
      "date                                                                         \n",
      "2025-08-31  19.660000      4.37  55.560001      4.33  24.040001  29.000000   \n",
      "2025-09-01  19.660000      4.37  55.560001      4.33  24.040001  29.000000   \n",
      "2025-09-30  21.969999      4.23  57.580002      3.45  24.590000  28.559999   \n",
      "2025-10-01  21.969999      4.23  57.580002      3.45  24.590000  28.559999   \n",
      "2025-10-31  20.840000      4.99  61.680000      3.27  23.879999  30.090000   \n",
      "\n",
      "ticker       VIVT3.SA   WEGE3.SA  YDUQ3.SA  \n",
      "date                                        \n",
      "2025-08-31  33.743816  37.545696     13.15  \n",
      "2025-09-01  33.743816  37.545696     13.15  \n",
      "2025-09-30  34.070000  36.590000     12.92  \n",
      "2025-10-01  34.070000  36.590000     12.92  \n",
      "2025-10-31  33.480000  41.419998     13.08  \n",
      "\n",
      "[5 rows x 97 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# --- Configura√ß√£o ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "INPUT_FILE = \"data/historical/all_histories_cleaned.parquet\"\n",
    "OUTPUT_DIR = \"data/historical\"\n",
    "\n",
    "def gerar_precos_pivotados():\n",
    "    logger.info(\"Iniciando o Passo 1: Gera√ß√£o dos Pre√ßos Pivotados (Wide)...\")\n",
    "    \n",
    "    # 1. Carregar os dados de pre√ßo limpos\n",
    "    try:\n",
    "        df_prices_clean = pd.read_parquet(INPUT_FILE)\n",
    "        logger.info(f\"Dados limpos '{INPUT_FILE}' carregados.\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"ARQUIVO N√ÉO ENCONTRADO: {INPUT_FILE}\")\n",
    "        logger.error(\"Execute o 'Script 1: Limpeza' primeiro.\")\n",
    "        return\n",
    "\n",
    "    df_prices_clean['date'] = pd.to_datetime(df_prices_clean['date'])\n",
    "\n",
    "    # 2. Criar a base de pre√ßos mensais\n",
    "    logger.info(\"Reamostrando para Frequ√™ncia Mensal (M e MS)...\")\n",
    "    # 2a. Fechamento (√öltimo dia do m√™s 'M')\n",
    "    df_prices_mensal_raw = df_prices_clean.set_index('date').groupby('ticker').resample('M').last()\n",
    "\n",
    "    # 2b. Abertura (Primeiro dia do m√™s 'MS')\n",
    "    df_open_mensal_raw = df_prices_clean.set_index('date').groupby('ticker').resample('MS').first()\n",
    "\n",
    "    # 3. Pivotar para o formato \"wide\"\n",
    "    logger.info(\"Corrigindo MultiIndex e Pivotando...\")\n",
    "    \n",
    "    # CORRE√á√ÉO: Dropar a coluna 'ticker' duplicada antes de resetar o √≠ndice\n",
    "    df_prices_mensal_long = df_prices_mensal_raw.drop(columns='ticker', errors='ignore').reset_index()\n",
    "    df_open_mensal_long = df_open_mensal_raw.drop(columns='ticker', errors='ignore').reset_index()\n",
    "\n",
    "    # Agora o .pivot() funcionar√°\n",
    "    df_close_wide = df_prices_mensal_long.pivot(index='date', columns='ticker', values='Adj Close')\n",
    "    df_open_wide = df_open_mensal_long.pivot(index='date', columns='ticker', values='Open')\n",
    "\n",
    "    # 4. Sincronizar os √≠ndices de data\n",
    "    logger.info(\"Sincronizando e preenchendo √≠ndices de data...\")\n",
    "    idx_union = df_close_wide.index.union(df_open_wide.index)\n",
    "    \n",
    "    df_close_wide = df_close_wide.reindex(idx_union, method='ffill')\n",
    "    df_open_wide = df_open_wide.reindex(idx_union, method='ffill')\n",
    "\n",
    "    # Preenche NaNs (pr√©-IPO e p√≥s-delist)\n",
    "    df_close_wide = df_close_wide.ffill().bfill()\n",
    "    df_open_wide = df_open_wide.ffill().bfill()\n",
    "\n",
    "    # --- 5. SALVAR OS ARQUIVOS FALTOSOS ---\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    path_close = os.path.join(OUTPUT_DIR, \"prices_close_wide.parquet\")\n",
    "    path_open = os.path.join(OUTPUT_DIR, \"prices_open_wide.parquet\")\n",
    "    \n",
    "    df_close_wide.to_parquet(path_close)\n",
    "    df_open_wide.to_parquet(path_open)\n",
    "    \n",
    "    logger.info(f\"‚úÖ ARQUIVO FALTOSO GERADO: {path_close}\")\n",
    "    logger.info(f\"‚úÖ ARQUIVO FALTOSO GERADO: {path_open}\")\n",
    "    print(\"\\n--- Amostra de Fechamento (Wide) ---\")\n",
    "    print(df_close_wide.tail())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gerar_precos_pivotados()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35a28fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:ERRO CR√çTICO: As colunas de fundamento chave ['ROIC'] n√£o foram encontradas ap√≥s o merge!\n",
      "ERROR:__main__:Verifique os nomes das colunas no arquivo de fundamentos e se o merge_asof funcionou (veja logs acima).\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72b52076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ticker       date  VOLATILIDADE\n",
      "13681  YDUQ3.SA 2025-06-30      0.032834\n",
      "13682  YDUQ3.SA 2025-07-31      0.029565\n",
      "13683  YDUQ3.SA 2025-08-31      0.026134\n",
      "13684  YDUQ3.SA 2025-09-30      0.027915\n",
      "13685  YDUQ3.SA 2025-10-31      0.025774\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Configura√ß√£o de logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Iniciando Passo 2: C√°lculo da Volatilidade...\")\n",
    "\n",
    "# Carregar os dados de pre√ßo DI√ÅRIOS (limpos)\n",
    "df_prices_daily = pd.read_parquet(\"data/historical/all_histories_cleaned.parquet\")\n",
    "df_prices_daily['date'] = pd.to_datetime(df_prices_daily['date'])\n",
    "\n",
    "# 1. Calcular retornos di√°rios\n",
    "df_prices_daily['returns'] = df_prices_daily.groupby('ticker')['Adj Close'].pct_change()\n",
    "\n",
    "# 2. Calcular a volatilidade m√≥vel de 63 dias (~3 meses de negocia√ß√£o)\n",
    "# .std() calcula o desvio padr√£o (volatilidade)\n",
    "# reset_index(0, drop=True) √© necess√°rio ap√≥s o .rolling() em um groupby\n",
    "df_prices_daily['VOLATILIDADE'] = df_prices_daily.groupby('ticker')['returns'].rolling(window=63).std().reset_index(0, drop=True)\n",
    "\n",
    "# 3. Resample da volatilidade para MENSAL (pegamos o √∫ltimo valor do m√™s)\n",
    "df_vol_mensal = df_prices_daily.set_index('date').groupby('ticker').resample('M').last()['VOLATILIDADE'].reset_index()\n",
    "\n",
    "logger.info(f\"‚úÖ Volatilidade mensal calculada. {len(df_vol_mensal)} registros.\")\n",
    "print(df_vol_mensal.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4c3fdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ticker       date  SENTIMENT_MEDIO  SENTIMENT_STD  NEWS_COUNT\n",
      "183  VIVT3 2025-09-30         0.333333       0.577350           3\n",
      "184  VIVT3 2025-10-31         0.000000       0.000000          19\n",
      "185  WEGE3 2025-09-30         0.085714       0.373491          35\n",
      "186  WEGE3 2025-10-31         0.127660       0.396562          47\n",
      "187  YDUQ3 2025-09-30         0.142857       0.377964           7\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Iniciando Passo 3: Agrega√ß√£o de Sentimento...\")\n",
    "\n",
    "# Carregar seu arquivo de not√≠cias com sentimento\n",
    "# (Vou usar o nome do seu screenshot)\n",
    "df_sent_raw = pd.read_parquet(\"data/news/news_with_sentiment.parquet\")\n",
    "\n",
    "# 1. Renomear colunas para padroniza√ß√£o\n",
    "df_sent_raw = df_sent_raw.rename(columns={\n",
    "    'ticker_query': 'ticker',\n",
    "    'published_date': 'date'\n",
    "})\n",
    "df_sent_raw['date'] = pd.to_datetime(df_sent_raw['date'], utc=True)\n",
    "df_sent_raw['date'] = df_sent_raw['date'].dt.tz_localize(None) # Remover timezone para o merge\n",
    "\n",
    "# 2. Garantir que o ticker tem o sufixo .SA (se necess√°rio)\n",
    "# Se seu ticker_query for \"PETR4\", esta linha ajusta para \"PETR4.SA\"\n",
    "if not df_sent_raw['ticker'].str.contains('.SA').any():\n",
    "    df_sent_raw['ticker'] = df_sent_raw['ticker'].apply(lambda x: f\"{x}.SA\" if not x.endswith(\".SA\") else x)\n",
    "\n",
    "# 3. Agregar por ticker e M√™s\n",
    "df_sent_mensal = df_sent_raw.set_index('date').groupby('ticker').resample('M').agg(\n",
    "    SENTIMENT_MEDIO=('numeric_sentiment', 'mean'), # Coluna que voc√™ mencionou\n",
    "    SENTIMENT_STD=('numeric_sentiment', 'std'),\n",
    "    NEWS_COUNT=('ticker', 'count')\n",
    ").reset_index()\n",
    "\n",
    "logger.info(f\"‚úÖ Sentimento mensal agregado. {len(df_sent_mensal)} registros.\")\n",
    "print(df_sent_mensal.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2f38e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9c6a845",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/aurum_scores_output/aurum_quality_scores_complete.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# --- 2. Carregar Fundamentos (Trimestrais) ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m df_fund = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/aurum_scores_output/aurum_quality_scores_complete.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m df_fund = df_fund.rename(columns={\u001b[33m'\u001b[39m\u001b[33mDT_FIM_EXERC\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m     24\u001b[39m df_fund[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(df_fund[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kaike\\projeto_aurum\\venv_aurum\\Lib\\site-packages\\pandas\\io\\parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kaike\\projeto_aurum\\venv_aurum\\Lib\\site-packages\\pandas\\io\\parquet.py:258\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    257\u001b[39m     to_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33msplit_blocks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    265\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    266\u001b[39m         path_or_handle,\n\u001b[32m    267\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m         **kwargs,\n\u001b[32m    271\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kaike\\projeto_aurum\\venv_aurum\\Lib\\site-packages\\pandas\\io\\parquet.py:141\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    131\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    134\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kaike\\projeto_aurum\\venv_aurum\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/aurum_scores_output/aurum_quality_scores_complete.parquet'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# (Execute os Passos 2 e 3 acima para ter 'df_vol_mensal' e 'df_sent_mensal')\n",
    "\n",
    "logger.info(\"Iniciando Passo 4: Unifica√ß√£o do DataFrame Mestre...\")\n",
    "\n",
    "# --- 1. Carregar a Base de Pre√ßos Mensal ---\n",
    "# Vamos usar o df_close_wide que voc√™ j√° criou e \"des-pivotar\" (melt)\n",
    "try:\n",
    "    df_close_wide = pd.read_parquet(\"data/historical/prices_close_wide.parquet\")\n",
    "    df_base_mensal = df_close_wide.melt(ignore_index=False, var_name='ticker', value_name='Adj Close').reset_index()\n",
    "    df_base_mensal = df_base_mensal.rename(columns={'index': 'date'})\n",
    "    logger.info(f\"Base de pre√ßos mensal carregada: {len(df_base_mensal)} registros (tickers * meses)\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(\"Execute o script de Pivot (Script 2) primeiro!\")\n",
    "    raise\n",
    "\n",
    "# --- 2. Carregar Fundamentos (Trimestrais) ---\n",
    "df_fund = pd.read_parquet(\"data/aurum_scores_output/aurum_quality_scores_complete.parquet\")\n",
    "df_fund = df_fund.rename(columns={'DT_FIM_EXERC': 'date'})\n",
    "df_fund['date'] = pd.to_datetime(df_fund['date'])\n",
    "\n",
    "# --- 3. Carregar o Mapeamento (DE-PARA) ---\n",
    "try:\n",
    "    df_mapping = pd.read_csv(\"ticker_cnpj_map.csv\") # O arquivo que voc√™ criou no Passo 1\n",
    "    # Limpar CNPJs (remover pontua√ß√£o) se necess√°rio\n",
    "    # df_mapping['CNPJ_CIA'] = df_mapping['CNPJ_CIA'].str.replace(r'[^\\d]', '', regex=True)\n",
    "except FileNotFoundError:\n",
    "    logger.error(\"Arquivo 'ticker_cnpj_map.csv' n√£o encontrado! Crie-o antes de continuar.\")\n",
    "    raise\n",
    "\n",
    "# Juntar o ticker aos dados de fundamento\n",
    "df_fund_com_ticker = pd.merge(df_fund, df_mapping, on='CNPJ_CIA', how='left')\n",
    "df_fund_com_ticker = df_fund_com_ticker.dropna(subset=['ticker'])\n",
    "logger.info(\"Fundamentos mapeados para tickers.\")\n",
    "\n",
    "# --- 4. Construir o DataFrame Mestre ---\n",
    "# Ordenar tudo por data √© crucial para o merge_asof\n",
    "df_master = df_base_mensal.sort_values(by='date')\n",
    "df_fund_com_ticker = df_fund_com_ticker.sort_values(by='date')\n",
    "\n",
    "# 4a. Juntar Fundamentos (Trimestrais)\n",
    "# \"Para cada m√™s na minha base, pegue o √∫ltimo balan√ßo dispon√≠vel\"\n",
    "df_master = pd.merge_asof(\n",
    "    df_master,\n",
    "    df_fund_com_ticker,\n",
    "    on='date',\n",
    "    by='ticker',\n",
    "    direction='backward' # 'backward' = olhar para tr√°s\n",
    ")\n",
    "logger.info(\"Merge 'as-of' dos fundamentos conclu√≠do.\")\n",
    "\n",
    "# 4b. Juntar Volatilidade (Mensal)\n",
    "df_master = pd.merge(\n",
    "    df_master,\n",
    "    df_vol_mensal,\n",
    "    on=['date', 'ticker'],\n",
    "    how='left'\n",
    ")\n",
    "logger.info(\"Merge da volatilidade conclu√≠do.\")\n",
    "\n",
    "# 4c. Juntar Sentimento (Mensal)\n",
    "df_master = pd.merge(\n",
    "    df_master,\n",
    "    df_sent_mensal,\n",
    "    on=['date', 'ticker'],\n",
    "    how='left'\n",
    ")\n",
    "logger.info(\"Merge do sentimento conclu√≠do.\")\n",
    "\n",
    "# --- 5. Limpeza Final ---\n",
    "# Preencher NaNs de sentimento (meses sem not√≠cias) com Neutro (0)\n",
    "df_master['SENTIMENT_MEDIO'] = df_master['SENTIMENT_MEDIO'].fillna(0)\n",
    "\n",
    "# **MUITO IMPORTANTE**: Remover linhas onde os fundamentos ainda n√£o existiam\n",
    "# (ex: antes do IPO da a√ß√£o ou antes dos dados de 2011)\n",
    "# Se 'ROE' √© nulo, significa que o merge_asof n√£o encontrou fundamentos para aquele m√™s.\n",
    "df_master = df_master.dropna(subset=['ROE', 'ROIC']) # Use as colunas chave do seu score\n",
    "\n",
    "logger.info(f\"Limpeza final conclu√≠da. DataFrame Mestre pronto com {len(df_master)} linhas.\")\n",
    "\n",
    "# --- 6. Salvar o Novo DataFrame Mestre ---\n",
    "output_path = \"data/aurum_master_features.parquet\"\n",
    "df_master.to_parquet(output_path, index=False)\n",
    "logger.info(f\"‚úÖ‚úÖ‚úÖ DataFrame Mestre salvo em: {output_path} ‚úÖ‚úÖ‚úÖ\")\n",
    "print(df_master.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a97624b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaike\\AppData\\Local\\Temp\\ipykernel_1924\\2068620262.py:23: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  df_prices_mensal_raw = df_prices_clean.set_index('date').groupby('ticker').resample('M').last()\n",
      "C:\\Users\\kaike\\AppData\\Local\\Temp\\ipykernel_1924\\2068620262.py:23: FutureWarning: DataFrameGroupBy.resample operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_prices_mensal_raw = df_prices_clean.set_index('date').groupby('ticker').resample('M').last()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame de Fechamento (Wide) - Amostra:\n",
      "ticker      ABEV3.SA   ALOS3.SA  ANIM3.SA  ASAI3.SA  AURE3.SA   AZZA3.SA  \\\n",
      "date                                                                       \n",
      "2025-08-31     12.35  23.764782      3.37     10.52     10.86  34.709999   \n",
      "2025-09-01     12.35  23.764782      3.37     10.52     10.86  34.709999   \n",
      "2025-09-30     12.09  25.721401      3.43      9.51     10.27  30.150000   \n",
      "2025-10-01     12.09  25.721401      3.43      9.51     10.27  30.150000   \n",
      "2025-10-31     12.12  24.480000      3.22      8.15     11.70  27.540001   \n",
      "\n",
      "ticker       BBAS3.SA   BBDC3.SA   BBDC4.SA   BBSE3.SA  ...   TOTS3.SA  \\\n",
      "date                                                    ...              \n",
      "2025-08-31  21.389999  14.151272  16.512844  32.820000  ...  42.957096   \n",
      "2025-09-01  21.389999  14.151272  16.512844  32.820000  ...  42.957096   \n",
      "2025-09-30  22.090000  15.212403  17.670698  33.259998  ...  45.930000   \n",
      "2025-10-01  22.090000  15.212403  17.670698  33.259998  ...  45.930000   \n",
      "2025-10-31  20.670000  15.320000  17.950001  31.950001  ...  41.939999   \n",
      "\n",
      "ticker       UGPA3.SA  USIM5.SA   VALE3.SA  VAMO3.SA   VBBR3.SA   VIVA3.SA  \\\n",
      "date                                                                         \n",
      "2025-08-31  19.660000      4.37  55.560001      4.33  24.040001  29.000000   \n",
      "2025-09-01  19.660000      4.37  55.560001      4.33  24.040001  29.000000   \n",
      "2025-09-30  21.969999      4.23  57.580002      3.45  24.590000  28.559999   \n",
      "2025-10-01  21.969999      4.23  57.580002      3.45  24.590000  28.559999   \n",
      "2025-10-31  20.840000      4.99  61.680000      3.27  23.879999  30.090000   \n",
      "\n",
      "ticker       VIVT3.SA   WEGE3.SA  YDUQ3.SA  \n",
      "date                                        \n",
      "2025-08-31  33.743816  37.545696     13.15  \n",
      "2025-09-01  33.743816  37.545696     13.15  \n",
      "2025-09-30  34.070000  36.590000     12.92  \n",
      "2025-10-01  34.070000  36.590000     12.92  \n",
      "2025-10-31  33.480000  41.419998     13.08  \n",
      "\n",
      "[5 rows x 97 columns]\n",
      "\n",
      "DataFrame de Abertura (Wide) - Amostra:\n",
      "ticker      ABEV3.SA   ALOS3.SA  ANIM3.SA  ASAI3.SA  AURE3.SA   AZZA3.SA  \\\n",
      "date                                                                       \n",
      "2025-08-31     12.55  21.660000      3.73      9.54      9.36  36.380001   \n",
      "2025-09-01     12.38  23.879999      3.39     10.73     10.81  34.799999   \n",
      "2025-09-30     12.38  23.879999      3.39     10.73     10.81  34.799999   \n",
      "2025-10-01     12.19  25.959999      3.47      9.51     10.23  30.309999   \n",
      "2025-10-31     12.19  25.959999      3.47      9.51     10.23  30.309999   \n",
      "\n",
      "ticker      BBAS3.SA  BBDC3.SA   BBDC4.SA   BBSE3.SA  ...   TOTS3.SA  \\\n",
      "date                                                  ...              \n",
      "2025-08-31      19.9     13.53  15.710000  33.939999  ...  43.990002   \n",
      "2025-09-01      21.4     14.41  16.840000  32.939999  ...  43.080002   \n",
      "2025-09-30      21.4     14.41  16.840000  32.939999  ...  43.080002   \n",
      "2025-10-01      22.1     15.30  17.799999  33.330002  ...  46.009998   \n",
      "2025-10-31      22.1     15.30  17.799999  33.330002  ...  46.009998   \n",
      "\n",
      "ticker       UGPA3.SA  USIM5.SA   VALE3.SA  VAMO3.SA   VBBR3.SA   VIVA3.SA  \\\n",
      "date                                                                         \n",
      "2025-08-31  17.410000      4.45  54.240002      3.92  21.650000  25.580000   \n",
      "2025-09-01  19.540001      4.35  55.139999      4.34  24.000000  29.209999   \n",
      "2025-09-30  19.540001      4.35  55.139999      4.34  24.000000  29.209999   \n",
      "2025-10-01  22.049999      4.25  57.759998      3.48  24.709999  28.700001   \n",
      "2025-10-31  22.049999      4.25  57.759998      3.48  24.709999  28.700001   \n",
      "\n",
      "ticker       VIVT3.SA   WEGE3.SA  YDUQ3.SA  \n",
      "date                                        \n",
      "2025-08-31  31.639999  37.270000     13.20  \n",
      "2025-09-01  34.000000  37.930000     13.18  \n",
      "2025-09-30  34.000000  37.930000     13.18  \n",
      "2025-10-01  34.099998  36.290001     13.00  \n",
      "2025-10-31  34.099998  36.290001     13.00  \n",
      "\n",
      "[5 rows x 97 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaike\\AppData\\Local\\Temp\\ipykernel_1924\\2068620262.py:26: FutureWarning: DataFrameGroupBy.resample operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_open_mensal_raw = df_prices_clean.set_index('date').groupby('ticker').resample('MS').first()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import os # Garante que 'os' est√° importado\n",
    "\n",
    "# Configura√ß√£o de logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 1. Carregar os dados de pre√ßo limpos\n",
    "try:\n",
    "    df_prices_clean = pd.read_parquet(\"data/historical/all_histories_cleaned.parquet\")\n",
    "    logger.info(\"Dados limpos 'all_histories_cleaned.parquet' carregados.\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(\"Arquivo limpo n√£o encontrado. Execute o Script 1 (Limpeza) primeiro.\")\n",
    "    # Parar a execu√ß√£o se o arquivo n√£o existir\n",
    "    raise\n",
    "\n",
    "df_prices_clean['date'] = pd.to_datetime(df_prices_clean['date'])\n",
    "\n",
    "# 2. Criar a base de pre√ßos mensais\n",
    "# 2a. Fechamento (Usamos .last() no fim do m√™s 'M')\n",
    "df_prices_mensal_raw = df_prices_clean.set_index('date').groupby('ticker').resample('M').last()\n",
    "\n",
    "# 2b. Abertura (Usamos .first() no in√≠cio do m√™s 'MS')\n",
    "df_open_mensal_raw = df_prices_clean.set_index('date').groupby('ticker').resample('MS').first()\n",
    "\n",
    "\n",
    "# 3. Pivotar para o formato \"wide\"\n",
    "\n",
    "# <<< A CORRE√á√ÉO EST√Å AQUI >>>\n",
    "# O .last() e .first() mantiveram a coluna 'ticker' original,\n",
    "# mas 'ticker' tamb√©m est√° no MultiIndex. Isso causa um conflito.\n",
    "# Vamos dropar a coluna 'ticker' redundante ANTES de resetar o √≠ndice.\n",
    "# 'errors=\"ignore\"' evita erro caso a coluna n√£o exista.\n",
    "\n",
    "df_prices_mensal_long = df_prices_mensal_raw.drop(columns='ticker', errors='ignore').reset_index()\n",
    "df_open_mensal_long = df_open_mensal_raw.drop(columns='ticker', errors='ignore').reset_index()\n",
    "\n",
    "\n",
    "# Agora o .pivot() funcionar√°\n",
    "logger.info(\"Pivotando dados de Fechamento...\")\n",
    "df_close_wide = df_prices_mensal_long.pivot(index='date', columns='ticker', values='Adj Close')\n",
    "\n",
    "logger.info(\"Pivotando dados de Abertura...\")\n",
    "df_open_wide = df_open_mensal_long.pivot(index='date', columns='ticker', values='Open')\n",
    "\n",
    "\n",
    "# 4. Sincronizar os √≠ndices de data (MUITO IMPORTANTE)\n",
    "# Garante que ambos os DataFrames (open e close) cubram o mesmo per√≠odo\n",
    "logger.info(\"Sincronizando √≠ndices de data...\")\n",
    "idx_union = df_close_wide.index.union(df_open_wide.index)\n",
    "\n",
    "# .reindex() com 'ffill' (forward-fill) preenche o pre√ßo de fechamento\n",
    "# nos dias de in√≠cio de m√™s (MS) que n√£o existiam no df_close_wide (M)\n",
    "df_close_wide = df_close_wide.reindex(idx_union, method='ffill')\n",
    "df_open_wide = df_open_wide.reindex(idx_union, method='ffill')\n",
    "\n",
    "# Preenche os NaNs iniciais (antes do IPO) que podem ter sido reintroduzidos\n",
    "# E tamb√©m os finais, caso uma a√ß√£o seja deslistada (usa bfill)\n",
    "df_close_wide = df_close_wide.ffill().bfill()\n",
    "df_open_wide = df_open_wide.ffill().bfill()\n",
    "\n",
    "logger.info(\"‚úÖ DataFrames de Pre√ßo (Wide) prontos para o backtest.\")\n",
    "\n",
    "print(\"\\nDataFrame de Fechamento (Wide) - Amostra:\")\n",
    "print(df_close_wide.tail())\n",
    "\n",
    "print(\"\\nDataFrame de Abertura (Wide) - Amostra:\")\n",
    "print(df_open_wide.tail())\n",
    "\n",
    "# --- 5. SALVAR OS ARQUIVOS WIDE ---\n",
    "# Salvar os dataframes pivotados para uso f√°cil no pr√≥ximo script\n",
    "output_dir = \"data/historical\"\n",
    "os.makedirs(output_dir, exist_ok=True) # Garante que o diret√≥rio existe\n",
    "\n",
    "df_close_wide.to_parquet(os.path.join(output_dir, \"prices_close_wide.parquet\"))\n",
    "df_open_wide.to_parquet(os.path.join(output_dir, \"prices_open_wide.parquet\"))\n",
    "\n",
    "logger.info(f\"üíæ DataFrames de pre√ßo pivotados salvos em {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe830a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb8a3447",
   "metadata": {},
   "source": [
    "### üìä Documenta√ß√£o dos DataFrames da CVM (DFP / ITR)\n",
    "\n",
    "Este documento descreve os **DataFrames gerados a partir dos arquivos p√∫blicos da CVM** ‚Äî especificamente, os conjuntos **BPP**, **BPA** e **DRE**, extra√≠dos dos demonstrativos financeiros enviados pelas companhias abertas brasileiras.\n",
    "\n",
    "Cada DataFrame representa **uma parte diferente das demonstra√ß√µes cont√°beis** disponibilizadas pela CVM (Comiss√£o de Valores Mobili√°rios), e √© resultado do processamento dos arquivos `.csv` baixados e descompactados pelo script `cvm_downloader_clean.py`.\n",
    "\n",
    "---\n",
    "\n",
    "#### üß± Estrutura e Fun√ß√£o de Cada DataFrame\n",
    "\n",
    "### **1. `raw_bpp` ‚Äî Balan√ßo Patrimonial Passivo**\n",
    "O DataFrame `raw_bpp` cont√©m as contas e valores que comp√µem o **lado do passivo** no balan√ßo patrimonial das companhias abertas.  \n",
    "Ele representa as **obriga√ß√µes, patrim√¥nio l√≠quido e contas de financiamento** de curto e longo prazo.\n",
    "\n",
    "**Fun√ß√£o:** permite analisar a **estrutura de capital e endividamento** das empresas.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. `raw_bpa` ‚Äî Balan√ßo Patrimonial Ativo**\n",
    "O DataFrame `raw_bpa` cont√©m as contas e valores que comp√µem o **lado do ativo** do balan√ßo patrimonial.  \n",
    "Inclui **ativos circulantes e n√£o circulantes**, como caixa, contas a receber, estoques e imobilizado.\n",
    "\n",
    "**Fun√ß√£o:** permite estudar a **composi√ß√£o dos ativos e a liquidez** das companhias.\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. `raw_dre` ‚Äî Demonstra√ß√£o do Resultado do Exerc√≠cio**\n",
    "O DataFrame `raw_dre` representa a **Demonstra√ß√£o do Resultado do Exerc√≠cio**, mostrando as receitas, custos, despesas e o lucro ou preju√≠zo de um per√≠odo.\n",
    "\n",
    "**Fun√ß√£o:** possibilita a **an√°lise de desempenho e rentabilidade** da empresa.\n",
    "\n",
    "---\n",
    "\n",
    "### üßæ Dicion√°rio de Colunas (comum aos tr√™s DataFrames)\n",
    "\n",
    "| Coluna | Tipo | Descri√ß√£o |\n",
    "|:-------|:-----|:-----------|\n",
    "| **CNPJ_CIA** | string | CNPJ da companhia aberta que enviou o demonstrativo. Identificador √∫nico da empresa. |\n",
    "| **DT_REFER** | date | Data de refer√™ncia do relat√≥rio entregue √† CVM (geralmente o √∫ltimo dia do per√≠odo reportado). |\n",
    "| **VERSAO** | int | Vers√£o do documento entregue √† CVM (1 = primeira entrega, >1 = reenvio/corre√ß√£o). |\n",
    "| **DENOM_CIA** | string | Nome completo da companhia conforme cadastro na CVM. |\n",
    "| **CD_CVM** | string/int | C√≥digo num√©rico da companhia na base da CVM (identificador √∫nico). |\n",
    "| **GRUPO_DFP** | string | Indica o grupo de demonstrativo (ex.: DFP = Demonstra√ß√µes Financeiras Padronizadas). |\n",
    "| **MOEDA** | string | Moeda de apresenta√ß√£o dos valores (ex.: 'REAL', 'USD'). |\n",
    "| **ESCALA_MOEDA** | string | Escala dos valores informados (ex.: 'UNIDADE', 'MIL', 'MILH√ÉO'). |\n",
    "| **ORDEM_EXERC** | string | Indica se o exerc√≠cio √© '√öLTIMO' (ano mais recente) ou 'PEN√öLTIMO' (ano anterior). |\n",
    "| **DT_FIM_EXERC** | date | Data de encerramento do exerc√≠cio financeiro (geralmente 31/12/AAAA). |\n",
    "| **CD_CONTA** | string | C√≥digo padronizado da conta cont√°bil conforme o plano da CVM. |\n",
    "| **DS_CONTA** | string | Descri√ß√£o textual da conta cont√°bil (ex.: ‚ÄúAtivo Circulante‚Äù, ‚ÄúLucro L√≠quido do Exerc√≠cio‚Äù). |\n",
    "| **VL_CONTA** | float | Valor num√©rico registrado na conta cont√°bil (em fun√ß√£o da escala da moeda). |\n",
    "| **ST_CONTA_FIXA** | string | Indicador se a conta √© fixa (‚ÄòS‚Äô) ou vari√°vel (‚ÄòN‚Äô) no plano cont√°bil. |\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Colunas espec√≠ficas por DataFrame\n",
    "\n",
    "#### üîπ `raw_bpp` (Passivo)\n",
    "Mesmas colunas da estrutura comum.  \n",
    "O foco est√° nas contas de **passivos** e **patrim√¥nio l√≠quido**, como:\n",
    "- Fornecedores  \n",
    "- Empr√©stimos e financiamentos  \n",
    "- Obriga√ß√µes fiscais  \n",
    "- Capital social  \n",
    "- Lucros acumulados  \n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ `raw_bpa` (Ativo)\n",
    "Mesmas colunas da estrutura comum.  \n",
    "O foco est√° nas contas de **ativos**, como:\n",
    "- Caixa e equivalentes  \n",
    "- Contas a receber  \n",
    "- Estoques  \n",
    "- Imobilizado  \n",
    "- Investimentos  \n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ `raw_dre` (Resultado)\n",
    "Inclui **duas datas adicionais**:\n",
    "\n",
    "| Coluna | Tipo | Descri√ß√£o |\n",
    "|:-------|:-----|:-----------|\n",
    "| **DT_INI_EXERC** | date | Data de in√≠cio do exerc√≠cio (geralmente 01/01/AAAA). |\n",
    "| **DT_FIM_EXERC** | date | Data de t√©rmino do exerc√≠cio (geralmente 31/12/AAAA). |\n",
    "\n",
    "Essas datas definem o **per√≠odo de apura√ß√£o** do resultado.\n",
    "\n",
    "Principais contas encontradas:\n",
    "- Receita l√≠quida de vendas  \n",
    "- Custo dos produtos vendidos  \n",
    "- Despesas operacionais  \n",
    "- Resultado financeiro  \n",
    "- Lucro (ou preju√≠zo) do exerc√≠cio  \n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ Rela√ß√£o entre os DataFrames\n",
    "\n",
    "| DataFrame | Tipo de Demonstra√ß√£o | Rela√ß√£o com outros |\n",
    "|:-----------|:--------------------|:-------------------|\n",
    "| **raw_bpa** | Ativo (Balan√ßo Patrimonial) | Relaciona-se com `raw_bpp` (mesmo per√≠odo e CNPJ_CIA) |\n",
    "| **raw_bpp** | Passivo (Balan√ßo Patrimonial) | Complementa o `raw_bpa` para totalizar o balan√ßo |\n",
    "| **raw_dre** | Resultado (DRE) | Demonstra o desempenho que afeta o patrim√¥nio l√≠quido do `raw_bpp` |\n",
    "\n",
    "\n",
    "### üìà Uso t√≠pico no pipeline\n",
    "\n",
    "1. **Download e extra√ß√£o** via `cvm_downloader_clean.py` ‚Üí gera os CSVs.\n",
    "2. **Leitura** dos arquivos CSV em pandas ‚Üí cria `raw_bpp`, `raw_bpa`, `raw_dre`.\n",
    "3. **Limpeza e padroniza√ß√£o** ‚Üí converte tipos, remove duplicatas, ajusta nomes.\n",
    "4. **Enriquecimento e an√°lise** ‚Üí c√°lculo de indicadores como liquidez, endividamento, rentabilidade.\n",
    "\n",
    "\n",
    "**Refer√™ncia oficial:**  \n",
    "üìö [Portal de Dados Abertos CVM ‚Äì Documentos DFP/ITR](https://dados.cvm.gov.br/dados/CIA_ABERTA/DOC/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fbf091",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **üì¶ cvm_downloader** ‚Äî Download & Unzip autom√°tico (CVM: DFP / ITR)\n",
    "\n",
    "**1. Descri√ß√£o curta**  \n",
    "Script robusto para baixar (streaming com progress bar) e descompactar os arquivos p√∫blicos da CVM (`DFP` e `ITR`) por ano. Inclui retries, backoff, grava√ß√£o segura (.part ‚Üí rename) e verifica√ß√£o para pular arquivos j√° existentes. Tem suporte a execu√ß√£o sequencial ou com --workers (thread pool).\n",
    "\n",
    "**2. Principais responsabilidades**\n",
    "- Construir URLs padr√£o: **https://dados.cvm.gov.br/dados/CIA_ABERTA/DOC/{doc_type}/DADOS/{doc_type}_cia_aberta_{year}.zip**\n",
    "- Baixar zips com **requests.Session** + retries\n",
    "- Exibir barra de progresso (**tqdm**)\n",
    "- Descompactar zips (**zipfile.ZipFile**)\n",
    "- Pular arquivos j√° baixados / extra√≠dos\n",
    "- Opcional: paralelizar downloads via **ThreadPoolExecutor**\n",
    "\n",
    "**3. Entradas / Par√¢metros**\n",
    "- --doc-types (ex.: DFP,ITR)\n",
    "- --start-year / --end-year (ex.: 2011 / 2025)\n",
    "- --workers (n¬∫ threads; default 1)\n",
    "\n",
    "**4. Sa√≠das**\n",
    "- data/cvm/zip/{DOC}_cia_aberta_{YEAR}.zip\n",
    "- data/cvm/unzipped/{DOC}_{YEAR}/... (conte√∫do extra√≠do)\n",
    "- Retorna resumo (ok / skipped / failed) e c√≥digo de sa√≠da CLI\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db4ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configura√ß√£o ---\n",
    "# Onde os dados ser√£o salvos\n",
    "BASE_DIR = os.path.join(\"data\", \"cvm\")\n",
    "ZIP_DIR = os.path.join(BASE_DIR, \"zip\")\n",
    "UNZIPPED_DIR = os.path.join(BASE_DIR, \"unzipped\")\n",
    "\n",
    "# Cria as pastas se n√£o existirem\n",
    "os.makedirs(ZIP_DIR, exist_ok=True)\n",
    "os.makedirs(UNZIPPED_DIR, exist_ok=True)\n",
    "\n",
    "# URL base para os arquivos da CVM\n",
    "URL_BASE = \"https://dados.cvm.gov.br/dados/CIA_ABERTA/DOC/{doc_type}/DADOS/\"\n",
    "\n",
    "# Tipos de documentos que queremos\n",
    "# DFP -> Anual, ITR -> Trimestral\n",
    "DOC_TYPES = ['DFP', 'ITR']\n",
    "\n",
    "YEARS = range(2011, 2026) \n",
    "\n",
    "def download_and_unzip(url, zip_path, unzipped_path):\n",
    "    \"\"\"Baixa e descompacta um arquivo ZIP se ele n√£o existir localmente.\"\"\"\n",
    "    if os.path.exists(zip_path):\n",
    "        print(f\"Arquivo j√° existe, pulando download: {os.path.basename(zip_path)}\")\n",
    "    else:\n",
    "        print(f\"Baixando: {os.path.basename(zip_path)}\")\n",
    "        try:\n",
    "            response = requests.get(url, stream=True)\n",
    "            response.raise_for_status() # Lan√ßa erro se a requisi√ß√£o falhar\n",
    "            \n",
    "            total_size = int(response.headers.get('content-length', 0))\n",
    "            \n",
    "            with open(zip_path, 'wb') as f, tqdm(\n",
    "                desc=os.path.basename(zip_path),\n",
    "                total=total_size,\n",
    "                unit='iB',\n",
    "                unit_scale=True,\n",
    "                unit_divisor=1024,\n",
    "            ) as bar:\n",
    "                for data in response.iter_content(chunk_size=1024):\n",
    "                    size = f.write(data)\n",
    "                    bar.update(size)\n",
    "            print(\"Download completo.\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erro no download de {url}: {e}\")\n",
    "            return # Sai da fun√ß√£o se o download falhar\n",
    "\n",
    "    print(f\"Descompactando: {os.path.basename(zip_path)}\")\n",
    "    try:\n",
    "        with ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(unzipped_path)\n",
    "        print(\"Descompactado com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao descompactar {zip_path}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for doc_type in DOC_TYPES:\n",
    "        for year in YEARS:\n",
    "            filename = f\"{doc_type}_cia_aberta_{year}.zip\"\n",
    "            url = URL_BASE.format(doc_type=doc_type) + filename\n",
    "            \n",
    "            zip_path = os.path.join(ZIP_DIR, filename)\n",
    "            unzipped_path = os.path.join(UNZIPPED_DIR, f\"{doc_type}_{year}\")\n",
    "            \n",
    "            download_and_unzip(url, zip_path, unzipped_path)\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "    print(\"\\nProcesso de download e extra√ß√£o conclu√≠do!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4c204d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### üß© cvm_parser ‚Äî Parser / Busca consolidada (_con_ files) e prepara√ß√£o\n",
    "\n",
    "**1. Descri√ß√£o curta**  \n",
    "Pipeline para localizar arquivos CSV consolidados _con_*.csv extra√≠dos da CVM, aplicar filtros (√öLTIMO/PEN√öLTIMO, escala MIL/UNIDADE), normalizar VL_CONTA, diagnosticar CNPJ(s) de interesse (ex.: Sanepar), concatenar, deduplicar e salvar resultado processado.\n",
    "\n",
    "**2. Principais responsabilidades**\n",
    "- Encontrar arquivos consolidados recursivamente (`UNZIPPED_DIR/**/*_con_*.csv`)\n",
    "- Ler CSVs com pd.read_csv(..., encoding='latin-1', sep=';')\n",
    "- Filtrar por ORDEM_EXERC (`√öLTIMO` / `PEN√öLTIMO`) e ESCALA_MOEDA (`MIL` / `UNIDADE`)\n",
    "- Converter VL_CONTA para num√©rico e aplicar ajuste (multiplica por 1000 quando `ESCALA_MOEDA == 'MIL'`)\n",
    "- Diagn√≥stico: detectar CNPJ_SAPR e imprimir valores de `ORDEM_EXERC` / `ESCALA_MOEDA`\n",
    "- Concatenar chunks, ordenar por CNPJ_CIA, DT_FIM_EXERC, VERSAO\n",
    "- Drop duplicates por ['CNPJ_CIA','DT_FIM_EXERC','CD_CONTA']\n",
    "- Salvar processed/raw_{doc}.parquet e processed/raw_{doc}.csv\n",
    "\n",
    "**3. Entradas**\n",
    "- data/cvm/unzipped/... (arquivos _con_*.csv)\n",
    "\n",
    "**4. Sa√≠das**\n",
    "- data/cvm/processed/raw_{dre|bpa|bpp}.parquet\n",
    "- data/cvm/processed/raw_{dre|bpa|bpp}.csv\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427a44bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Configura√ß√£o (edite aqui)\n",
    "# -----------------------\n",
    "BASE_DIR = Path(\"data\") / \"cvm\"\n",
    "UNZIPPED_DIR = BASE_DIR / \"unzipped\"\n",
    "PROCESSED_DIR = BASE_DIR / \"processed\"\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DOC_PATTERNS_BROAD = {\n",
    "    \"dre\": \"*_DRE_con_*.csv\",\n",
    "    \"bpa\": \"*_BPA_con_*.csv\",\n",
    "    \"bpp\": \"*_BPP_con_*.csv\",\n",
    "}\n",
    "\n",
    "CNPJ_SAPR = \"76.484.013/0001-45\"\n",
    "CNPJs_DEBUG = [CNPJ_SAPR, \"33.839.910/0001-11\"]  # VIVA3\n",
    "\n",
    "CATEGORY_COLS = [\n",
    "    \"CNPJ_CIA\", \"DENOM_CIA\", \"GRUPO_DFP\", \"MOEDA\",\n",
    "    \"ESCALA_MOEDA\", \"ORDEM_EXERC\", \"CD_CONTA\",\n",
    "    \"DS_CONTA\", \"ST_CONTA_FIXA\"\n",
    "]\n",
    "\n",
    "# -----------------------\n",
    "# Logging\n",
    "# -----------------------\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "logger = logging.getLogger(\"cvm_parser\")\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def find_consolidated_files(unzipped_dir: Path, pattern: str) -> List[Path]:\n",
    "    \"\"\"Retorna lista de arquivos que batem com o padr√£o (recursivo).\"\"\"\n",
    "    return list(unzipped_dir.rglob(pattern))\n",
    "\n",
    "\n",
    "def _ensure_category(df: pd.DataFrame, col: str) -> None:\n",
    "    \"\"\"Converte coluna para category quando apropriado (silencioso).\"\"\"\n",
    "    try:\n",
    "        if col in df.columns and not pd.api.types.is_categorical_dtype(df[col]):\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "    except Exception:\n",
    "        # n√£o quebrar a execu√ß√£o por problemas de convers√£o\n",
    "        pass\n",
    "\n",
    "\n",
    "def _read_csv_safe(path: Path) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"L√™ um CSV com par√¢metros padr√£o usados no pipeline; captura exce√ß√µes e retorna None se falhar.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path, encoding=\"latin-1\", sep=\";\", low_memory=False, dtype={\"CNPJ_CIA\": str})\n",
    "        return df\n",
    "    except Exception as exc:\n",
    "        logger.warning(\"Falha lendo %s: %s\", path, exc)\n",
    "        return None\n",
    "\n",
    "\n",
    "def _process_df_chunk(df: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Aplicar filtros e tratamentos ao chunk (j√° lido).\n",
    "    Retorna DataFrame filtrado pronto para concatenar ou None se vazio.\n",
    "    \"\"\"\n",
    "    # garantir colunas chave\n",
    "    if \"DT_FIM_EXERC\" not in df.columns:\n",
    "        return None\n",
    "\n",
    "    # normalizar e otimizar\n",
    "    for col in CATEGORY_COLS:\n",
    "        if col in df.columns:\n",
    "            _ensure_category(df, col)\n",
    "\n",
    "    # converter data e remover linhas sem data v√°lida\n",
    "    df[\"DT_FIM_EXERC\"] = pd.to_datetime(df[\"DT_FIM_EXERC\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"DT_FIM_EXERC\"])\n",
    "    if df.empty:\n",
    "        return None\n",
    "\n",
    "    # aplicar filtro ORDEM_EXERC & ESCALA_MOEDA quando existirem\n",
    "    if \"ORDEM_EXERC\" in df.columns and \"ESCALA_MOEDA\" in df.columns:\n",
    "        mask = (\n",
    "            df[\"ORDEM_EXERC\"].isin([\"√öLTIMO\", \"PEN√öLTIMO\"]) &\n",
    "            df[\"ESCALA_MOEDA\"].isin([\"MIL\", \"UNIDADE\"])\n",
    "        )\n",
    "        df = df.loc[mask].copy()\n",
    "\n",
    "    if df.empty:\n",
    "        return None\n",
    "\n",
    "    # garantir VL_CONTA num√©rico; remover nulos\n",
    "    if \"VL_CONTA\" in df.columns:\n",
    "        df[\"VL_CONTA\"] = pd.to_numeric(df[\"VL_CONTA\"], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[\"VL_CONTA\"])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # ajustar escala MIL ‚Üí multiplicar por 1000\n",
    "    if \"ESCALA_MOEDA\" in df.columns:\n",
    "        if pd.api.types.is_categorical_dtype(df[\"ESCALA_MOEDA\"]):\n",
    "            cats = df[\"ESCALA_MOEDA\"].cat.categories\n",
    "            is_mil = df[\"ESCALA_MOEDA\"].cat.codes == int(np.where(cats == \"MIL\")[0][0]) if \"MIL\" in cats else False\n",
    "        else:\n",
    "            is_mil = df[\"ESCALA_MOEDA\"].astype(str) == \"MIL\"\n",
    "        # aplicar ajuste com np.where (vetorizado)\n",
    "        df[\"VL_CONTA\"] = np.where(is_mil, df[\"VL_CONTA\"] * 1000.0, df[\"VL_CONTA\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Pipeline principal\n",
    "# -----------------------\n",
    "def parse_and_consolidate_final(\n",
    "    doc_name: str,\n",
    "    broad_pattern: str,\n",
    "    unzipped_dir: Path = UNZIPPED_DIR,\n",
    "    processed_dir: Path = PROCESSED_DIR,\n",
    ") -> Dict[str, object]:\n",
    "    \"\"\"\n",
    "    Encontra arquivos consolidados (_con_), processa, concatena, deduplica e salva parquet/csv.\n",
    "    Retorna dicion√°rio com estat√≠sticas do processamento.\n",
    "    \"\"\"\n",
    "    logger.info(\"Iniciando processamento CONSOLIDADO para: %s (padr√£o: %s)\", doc_name.upper(), broad_pattern)\n",
    "    files = find_consolidated_files(unzipped_dir, broad_pattern)\n",
    "    if not files:\n",
    "        logger.info(\"Nenhum arquivo encontrado para o padr√£o: %s\", broad_pattern)\n",
    "        return {\"status\": \"no_files\", \"files_count\": 0}\n",
    "\n",
    "    df_chunks = []\n",
    "    total_rows_read = 0\n",
    "    total_rows_after_filter = 0\n",
    "    sapr_found = False\n",
    "\n",
    "    for path in tqdm(files, desc=f\"Processando {doc_name.upper()}\"):\n",
    "        df = _read_csv_safe(path)\n",
    "        if df is None:\n",
    "            continue\n",
    "\n",
    "        total_rows_read += len(df)\n",
    "\n",
    "        # diagnostico SAPR (apenas relat√≥rio, sem interromper)\n",
    "        if \"CNPJ_CIA\" in df.columns:\n",
    "            df[\"CNPJ_CIA\"] = df[\"CNPJ_CIA\"].astype(str).str.strip()\n",
    "            df_sapr = df[df[\"CNPJ_CIA\"] == CNPJ_SAPR]\n",
    "            if not df_sapr.empty and not sapr_found:\n",
    "                sapr_found = True\n",
    "                logger.info(\"[DIAGN√ìSTICO SAPR11] Encontrado em: %s\", path.name)\n",
    "                if \"ORDEM_EXERC\" in df_sapr.columns:\n",
    "                    logger.info(\"  ORDEM_EXERC values: %s\", df_sapr[\"ORDEM_EXERC\"].unique())\n",
    "                if \"ESCALA_MOEDA\" in df_sapr.columns:\n",
    "                    logger.info(\"  ESCALA_MOEDA values: %s\", df_sapr[\"ESCALA_MOEDA\"].unique())\n",
    "\n",
    "        # processa e filtra o chunk\n",
    "        try:\n",
    "            processed = _process_df_chunk(df)\n",
    "            if processed is not None and not processed.empty:\n",
    "                total_rows_after_filter += len(processed)\n",
    "                df_chunks.append(processed)\n",
    "        except Exception as exc:\n",
    "            logger.warning(\"Erro processando arquivo %s: %s\", path, exc)\n",
    "        finally:\n",
    "            # liberar mem√≥ria\n",
    "            del df\n",
    "            gc.collect()\n",
    "\n",
    "    logger.info(\"Totais: linhas lidas=%d, linhas ap√≥s filtros=%d\", total_rows_read, total_rows_after_filter)\n",
    "    if not sapr_found:\n",
    "        logger.warning(\"[DIAGN√ìSTICO SAPR11] CNPJ Sanepar (%s) N√ÉO encontrado na busca consolidada.\", CNPJ_SAPR)\n",
    "\n",
    "    if not df_chunks:\n",
    "        logger.info(\"Nenhum chunk com dados v√°lidos ap√≥s filtros. Abortando concatena√ß√£o.\")\n",
    "        return {\"status\": \"no_data_after_filter\", \"files_count\": len(files)}\n",
    "\n",
    "    # concat + sort + dedupe\n",
    "    logger.info(\"Concatenando %d chunks...\", len(df_chunks))\n",
    "    consolidated_df = pd.concat(df_chunks, ignore_index=True)\n",
    "    consolidated_df.sort_values(by=[\"CNPJ_CIA\", \"DT_FIM_EXERC\", \"VERSAO\"], ascending=[True, True, False], inplace=True)\n",
    "\n",
    "    dedup_subset = [\"CNPJ_CIA\", \"DT_FIM_EXERC\", \"CD_CONTA\"]\n",
    "    final_df = consolidated_df.drop_duplicates(subset=dedup_subset, keep=\"first\").copy()\n",
    "\n",
    "    # diagn√≥stico r√°pido para CNPJs de debug\n",
    "    if \"CNPJ_CIA\" in final_df.columns:\n",
    "        debug_mask = final_df[\"CNPJ_CIA\"].astype(str).isin([str(x) for x in CNPJs_DEBUG])\n",
    "        debug_data = final_df.loc[debug_mask]\n",
    "        if not debug_data.empty:\n",
    "            grouped = debug_data.groupby([\"CNPJ_CIA\", \"DT_FIM_EXERC\"]).size().reset_index(name=\"contagem_contas\")\n",
    "            logger.info(\"Dados SAPR11/VIVA3 no DF final:\\n%s\", grouped.head(20).to_string(index=False))\n",
    "        else:\n",
    "            logger.info(\"Nenhum dado SAPR11/VIVA3 no DF final.\")\n",
    "\n",
    "    # salvar resultados\n",
    "    out_parquet = processed_dir / f\"raw_{doc_name}.parquet\"\n",
    "    out_csv = processed_dir / f\"raw_{doc_name}.csv\"\n",
    "\n",
    "    try:\n",
    "        logger.info(\"Salvando Parquet: %s\", out_parquet)\n",
    "        final_df.to_parquet(out_parquet, index=False)\n",
    "        logger.info(\"Salvando CSV: %s\", out_csv)\n",
    "        final_df.to_csv(out_csv, index=False, sep=\";\", encoding=\"utf-8-sig\")\n",
    "        logger.info(\"Shape final salvo: %s\", final_df.shape)\n",
    "    except Exception as exc:\n",
    "        logger.exception(\"Falha ao salvar arquivos: %s\", exc)\n",
    "        return {\"status\": \"save_error\", \"error\": str(exc)}\n",
    "\n",
    "    # limpeza final\n",
    "    del df_chunks, consolidated_df, final_df\n",
    "    gc.collect()\n",
    "\n",
    "    return {\n",
    "        \"status\": \"ok\",\n",
    "        \"files_count\": len(files),\n",
    "        \"rows_read\": total_rows_read,\n",
    "        \"rows_after_filter\": total_rows_after_filter,\n",
    "        \"saved_parquet\": str(out_parquet),\n",
    "        \"saved_csv\": str(out_csv),\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results = {}\n",
    "    for name, pattern in DOC_PATTERNS_BROAD.items():\n",
    "        results[name] = parse_and_consolidate_final(name, pattern)\n",
    "        gc.collect()\n",
    "\n",
    "    logger.info(\"Processo de parsing (v9.0 - Busca Consolidada) conclu√≠do!\")\n",
    "    logger.info(\"Consulte logs acima para mensagens '---> [DIAGN√ìSTICO SAPR11 v9.0]'.\")\n",
    "    logger.info(\"Resumo por documento: %s\", results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cd176a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "### **üîß cv_processor ‚Äî** Transforma√ß√£o para formato WIDE (fundamentals_wide)\n",
    "\n",
    "**1. Descri√ß√£o curta**  \n",
    "Transforma os arquivos processados (`processed/raw_dre.parquet`, `raw_bpa.parquet`, `raw_bpp.parquet`) em tabelas *wide* prontos para an√°lise fundamentalista. Mapeia c√≥digos de conta para nomes leg√≠veis (plano de contas), filtra pelo √öLTIMO exerc√≠cio, pivota (long ‚Üí wide) e faz o merge final entre DRE, BPA e BPP.\n",
    "\n",
    "**2. Principais responsabilidades**\n",
    "- Ler `processed/raw_{doc}.parquet` para cada doc type (dre, bpa, bpp)\n",
    "- Filtrar linhas por CD_CONTA usando`MAPA_CONTAS_*\n",
    "- Manter apenas ORDEM_EXERC == '√öLTIMO' quando dispon√≠vel\n",
    "- Converter VL_CONTA para num√©rico e remover nulos\n",
    "- Pivot (index: CNPJ_CIA, DENOM_CIA, DT_FIM_EXERC; columns: nome leg√≠vel da CONTA)\n",
    "- Merge outer entre DRE / BPA / BPP em ['CNPJ_CIA','DENOM_CIA','DT_FIM_EXERC']\n",
    "- Salvar data/cvm/final/fundamentals_wide.parquet (+ CSV ;)\n",
    "\n",
    "**3. Entradas**\n",
    "- data/cvm/processed/raw_dre.parquet\n",
    "- data/cvm/processed/raw_bpa.parquet\n",
    "- data/cvm/processed/raw_bpp.parquet\n",
    "\n",
    "**4. Sa√≠das**\n",
    "- data/cvm/final/fundamentals_wide.parquet\n",
    "- data/cvm/final/fundamentals_wide.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f01c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "import gc # Para garbage collection\n",
    "\n",
    "# -----------------------\n",
    "# Configura√ß√£o (ATUALIZADA)\n",
    "# -----------------------\n",
    "BASE_DIR = Path(\"data\") / \"cvm\"\n",
    "PROCESSED_DIR = BASE_DIR / \"processed\"\n",
    "FINAL_DIR = BASE_DIR / \"final\"\n",
    "FINAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Mapa de contas PRINCIPAIS (agregadas)\n",
    "MAPA_CONTAS_DRE_MAIN = {\n",
    "    \"3.01\": \"Receita L√≠quida\",\n",
    "    \"3.02\": \"Custo dos Bens e/ou Servi√ßos Vendidos\",\n",
    "    \"3.03\": \"Lucro Bruto\",\n",
    "    \"3.05\": \"EBIT\",\n",
    "    \"3.07\": \"EBT\",\n",
    "    \"3.11\": \"Lucro L√≠quido Consolidado\",\n",
    "}\n",
    "\n",
    "MAPA_CONTAS_BPA_MAIN = {\n",
    "    \"1\": \"Ativo Total\",\n",
    "    \"1.01\": \"Ativo Circulante\",\n",
    "    \"1.02\": \"Ativo N√£o Circulante\",\n",
    "}\n",
    "\n",
    "MAPA_CONTAS_BPP_MAIN = {\n",
    "    \"2\": \"Passivo Total\",\n",
    "    \"2.01\": \"Passivo Circulante\",\n",
    "    \"2.02\": \"Passivo N√£o Circulante\",\n",
    "    \"2.03\": \"Patrim√¥nio L√≠quido Consolidado\",\n",
    "}\n",
    "\n",
    "# --- NOVO: Mapa de contas DETALHADAS (para colunas espec√≠ficas) ---\n",
    "# Estes c√≥digos s√£o comuns, mas podem precisar de ajustes finos\n",
    "# Verifique o plano de contas da CVM para mais detalhes se necess√°rio\n",
    "MAPA_CONTAS_DETALHADAS_BPA = {\n",
    "    \"1.01.01\": \"Caixa e Equivalentes\", # Caixa e Equivalentes de Caixa\n",
    "}\n",
    "\n",
    "MAPA_CONTAS_DETALHADAS_BPP = {\n",
    "    \"2.01.04\": \"D√≠vida Curto Prazo\", # Empr√©stimos e Financiamentos CP\n",
    "    \"2.02.01\": \"D√≠vida Longo Prazo\", # Empr√©stimos e Financiamentos LP\n",
    "}\n",
    "# --- FIM NOVO ---\n",
    "\n",
    "# Estrutura unificada dos mapas\n",
    "MAPA_CONTAS_GERAL = {\n",
    "    \"dre\": {\"main\": MAPA_CONTAS_DRE_MAIN, \"detailed\": None}, # DRE n√£o tem detalhado por enquanto\n",
    "    \"bpa\": {\"main\": MAPA_CONTAS_BPA_MAIN, \"detailed\": MAPA_CONTAS_DETALHADAS_BPA},\n",
    "    \"bpp\": {\"main\": MAPA_CONTAS_BPP_MAIN, \"detailed\": MAPA_CONTAS_DETALHADAS_BPP},\n",
    "}\n",
    "\n",
    "# Colunas chave esperadas\n",
    "INDEX_COLS = [\"CNPJ_CIA\", \"DENOM_CIA\", \"DT_FIM_EXERC\"]\n",
    "\n",
    "# -----------------------\n",
    "# Logging\n",
    "# -----------------------\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "logger = logging.getLogger(\"cvm_transform\")\n",
    "\n",
    "# -----------------------\n",
    "# Fun√ß√µes utilit√°rias\n",
    "# -----------------------\n",
    "def _read_processed_parquet(path: Path) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"L√™ um parquet com tratamento de erro.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_parquet(path)\n",
    "        logger.info(\"Lido parquet: %s (shape=%s)\", path.name, df.shape)\n",
    "        return df\n",
    "    except Exception as exc:\n",
    "        logger.warning(\"Falha ao ler parquet %s: %s\", path.name, exc)\n",
    "        return None\n",
    "\n",
    "# --- ATUALIZADO: Fun√ß√£o process_and_pivot_file ---\n",
    "def process_and_pivot_file(\n",
    "    doc_name: str,\n",
    "    main_account_map: Dict[str, str],\n",
    "    detailed_account_map: Optional[Dict[str, str]] = None, # Aceita mapa detalhado opcional\n",
    "    processed_dir: Path = PROCESSED_DIR,\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Carrega raw parquet, filtra contas (principais E detalhadas),\n",
    "    mant√©m '√öLTIMO' exerc√≠cio e pivota para wide.\n",
    "    \"\"\"\n",
    "    input_file = processed_dir / f\"raw_{doc_name}.parquet\"\n",
    "    logger.info(\"Processando %s -> %s\", doc_name.upper(), input_file.name)\n",
    "\n",
    "    if not input_file.exists():\n",
    "        logger.warning(\"Arquivo n√£o encontrado: %s. Pulando %s.\", input_file.name, doc_name)\n",
    "        return None\n",
    "\n",
    "    df = _read_processed_parquet(input_file)\n",
    "    if df is None or df.empty: return None\n",
    "\n",
    "    # Validar colunas essenciais\n",
    "    essential_cols = [\"CD_CONTA\", \"VL_CONTA\", \"DT_FIM_EXERC\", \"ORDEM_EXERC\"] + INDEX_COLS\n",
    "    missing_essentials = [c for c in essential_cols if c not in df.columns]\n",
    "    if missing_essentials:\n",
    "        logger.error(\"Colunas essenciais ausentes em %s: %s. Pulando.\", input_file.name, missing_essentials)\n",
    "        return None\n",
    "\n",
    "    # --- L√ìGICA ATUALIZADA: Combinar contas principais e detalhadas ---\n",
    "    all_accounts_to_keep = list(main_account_map.keys())\n",
    "    if detailed_account_map:\n",
    "        all_accounts_to_keep.extend(list(detailed_account_map.keys()))\n",
    "        logger.info(f\"Contas detalhadas a serem extra√≠das: {list(detailed_account_map.keys())}\")\n",
    "\n",
    "    logger.info(f\"Total de c√≥digos de conta a serem filtrados: {len(all_accounts_to_keep)}\")\n",
    "    df = df[df[\"CD_CONTA\"].isin(all_accounts_to_keep)].copy()\n",
    "    logger.info(\"Ap√≥s filtrar contas (principais + detalhadas): shape=%s\", df.shape)\n",
    "    if df.empty:\n",
    "        logger.info(\"Nenhuma conta de interesse encontrada. Pulando.\")\n",
    "        return None\n",
    "    # --- FIM L√ìGICA ATUALIZADA ---\n",
    "\n",
    "    # Converter data e filtrar por ORDEM_EXERC == '√öLTIMO'\n",
    "    df[\"DT_FIM_EXERC\"] = pd.to_datetime(df[\"DT_FIM_EXERC\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"DT_FIM_EXERC\"])\n",
    "    df = df[df[\"ORDEM_EXERC\"] == \"√öLTIMO\"].copy()\n",
    "    logger.info(\"Ap√≥s filtrar ORDEM_EXERC == '√öLTIMO': shape=%s\", df.shape)\n",
    "    if df.empty:\n",
    "        logger.info(\"Nenhuma linha '√öLTIMO' encontrada. Pulando.\")\n",
    "        return None\n",
    "\n",
    "    # --- L√ìGICA ATUALIZADA: Mapear nome da conta (prioriza detalhado) ---\n",
    "    # Primeiro tenta mapear com o mapa detalhado, depois com o principal\n",
    "    map_detalhado = detailed_account_map if detailed_account_map else {}\n",
    "    df[\"CONTA\"] = df[\"CD_CONTA\"].map(map_detalhado).fillna(df[\"CD_CONTA\"].map(main_account_map))\n",
    "    # Remove contas que n√£o foram mapeadas por nenhum dos mapas (se houver)\n",
    "    df = df.dropna(subset=[\"CONTA\"])\n",
    "    logger.info(\"Contas mapeadas. Exemplo de nomes: %s\", df[\"CONTA\"].unique()[:5])\n",
    "    if df.empty:\n",
    "        logger.info(\"Nenhuma conta mapeada resultou em nome v√°lido. Pulando.\")\n",
    "        return None\n",
    "    # --- FIM L√ìGICA ATUALIZADA ---\n",
    "\n",
    "    # Garantir VL_CONTA num√©rico e remover ausentes\n",
    "    df[\"VL_CONTA\"] = pd.to_numeric(df[\"VL_CONTA\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"VL_CONTA\"])\n",
    "    if df.empty:\n",
    "        logger.info(\"Sem valores num√©ricos para VL_CONTA. Pulando.\")\n",
    "        return None\n",
    "\n",
    "    # Pivot (long -> wide)\n",
    "    try:\n",
    "        logger.info(\"Pivotando (long -> wide)...\")\n",
    "        # Usar fill_value=0 pode ser √∫til se algumas contas n√£o aparecem em todos os trimestres\n",
    "        df_wide = df.pivot_table(\n",
    "            index=INDEX_COLS,\n",
    "            columns=\"CONTA\", # Usar√° os nomes mapeados (incluindo os detalhados)\n",
    "            values=\"VL_CONTA\",\n",
    "            aggfunc=\"sum\",\n",
    "            fill_value=0 # Preenche com 0 contas ausentes naquele per√≠odo/empresa\n",
    "        )\n",
    "        df_wide = df_wide.reset_index()\n",
    "        df_wide.columns.name = None # Limpa o nome do √≠ndice das colunas\n",
    "        logger.info(\"Pivot conclu√≠do: shape=%s\", df_wide.shape)\n",
    "        logger.info(\"Colunas geradas pelo pivot: %s\", df_wide.columns.tolist())\n",
    "    except Exception as exc:\n",
    "        logger.exception(\"Erro ao pivotar %s: %s\", input_file.name, exc)\n",
    "        return None\n",
    "\n",
    "    del df\n",
    "    gc.collect()\n",
    "    return df_wide\n",
    "\n",
    "# -----------------------\n",
    "# Fun√ß√£o para juntar todos os DF wide (dre,bpa,bpp) - SEM ALTERA√á√ÉO\n",
    "# -----------------------\n",
    "def merge_fundamentals(dfs_wide: Dict[str, pd.DataFrame]) -> Optional[pd.DataFrame]:\n",
    "    \"\"\" Junta os DataFrames wide (DRE, BPA, BPP) \"\"\"\n",
    "    if not dfs_wide:\n",
    "        logger.warning(\"Nenhum DataFrame wide fornecido para merge.\")\n",
    "        return None\n",
    "    valid_dfs = {k: v for k, v in dfs_wide.items() if v is not None and not v.empty}\n",
    "    if not valid_dfs:\n",
    "        logger.warning(\"Nenhum DataFrame wide V√ÅLIDO fornecido para merge.\")\n",
    "        return None\n",
    "\n",
    "    keys = list(valid_dfs.keys())\n",
    "    base = valid_dfs[keys[0]].copy()\n",
    "    logger.info(\"Usando %s como base para merge (shape=%s)\", keys[0], base.shape)\n",
    "\n",
    "    for k in keys[1:]:\n",
    "        logger.info(\"Mesclando com %s (shape=%s)\", k, valid_dfs[k].shape)\n",
    "        # Verifica colunas duplicadas (exceto as de √≠ndice) antes do merge\n",
    "        cols_to_merge = valid_dfs[k].columns.difference(base.columns).tolist() + INDEX_COLS\n",
    "        base = pd.merge(base, valid_dfs[k][cols_to_merge], on=INDEX_COLS, how=\"outer\")\n",
    "        logger.info(\"Shape ap√≥s merge com %s: %s\", k, base.shape)\n",
    "\n",
    "    base = base.sort_values(by=[\"CNPJ_CIA\", \"DT_FIM_EXERC\"]).reset_index(drop=True)\n",
    "    logger.info(\"Merge finalizado: shape=%s\", base.shape)\n",
    "    logger.info(\"Colunas finais: %s\", base.columns.tolist())\n",
    "    return base\n",
    "\n",
    "\n",
    "def save_final(df: pd.DataFrame, final_dir: Path = FINAL_DIR, fname: str = \"fundamentals_wide.parquet\") -> Dict[str, str]:\n",
    "    \"\"\" Salva o DataFrame final em parquet e CSV \"\"\"\n",
    "    final_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_parquet = final_dir / fname\n",
    "    out_csv = final_dir / str(fname).replace(\".parquet\", \".csv\")\n",
    "\n",
    "    try:\n",
    "        logger.info(\"Salvando parquet final em: %s\", out_parquet)\n",
    "        df.to_parquet(out_parquet, index=False)\n",
    "        logger.info(\"Salvando CSV final em: %s\", out_csv)\n",
    "        df.to_csv(out_csv, index=False, sep=\";\", encoding=\"utf-8-sig\")\n",
    "        return {\"parquet\": str(out_parquet), \"csv\": str(out_csv)}\n",
    "    except Exception as exc:\n",
    "        logger.exception(\"Erro ao salvar arquivo final: %s\", exc)\n",
    "        raise\n",
    "\n",
    "# -----------------------\n",
    "# Execu√ß√£o principal (ATUALIZADA)\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Iniciando transforma√ß√£o para formato WIDE (com contas detalhadas)...\")\n",
    "\n",
    "    dfs_wide = {}\n",
    "    # --- L√ìGICA ATUALIZADA: Iterar sobre a nova estrutura de mapas ---\n",
    "    for doc_type, maps_dict in MAPA_CONTAS_GERAL.items():\n",
    "        main_map = maps_dict.get(\"main\")\n",
    "        detailed_map = maps_dict.get(\"detailed\") # Pode ser None\n",
    "\n",
    "        if main_map: # S√≥ processa se houver um mapa principal\n",
    "            logger.info(\"-\" * 20)\n",
    "            df_wide = process_and_pivot_file(doc_type, main_map, detailed_map) # Passa ambos os mapas\n",
    "            if df_wide is not None and not df_wide.empty:\n",
    "                dfs_wide[doc_type] = df_wide\n",
    "            else:\n",
    "                logger.warning(\"Processamento de %s n√£o gerou DataFrame v√°lido.\", doc_type)\n",
    "        else:\n",
    "             logger.warning(\"Mapa principal n√£o definido para %s. Pulando.\", doc_type)\n",
    "    # --- FIM L√ìGICA ATUALIZADA ---\n",
    "\n",
    "    if not dfs_wide:\n",
    "        logger.error(\"Nenhum dataframe produzido. Encerrando sem salvar.\")\n",
    "    else:\n",
    "        logger.info(\"-\" * 20)\n",
    "        logger.info(\"Iniciando merge dos DataFrames DRE, BPA, BPP...\")\n",
    "        final_df = merge_fundamentals(dfs_wide)\n",
    "        if final_df is None or final_df.empty:\n",
    "            logger.error(\"DataFrame final vazio ap√≥s merge. Encerrando.\")\n",
    "        else:\n",
    "            save_paths = save_final(final_df)\n",
    "            logger.info(\"-\" * 20)\n",
    "            logger.info(\"Processamento conclu√≠do com sucesso!\")\n",
    "            logger.info(\"Arquivos salvos: %s\", save_paths)\n",
    "            logger.info(\"Shape final mestre: %s\", final_df.shape)\n",
    "            logger.info(\"Colunas finais geradas: %s\", final_df.columns.tolist())\n",
    "            logger.info(\"Amostra do resultado final:\\n%s\",\n",
    "                        final_df.head()[INDEX_COLS + list(final_df.columns.difference(INDEX_COLS))].to_string(index=False)) # Reordena colunas para amostra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0358e077",
   "metadata": {},
   "source": [
    "### **Documenta√ß√£o:** Calculadora de Quality Score (AurumQualityScoreCalculator)\n",
    "\n",
    "#### **1. Objetivo**\n",
    "\n",
    "Este script √© um dos pilares centrais do **Pilar 1 (Qualidade Financeira)** do projeto Aurum. Sua responsabilidade √© transformar os dados fundamentalistas brutos (extra√≠dos da CVM e formatados no `fundamentals_wide.parquet`) em m√©tricas de performance acion√°veis.\n",
    "\n",
    "O script executa um pipeline completo que:\n",
    "1.  Carrega os dados brutos da CVM.\n",
    "2.  Calcula os valores **TTM (Trailing Twelve Months / √öltimos 12 Meses)**, corrigindo a natureza \"Acumulada no Ano\" (YTD) dos dados da CVM.\n",
    "3.  Calcula um conjunto abrangente de **ratios financeiros** (Rentabilidade, Margens, Alavancagem, etc.) usando os dados TTM.\n",
    "4.  Calcula um **Aurum Quality Score** inicial, baseado em um *ranking percentile cross-sectional* (por data) desses ratios.\n",
    "5.  Salva os resultados hist√≥ricos e os mais recentes.\n",
    "\n",
    "#### **2. Configura√ß√£o (Input)**\n",
    "\n",
    "O script depende de um √∫nico arquivo de entrada, que deve ser gerado pelo pipeline de processamento da CVM:\n",
    "\n",
    "* **`ata/cvm/final/fundamentals_wide.parquet**: Um arquivo Parquet contendo os dados de Balan√ßo Patrimonial (BP) e Demonstra√ß√£o do Resultado (DRE) em formato \"wide\" (uma linha por empresa/data, m√∫ltiplas colunas de m√©tricas).\n",
    "    * **Importante:** O script assume que os dados da DRE (ex: `Receita L√≠quida`, `Lucro Bruto`) est√£o no formato **YTD (Acumulado no Ano)**, que √© o padr√£o da CVM.\n",
    "\n",
    "#### 3. Pipeline de Execu√ß√£o (Passo a Passo)\n",
    "\n",
    "A classe **AurumQualityScoreCalculator** gerencia todo o fluxo de trabalho:\n",
    "\n",
    "##### **Passo 1:** Carregar e Preparar Dados (`load_and_prepare_data`)\n",
    "\n",
    "* Carrega o arquivo `fundamentals_wide.parquet`.\n",
    "* Chama _clean_fundamentals_data para:\n",
    "    * Converter DT_FIM_EXERC para datetime.\n",
    "    * Remover quaisquer linhas duplicadas por CNPJ_CIA e DT_FIM_EXERC.\n",
    "    * **Ordenar** os dados por CNPJ_CIA e DT_FIM_EXERC, o que √© **essencial** para o c√°lculo de TTM.\n",
    "\n",
    "##### **Passo 2:** C√°lculo de TTM (`calculate_ttm_data`)\n",
    "\n",
    "Este √© o passo mais cr√≠tico do script. Ele converte os dados de fluxo (DRE) de YTD para TTM.\n",
    "\n",
    "* **L√≥gica:** O script agrupa por `CNPJ_CIA` e aplica a fun√ß√£o `rolling_sum_group`.\n",
    "* **Desacumula√ß√£o (YTD -> Trimestral):** Para cada coluna de DRE, ele calcula o valor trimestral fazendo:\n",
    "    quarterly = group[col] - group[col].shift(1).fillna(0)\n",
    "* **Anualiza√ß√£o (Trimestral -> TTM):** Em seguida, ele calcula a soma m√≥vel dos √∫ltimos 4 valores trimestrais:\n",
    "    group[f'{col}_ttm'] = quarterly.rolling(window=4, min_periods=4).sum()\n",
    "* O resultado √© salvo em self.ratios_df, pronto para o c√°lculo dos ratios.\n",
    "\n",
    "##### Passo 3: C√°lculo de Ratios Financeiros (`calculate_financial_ratios`)\n",
    "\n",
    "* Usando o self.ratios_df (que agora cont√©m as colunas `_ttm`), este m√©todo calcula os principais indicadores financeiros.\n",
    "* **Categorias:**\n",
    "    * **Contas de D√≠vida: D√≠vida Bruta, D√≠vida L√≠quida, Capital Investido**.\n",
    "    * **Rentabilidade (com TTM):** ROE, ROA, ROIC.\n",
    "    * **Margens (com TTM):** MARGEM_EBIT, MARGEM_LIQUIDA, MARGEM_BRUTA.\n",
    "    * **Alavancagem:** ALAVANCAGEM (Passivo/Ativo), DIVIDA_PL, DIVIDA_LIQ_EBIT.\n",
    "    * **Liquidez:** LIQUIDEZ_CORRENTE.\n",
    "    * **Efici√™ncia (com TTM):** GIRO_ATIVO.\n",
    "* Usa safe_divide para evitar erros de divis√£o por zero.\n",
    "\n",
    "##### Passo 3.1: Tratamento de Outliers (`_handle_ratio_outliers`)\n",
    "\n",
    "* Ap√≥s o c√°lculo, os ratios s√£o limpos.\n",
    "* Valores `infinitos` s√£o substitu√≠dos por `NaN`.\n",
    "* Os dados s√£o \"winsorizados\": valores extremos s√£o \"clipados\" (limitados) aos **percentis 1% (inferior) e 99% (superior)**. Isso torna o scoring subsequente mais robusto.\n",
    "\n",
    "##### Passo 4: C√°lculo do Score de Qualidade (`calculate_quality_scores`)\n",
    "\n",
    "Este m√©todo implementa a **Metodologia de Ranking Cross-Sectional**.\n",
    "\n",
    "* **Configura√ß√£o:** Define as m√©tricas, pesos e a dire√ß√£o (ex: `ROIC` -> maior √© melhor, `DIVIDA_LIQ_EBIT` -> menor √© melhor).\n",
    "* **Ranking por Data:** O script agrupa os dados por `DT_FIM_EXERC` (`grouped_by_date`).\n",
    "* Para cada m√©trica, ele calcula o **ranking percentile** de cada empresa *dentro daquele per√≠odo*:\n",
    "    `scores_df[score_col] = grouped_by_date[metric].rank(pct=True) * 100`\n",
    "* **Score Composto:** O `aurum_quality_score` √© calculado como a soma ponderada desses rankings (percentis de 0 a 100). Valores `NaN` s√£o preenchidos com a mediana (50) para n√£o penalizar empresas excessivamente.\n",
    "* **Classifica√ß√£o Final:** O script calcula `quality_quintile` e `quality_grade` (A-E) com base no ranking *final* do `aurum_quality_score` de cada per√≠odo.\n",
    "\n",
    "##### Passo 5 e 6: Salvar Resultados (`get_latest_scores` e `save_results`)\n",
    "\n",
    "* O script gera e salva quatro arquivos distintos no diret√≥rio `data/aurum_scores_output/`.\n",
    "\n",
    "#### 6. Sa√≠da (Output)\n",
    "\n",
    "A execu√ß√£o do script gera os seguintes arquivos em `data/aurum_scores_output/`:\n",
    "\n",
    "1.  **`aurum_quality_scores_complete.parquet`**:\n",
    "    * O arquivo mais importante. Cont√©m o **hist√≥rico completo** de todas as empresas, datas, ratios calculados e scores. Este ser√° o input para o `AurumScoringSystem` avan√ßado e para o *backtesting*.\n",
    "\n",
    "2.  **`aurum_quality_scores_latest.parquet`**:\n",
    "    * Um arquivo de conveni√™ncia que cont√©m apenas o **√∫ltimo registro (data mais recente)** de score/ratios para cada empresa.\n",
    "\n",
    "3.  **`aurum_quality_scores_latest.csv`**:\n",
    "    * Vers√£o CSV do arquivo acima, para f√°cil visualiza√ß√£o em planilhas.\n",
    "\n",
    "4.  **`aurum_scores_statistics.txt`**:\n",
    "    * Um relat√≥rio de texto (`.txt`) leg√≠vel, mostrando as estat√≠sticas do √∫ltimo per√≠odo (distribui√ß√£o de notas, Top 10 empresas) para uma verifica√ß√£o r√°pida.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "568a033e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 727/727 [00:06<00:00, 106.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéâ AURUM QUALITY SCORE - RESULTADOS FINAIS (√öLTIMO PER√çODO)\n",
      "============================================================\n",
      "\n",
      "üìä TOTAL DE EMPRESAS: 727\n",
      "üìà SCORE M√âDIO: 49.33\n",
      "\n",
      "üìã DISTRIBUI√á√ÉO DAS NOTAS:\n",
      "   Nota A: 201 empresas\n",
      "   Nota B:  82 empresas\n",
      "   Nota C:  72 empresas\n",
      "   Nota D: 146 empresas\n",
      "   Nota E: 226 empresas\n",
      "\n",
      "ü•á TOP 10 EMPRESAS:\n",
      "    1. CAMIL ALIMENTOS S.A.                100.00 (Nota A)\n",
      "    2. MINUPAR PARTICIPACOES S.A.           91.13 (Nota A)\n",
      "    3. 521 PARTICIPACOES S.A. - EM LIQUIDA  89.17 (Nota A)\n",
      "    4. STEIN SP II PARTICIPA√á√ïES S.A.       87.37 (Nota A)\n",
      "    5. EPR INFRAESTRUTURA PR S.A.           85.60 (Nota A)\n",
      "    6. COMERCIAL QUINTELLA COM EXP SA EM L  84.67 (Nota A)\n",
      "    7. SONDOTECNICA ENGENHARIA SOLOS S.A.   84.42 (Nota A)\n",
      "    8. M√âLIUZ S.A.                          83.91 (Nota A)\n",
      "    9. FICTOR ALIMENTOS S.A.                82.46 (Nota A)\n",
      "   10. GRANJA FARIA S.A.                    82.18 (Nota A)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "tqdm.pandas()\n",
    "\n",
    "class AurumQualityScoreCalculator:\n",
    "    \"\"\" Calcula TTM, ratios financeiros (incluindo ROIC correto) e scores \"\"\"\n",
    "\n",
    "    def __init__(self, fundamentals_path: str):\n",
    "        self.fundamentals_path = Path(fundamentals_path)\n",
    "        self.fundamentals_df = None\n",
    "        self.ratios_df = None\n",
    "        self.scores_df = None\n",
    "        self.dre_cols = [\n",
    "            'Receita L√≠quida', 'Custo dos Bens e/ou Servi√ßos Vendidos',\n",
    "            'Lucro Bruto', 'EBIT', 'EBT', 'Lucro L√≠quido Consolidado'\n",
    "        ]\n",
    "\n",
    "    def load_and_prepare_data(self) -> pd.DataFrame:\n",
    "        logger.info(\"üì• 1. CARREGANDO NOVO fundamentals_wide.parquet...\")\n",
    "        try:\n",
    "            self.fundamentals_df = pd.read_parquet(self.fundamentals_path)\n",
    "            logger.info(f\"‚úÖ Dados carregados: {self.fundamentals_df.shape}\")\n",
    "            logger.info(f\"Colunas encontradas: {self.fundamentals_df.columns.tolist()}\")\n",
    "            # Verificar se as novas colunas est√£o presentes\n",
    "            new_cols = ['Caixa e Equivalentes', 'D√≠vida Curto Prazo', 'D√≠vida Longo Prazo']\n",
    "            missing_new = [col for col in new_cols if col not in self.fundamentals_df.columns]\n",
    "            if missing_new:\n",
    "                 logger.error(f\"‚ùå ERRO: Novas colunas {missing_new} N√ÉO encontradas no input!\")\n",
    "                 raise ValueError(f\"Novas colunas faltando: {missing_new}\")\n",
    "            else:\n",
    "                 logger.info(\"‚úÖ Novas colunas (Caixa, D√≠vida CP, D√≠vida LP) encontradas!\")\n",
    "\n",
    "            self.fundamentals_df = self._clean_fundamentals_data(self.fundamentals_df)\n",
    "            return self.fundamentals_df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erro ao carregar dados: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _clean_fundamentals_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df_clean = df.copy()\n",
    "        df_clean['DT_FIM_EXERC'] = pd.to_datetime(df_clean['DT_FIM_EXERC'], errors='coerce')\n",
    "        df_clean = df_clean.dropna(subset=['DT_FIM_EXERC'])\n",
    "        df_clean = df_clean.drop_duplicates(subset=['CNPJ_CIA', 'DT_FIM_EXERC'])\n",
    "        df_clean = df_clean.sort_values(['CNPJ_CIA', 'DT_FIM_EXERC'])\n",
    "        return df_clean\n",
    "\n",
    "    def calculate_ttm_data(self) -> pd.DataFrame:\n",
    "        if self.fundamentals_df is None: raise ValueError(\"Dados n√£o carregados.\")\n",
    "        logger.info(\"‚è≥ 2. CALCULANDO TTM...\")\n",
    "        df_ttm = self.fundamentals_df.copy()\n",
    "        def rolling_sum_group(group):\n",
    "            for col in self.dre_cols:\n",
    "                if col in group.columns:\n",
    "                    quarterly = group[col] - group[col].shift(1).fillna(0)\n",
    "                    group[f'{col}_ttm'] = quarterly.rolling(window=4, min_periods=4).sum()\n",
    "                else: group[f'{col}_ttm'] = np.nan\n",
    "            return group\n",
    "        grouped = df_ttm.groupby('CNPJ_CIA', group_keys=False)\n",
    "        self.ratios_df = grouped.progress_apply(rolling_sum_group)\n",
    "        logger.info(f\"‚úÖ TTM calculado.\")\n",
    "        return self.ratios_df\n",
    "\n",
    "    def calculate_financial_ratios(self) -> pd.DataFrame:\n",
    "        \"\"\" PASSO 3: Calcula ratios (com ROIC correto e ratios de d√≠vida) \"\"\"\n",
    "        if self.ratios_df is None: raise ValueError(\"Dados TTM n√£o calculados.\")\n",
    "        logger.info(\"üßÆ 3. CALCULANDO RATIOS FINANCEIROS (COM TTM)...\")\n",
    "        df = self.ratios_df.copy()\n",
    "\n",
    "        def get_col(df, col_name): return df.get(col_name, np.nan)\n",
    "        def safe_divide(num, den): return np.where(den == 0, np.nan, num / den)\n",
    "\n",
    "        # --- Contas de D√≠vida (USANDO AS NOVAS COLUNAS) ---\n",
    "        divida_cp = get_col(df, 'D√≠vida Curto Prazo').fillna(0) # Usando a coluna correta\n",
    "        divida_lp = get_col(df, 'D√≠vida Longo Prazo').fillna(0) # Usando a coluna correta\n",
    "        caixa = get_col(df, 'Caixa e Equivalentes').fillna(0)   # Usando a coluna correta\n",
    "        pl = get_col(df, 'Patrim√¥nio L√≠quido Consolidado').fillna(0)\n",
    "        \n",
    "        df['D√≠vida Bruta'] = divida_cp + divida_lp\n",
    "        df['Capital Investido'] = df['D√≠vida Bruta'] + pl # Defini√ß√£o correta\n",
    "        df['D√≠vida L√≠quida'] = df['D√≠vida Bruta'] - caixa # Defini√ß√£o correta\n",
    "        logger.info(\"‚úÖ D√≠vida Bruta, Capital Investido e D√≠vida L√≠quida calculados.\")\n",
    "\n",
    "        # --- RENTABILIDADE ---\n",
    "        logger.info(\"üìà Calculando RENTABILIDADE...\")\n",
    "        df['ROE'] = safe_divide(get_col(df, 'Lucro L√≠quido Consolidado_ttm'), pl)\n",
    "        df['ROA'] = safe_divide(get_col(df, 'Lucro L√≠quido Consolidado_ttm'), get_col(df, 'Ativo Total'))\n",
    "        # <<< ROIC CORRETO >>>\n",
    "        df['ROIC'] = safe_divide(get_col(df, 'EBIT_ttm'), df['Capital Investido'])\n",
    "        logger.info(f\"‚úÖ ROIC (correto) calculado. M√©dia: {df['ROIC'].mean():.4f}\")\n",
    "        # <<< FIM ROIC CORRETO >>>\n",
    "\n",
    "        # --- MARGENS ---\n",
    "        df['MARGEM_EBIT'] = safe_divide(get_col(df, 'EBIT_ttm'), get_col(df, 'Receita L√≠quida_ttm'))\n",
    "        df['MARGEM_LIQUIDA'] = safe_divide(get_col(df, 'Lucro L√≠quido Consolidado_ttm'), get_col(df, 'Receita L√≠quida_ttm'))\n",
    "        df['MARGEM_BRUTA'] = safe_divide(get_col(df, 'Lucro Bruto_ttm'), get_col(df, 'Receita L√≠quida_ttm'))\n",
    "\n",
    "        # --- ALAVANCAGEM ---\n",
    "        logger.info(\"‚öñÔ∏è Calculando ALAVANCAGEM...\")\n",
    "        df['ALAVANCAGEM'] = safe_divide(get_col(df, 'Passivo Total'), get_col(df, 'Ativo Total'))\n",
    "        df['DIVIDA_PL'] = safe_divide(df['D√≠vida Bruta'], pl) # Agora pode ser calculado\n",
    "        df['DIVIDA_LIQ_EBIT'] = safe_divide(df['D√≠vida L√≠quida'], get_col(df, 'EBIT_ttm')) # Agora pode ser calculado\n",
    "        logger.info(\"‚úÖ Ratios de alavancagem (incluindo DIVIDA_PL, DIVIDA_LIQ_EBIT) calculados.\")\n",
    "\n",
    "        # --- LIQUIDEZ ---\n",
    "        df['LIQUIDEZ_CORRENTE'] = safe_divide(get_col(df, 'Ativo Circulante'), get_col(df, 'Passivo Circulante'))\n",
    "\n",
    "        # --- EFICI√äNCIA ---\n",
    "        df['GIRO_ATIVO'] = safe_divide(get_col(df, 'Receita L√≠quida_ttm'), get_col(df, 'Ativo Total'))\n",
    "\n",
    "        self.ratios_df = self._handle_ratio_outliers(df)\n",
    "        logger.info(\"üéØ RATIOS CALCULADOS e limpos.\")\n",
    "        logger.info(f\"Colunas FINAIS de Ratios: {self.ratios_df.columns.tolist()}\")\n",
    "        return self.ratios_df\n",
    "\n",
    "    def _handle_ratio_outliers(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df_clean = df.copy()\n",
    "        # Lista completa agora que temos os dados\n",
    "        ratio_columns = [\n",
    "            'ROE', 'ROA', 'ROIC', 'MARGEM_EBIT', 'MARGEM_LIQUIDA', 'MARGEM_BRUTA',\n",
    "            'ALAVANCAGEM', 'DIVIDA_PL', 'DIVIDA_LIQ_EBIT', 'LIQUIDEZ_CORRENTE', 'GIRO_ATIVO'\n",
    "        ]\n",
    "        for col in ratio_columns:\n",
    "            if col in df_clean.columns:\n",
    "                df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
    "                if df_clean[col].notna().sum() > 0:\n",
    "                    lower = df_clean[col].quantile(0.01)\n",
    "                    upper = df_clean[col].quantile(0.99)\n",
    "                    df_clean[col] = df_clean[col].clip(lower=lower, upper=upper)\n",
    "        return df_clean\n",
    "\n",
    "    def calculate_quality_scores(self) -> pd.DataFrame:\n",
    "        \"\"\" PASSO 4: Calcula scores individuais e score composto \"\"\"\n",
    "        if self.ratios_df is None: raise ValueError(\"Ratios n√£o calculados.\")\n",
    "        logger.info(\"üéØ 4. CALCULANDO SCORES DE QUALIDADE...\")\n",
    "        scores_df = self.ratios_df.copy()\n",
    "        grouped_by_date = scores_df.groupby('DT_FIM_EXERC')\n",
    "\n",
    "        # Configura√ß√£o original (ou ajuste como preferir)\n",
    "        metrics_config = {\n",
    "            'ROIC': {'direction': 1, 'weight': 0.25}, # ROIC correto\n",
    "            'ROE': {'direction': 1, 'weight': 0.15},\n",
    "            'MARGEM_EBIT': {'direction': 1, 'weight': 0.15},\n",
    "            'MARGEM_LIQUIDA': {'direction': 1, 'weight': 0.10},\n",
    "            'DIVIDA_LIQ_EBIT': {'direction': -1, 'weight': 0.15}, # Agora existe\n",
    "            'LIQUIDEZ_CORRENTE': {'direction': 1, 'weight': 0.10},\n",
    "            'GIRO_ATIVO': {'direction': 1, 'weight': 0.10},\n",
    "            # 'ALAVANCAGEM': {'direction': -1, 'weight': 0.0}, # Pode remover ou ajustar peso\n",
    "        }\n",
    "        # Validar e ajustar pesos para somar 1.0\n",
    "        total_w = sum(c['weight'] for c in metrics_config.values())\n",
    "        if abs(total_w - 1.0) > 0.01:\n",
    "             logger.warning(f\"Soma dos pesos √© {total_w:.2f}. Ajustando proporcionalmente...\")\n",
    "             for k in metrics_config: metrics_config[k]['weight'] /= total_w\n",
    "\n",
    "        logger.info(f\"Configura√ß√£o de m√©tricas para score: { {k: v['weight'] for k, v in metrics_config.items()} }\")\n",
    "\n",
    "        for metric, config in metrics_config.items():\n",
    "            if metric in scores_df.columns and scores_df[metric].notna().any():\n",
    "                score_col = f'score_{metric}'\n",
    "                if config['direction'] == 1:\n",
    "                    scores_df[score_col] = grouped_by_date[metric].rank(pct=True) * 100\n",
    "                else:\n",
    "                    scores_df[score_col] = grouped_by_date[metric].rank(ascending=False, pct=True) * 100\n",
    "                logger.info(f\"  ‚úÖ Score {metric} calculado.\")\n",
    "            else:\n",
    "                 logger.warning(f\"M√©trica '{metric}' n√£o encontrada ou sem dados para score.\")\n",
    "\n",
    "        logger.info(\"‚öñÔ∏è Calculando score composto...\")\n",
    "        scores_df['aurum_quality_score'] = 0.0\n",
    "        total_applied_weight = 0.0\n",
    "        for metric, config in metrics_config.items():\n",
    "            score_col = f'score_{metric}'\n",
    "            if score_col in scores_df.columns:\n",
    "                scores_df['aurum_quality_score'] += scores_df[score_col].fillna(50) * config['weight']\n",
    "                total_applied_weight += config['weight']\n",
    "\n",
    "        if total_applied_weight > 0:\n",
    "            scores_df['aurum_quality_score'] /= total_applied_weight\n",
    "\n",
    "        # ... (c√≥digo de classifica√ß√£o e restante igual ao anterior) ...\n",
    "        logger.info(\"üèÜ Classificando empresas...\")\n",
    "        try:\n",
    "             valid_scores = scores_df['aurum_quality_score'].dropna()\n",
    "             if not valid_scores.empty:\n",
    "                  ranks = valid_scores.rank(ascending=False, pct=True)\n",
    "                  scores_df['final_rank'] = ranks\n",
    "                  try:\n",
    "                       scores_df['quality_quintile'] = pd.qcut(scores_df['final_rank'].dropna(), 5, labels=[f'{i}¬∫ Quintil' for i in range(1, 6)])\n",
    "                  except ValueError: scores_df['quality_quintile'] = pd.cut(scores_df['final_rank'].dropna(), 5, labels=False)\n",
    "                  scores_df['quality_grade'] = pd.cut(scores_df['final_rank'].dropna(), bins=[0, 0.2, 0.4, 0.6, 0.8, 1.0], labels=['A', 'B', 'C', 'D', 'E'], right=True, include_lowest=True)\n",
    "             else: scores_df[['final_rank', 'quality_quintile', 'quality_grade']] = np.nan\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Erro ao calcular ranks/grades: {e}\")\n",
    "             scores_df[['final_rank', 'quality_quintile', 'quality_grade']] = np.nan\n",
    "        \n",
    "        self.scores_df = scores_df\n",
    "        logger.info(f\"Colunas FINAIS ANTES de salvar: {self.scores_df.columns.tolist()}\")\n",
    "        return scores_df\n",
    "\n",
    "    # M√©todos get_latest_scores, save_results, _save_statistics s√£o iguais aos anteriores\n",
    "    # ... (Copie e cole os m√©todos get_latest_scores, save_results, _save_statistics da vers√£o anterior aqui) ...\n",
    "    def get_latest_scores(self) -> pd.DataFrame:\n",
    "        if self.scores_df is None: raise ValueError(\"Scores n√£o calculados.\")\n",
    "        latest_scores = self.scores_df.sort_values('DT_FIM_EXERC').groupby('CNPJ_CIA').last().reset_index()\n",
    "        return latest_scores\n",
    "\n",
    "    def save_results(self, output_dir: str = \"data/aurum_scores\"):\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        if self.scores_df is not None:\n",
    "            logger.info(f\"Salvando DataFrame com colunas: {self.scores_df.columns.tolist()}\")\n",
    "            if 'ROIC' not in self.scores_df.columns: logger.error(\"ERRO FATAL: Coluna 'ROIC' AUSENTE antes de salvar!\")\n",
    "            scores_path = output_path / \"aurum_quality_scores_complete.parquet\"\n",
    "            self.scores_df.to_parquet(scores_path, index=False)\n",
    "            logger.info(f\"üíæ Scores completos (hist√≥rico) salvos: {scores_path}\")\n",
    "        latest_scores = self.get_latest_scores()\n",
    "        latest_path = output_path / \"aurum_quality_scores_latest.parquet\"\n",
    "        latest_csv_path = output_path / \"aurum_quality_scores_latest.csv\"\n",
    "        latest_scores.to_parquet(latest_path, index=False)\n",
    "        latest_scores.to_csv(latest_csv_path, index=False, sep=';', encoding='utf-8-sig')\n",
    "        logger.info(f\"üíæ Scores mais recentes salvos: {latest_path} e {latest_csv_path}\")\n",
    "        stats_path = output_path / \"aurum_scores_statistics.txt\"\n",
    "        self._save_statistics(latest_scores, stats_path)\n",
    "        logger.info(f\"üíæ Estat√≠sticas salvas: {stats_path}\")\n",
    "        return {'complete_scores': scores_path,'latest_scores': latest_path,'statistics': stats_path}\n",
    "\n",
    "    def _save_statistics(self, scores_df: pd.DataFrame, stats_path: Path):\n",
    "        with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"AURUM QUALITY SCORE - ESTAT√çSTICAS (√öLTIMO PER√çODO)\\n\" + \"=\" * 50 + \"\\n\\n\")\n",
    "            f.write(f\"Total de empresas: {len(scores_df)}\\n\")\n",
    "            max_date = scores_df['DT_FIM_EXERC'].max()\n",
    "            f.write(f\"Per√≠odo mais recente: {max_date if pd.notna(max_date) else 'N/A'}\\n\\n\")\n",
    "            if 'aurum_quality_score' in scores_df.columns:\n",
    "                 f.write(\"DISTRIBUI√á√ÉO DOS SCORES:\\n\" + f\"  M√©dia: {scores_df['aurum_quality_score'].mean():.2f}\\n\" +\n",
    "                         f\"  Mediana: {scores_df['aurum_quality_score'].median():.2f}\\n\" + f\"  M√≠nimo: {scores_df['aurum_quality_score'].min():.2f}\\n\" +\n",
    "                         f\"  M√°ximo: {scores_df['aurum_quality_score'].max():.2f}\\n\\n\")\n",
    "            else: f.write(\"DISTRIBUI√á√ÉO DOS SCORES: N/A\\n\\n\")\n",
    "            if 'quality_grade' in scores_df.columns:\n",
    "                 f.write(\"DISTRIBUI√á√ÉO POR NOTAS:\\n\")\n",
    "                 grade_counts = scores_df['quality_grade'].value_counts().sort_index(ascending=True)\n",
    "                 for grade, count in grade_counts.items(): f.write(f\"  Nota {grade}: {count} empresas\\n\")\n",
    "            else: f.write(\"DISTRIBUI√á√ÉO POR NOTAS: N/A\\n\")\n",
    "            if 'aurum_quality_score' in scores_df.columns:\n",
    "                 f.write(\"\\nTOP 10 EMPRESAS:\\n\")\n",
    "                 top_10 = scores_df.nlargest(10, 'aurum_quality_score')[['DENOM_CIA', 'aurum_quality_score', 'quality_grade']]\n",
    "                 for i, (_, row) in enumerate(top_10.iterrows(), 1):\n",
    "                      grade = row.get('quality_grade', 'N/A')\n",
    "                      f.write(f\"  {i:2d}. {str(row['DENOM_CIA'])[:35]:35} {row['aurum_quality_score']:6.2f} (Nota {grade})\\n\")\n",
    "            else: f.write(\"\\nTOP 10 EMPRESAS: N/A\\n\")\n",
    "\n",
    "\n",
    "# ==================== EXECU√á√ÉO PRINCIPAL ====================\n",
    "def main():\n",
    "    logger.info(\"üöÄ INICIANDO PIPELINE DO AURUM QUALITY SCORE (Vers√£o Final com ROIC Correto)\")\n",
    "    try:\n",
    "        calculator = AurumQualityScoreCalculator(\n",
    "            fundamentals_path=\"data/cvm/final/fundamentals_wide.parquet\" # Ler o NOVO input\n",
    "        )\n",
    "        calculator.load_and_prepare_data()\n",
    "        calculator.calculate_ttm_data()\n",
    "        calculator.calculate_financial_ratios() # Calcular ROIC correto e ratios de d√≠vida\n",
    "        calculator.calculate_quality_scores()   # Usar ROIC correto e ratios de d√≠vida\n",
    "        output_files = calculator.save_results(output_dir=\"data/aurum_scores\") # Salvar no diret√≥rio correto\n",
    "        logger.info(f\"üíæ Resultados (com ROIC correto) salvos em: {output_files}\")\n",
    "\n",
    "        latest_scores = calculator.get_latest_scores()\n",
    "        # ... (impress√£o dos resultados igual) ...\n",
    "        print(\"\\n\" + \"=\"*60 + \"\\nüéâ AURUM QUALITY SCORE - RESULTADOS FINAIS (√öLTIMO PER√çODO)\\n\" + \"=\"*60)\n",
    "        print(f\"\\nüìä TOTAL DE EMPRESAS: {len(latest_scores)}\")\n",
    "        if 'aurum_quality_score' in latest_scores.columns: print(f\"üìà SCORE M√âDIO: {latest_scores['aurum_quality_score'].mean():.2f}\")\n",
    "        print(f\"\\nüìã DISTRIBUI√á√ÉO DAS NOTAS:\")\n",
    "        if 'quality_grade' in latest_scores.columns:\n",
    "             grade_dist = latest_scores['quality_grade'].value_counts().sort_index(ascending=True)\n",
    "             for grade, count in grade_dist.items(): print(f\"   Nota {grade}: {count:3d} empresas\")\n",
    "        print(f\"\\nü•á TOP 10 EMPRESAS:\")\n",
    "        if 'aurum_quality_score' in latest_scores.columns:\n",
    "             top_10 = latest_scores.nlargest(10, 'aurum_quality_score')[['DENOM_CIA', 'aurum_quality_score', 'quality_grade']]\n",
    "             for i, (_, row) in enumerate(top_10.iterrows(), 1):\n",
    "                  grade = row.get('quality_grade', 'N/A')\n",
    "                  print(f\"   {i:2d}. {str(row['DENOM_CIA'])[:35]:35} {row['aurum_quality_score']:6.2f} (Nota {grade})\")\n",
    "\n",
    "        return calculator\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå ERRO NO PIPELINE: {e}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    aurum_calculator = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87842566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Informa√ß√µes do DataFrame Mestre Gerado ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 19581 entries, 2232 to 34531\n",
      "Data columns (total 57 columns):\n",
      " #   Column                                     Non-Null Count  Dtype         \n",
      "---  ------                                     --------------  -----         \n",
      " 0   date                                       19581 non-null  datetime64[ns]\n",
      " 1   ticker                                     19581 non-null  object        \n",
      " 2   Adj Close                                  19581 non-null  float64       \n",
      " 3   CNPJ_CIA                                   19581 non-null  object        \n",
      " 4   DENOM_CIA                                  19581 non-null  object        \n",
      " 5   Custo dos Bens e/ou Servi√ßos Vendidos      19581 non-null  float64       \n",
      " 6   EBIT                                       19581 non-null  float64       \n",
      " 7   EBT                                        19581 non-null  float64       \n",
      " 8   Lucro Bruto                                19581 non-null  float64       \n",
      " 9   Lucro L√≠quido Consolidado                  19581 non-null  float64       \n",
      " 10  Receita L√≠quida                            19581 non-null  float64       \n",
      " 11  Ativo Circulante                           19581 non-null  float64       \n",
      " 12  Ativo N√£o Circulante                       19581 non-null  float64       \n",
      " 13  Ativo Total                                19581 non-null  float64       \n",
      " 14  Caixa e Equivalentes                       19581 non-null  float64       \n",
      " 15  D√≠vida Curto Prazo                         19581 non-null  float64       \n",
      " 16  D√≠vida Longo Prazo                         19581 non-null  float64       \n",
      " 17  Passivo Circulante                         19581 non-null  float64       \n",
      " 18  Passivo N√£o Circulante                     19581 non-null  float64       \n",
      " 19  Passivo Total                              19581 non-null  float64       \n",
      " 20  Patrim√¥nio L√≠quido Consolidado             19581 non-null  float64       \n",
      " 21  Receita L√≠quida_ttm                        19581 non-null  float64       \n",
      " 22  Custo dos Bens e/ou Servi√ßos Vendidos_ttm  19581 non-null  float64       \n",
      " 23  Lucro Bruto_ttm                            19581 non-null  float64       \n",
      " 24  EBIT_ttm                                   19581 non-null  float64       \n",
      " 25  EBT_ttm                                    19581 non-null  float64       \n",
      " 26  Lucro L√≠quido Consolidado_ttm              19581 non-null  float64       \n",
      " 27  D√≠vida Bruta                               19581 non-null  float64       \n",
      " 28  Capital Investido                          19581 non-null  float64       \n",
      " 29  D√≠vida L√≠quida                             19581 non-null  float64       \n",
      " 30  ROE                                        19581 non-null  float64       \n",
      " 31  ROA                                        19581 non-null  float64       \n",
      " 32  ROIC                                       19581 non-null  float64       \n",
      " 33  MARGEM_EBIT                                18798 non-null  float64       \n",
      " 34  MARGEM_LIQUIDA                             18798 non-null  float64       \n",
      " 35  MARGEM_BRUTA                               18798 non-null  float64       \n",
      " 36  ALAVANCAGEM                                19581 non-null  float64       \n",
      " 37  DIVIDA_PL                                  19581 non-null  float64       \n",
      " 38  DIVIDA_LIQ_EBIT                            19581 non-null  float64       \n",
      " 39  LIQUIDEZ_CORRENTE                          19362 non-null  float64       \n",
      " 40  GIRO_ATIVO                                 19581 non-null  float64       \n",
      " 41  score_ROIC                                 19581 non-null  float64       \n",
      " 42  score_ROE                                  19581 non-null  float64       \n",
      " 43  score_MARGEM_EBIT                          18798 non-null  float64       \n",
      " 44  score_MARGEM_LIQUIDA                       18798 non-null  float64       \n",
      " 45  score_DIVIDA_LIQ_EBIT                      19581 non-null  float64       \n",
      " 46  score_LIQUIDEZ_CORRENTE                    19362 non-null  float64       \n",
      " 47  score_GIRO_ATIVO                           19581 non-null  float64       \n",
      " 48  aurum_quality_score                        19581 non-null  float64       \n",
      " 49  final_rank                                 19581 non-null  float64       \n",
      " 50  quality_quintile                           19581 non-null  category      \n",
      " 51  quality_grade                              19581 non-null  category      \n",
      " 52  ticker_simple                              19581 non-null  object        \n",
      " 53  VOLATILIDADE                               19581 non-null  float64       \n",
      " 54  SENTIMENT_MEDIO                            19581 non-null  float64       \n",
      " 55  SENTIMENT_STD                              0 non-null      float64       \n",
      " 56  NEWS_COUNT                                 0 non-null      float64       \n",
      "dtypes: category(2), datetime64[ns](1), float64(50), object(4)\n",
      "memory usage: 8.4+ MB\n",
      "\n",
      "--- Amostra do DataFrame Mestre ---\n",
      "            date    ticker       ROE      ROIC  SENTIMENT_MEDIO  VOLATILIDADE\n",
      "17883 2018-09-01  CXSE3.SA  0.090589  0.105510              0.0      0.023938\n",
      "8144  2014-06-30  MGLU3.SA -0.011675  0.011586              0.0      0.025622\n",
      "33673 2025-06-30  PETR3.SA  0.154574  0.095068              0.0      0.019893\n",
      "32251 2024-11-01  INTB3.SA  0.013896  0.013817              0.0      0.016475\n",
      "18283 2018-11-01  EGIE3.SA  0.038368  0.048932              0.0      0.012728\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Configura√ß√£o ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Arquivos de Input (VERIFICADOS) ---\n",
    "PATH_PRECOS_LIMPOS = \"data/historical/all_histories_cleaned.parquet\"\n",
    "PATH_PRECOS_WIDE = \"data/historical/prices_close_wide.parquet\"\n",
    "PATH_FUNDAMENTOS = \"data/aurum_scores/aurum_quality_scores_complete.parquet\" # Seu arquivo de fundamentos com ROIC\n",
    "PATH_SENTIMENTO = \"data/news/news_with_sentiment.parquet\"\n",
    "PATH_DE_PARA = \"data/ticker_cnpj_map.parquet\" # Seu arquivo de mapeamento\n",
    "\n",
    "# --- Arquivo de Output ---\n",
    "OUTPUT_DIR = \"data\"\n",
    "OUTPUT_FILENAME = \"aurum_master_features.parquet\"\n",
    "output_path = Path(OUTPUT_DIR) / OUTPUT_FILENAME\n",
    "\n",
    "def calcular_volatilidade_mensal():\n",
    "    # ... (c√≥digo igual, sem altera√ß√µes) ...\n",
    "    logger.info(\"Iniciando Unifica√ß√£o (Passo 1/3): C√°lculo da Volatilidade...\")\n",
    "    try: df_prices_daily = pd.read_parquet(PATH_PRECOS_LIMPOS)\n",
    "    except FileNotFoundError: logger.error(f\"Arquivo n√£o encontrado: {PATH_PRECOS_LIMPOS}\"); return None\n",
    "    df_prices_daily['date'] = pd.to_datetime(df_prices_daily['date'])\n",
    "    df_prices_daily['returns'] = df_prices_daily.groupby('ticker')['Adj Close'].pct_change()\n",
    "    df_prices_daily['VOLATILIDADE'] = df_prices_daily.groupby('ticker')['returns'].rolling(window=63, min_periods=30).std().reset_index(0, drop=True)\n",
    "    df_vol_mensal = df_prices_daily.set_index('date').groupby('ticker').resample('M').last()['VOLATILIDADE'].reset_index()\n",
    "    logger.info(f\"‚úÖ Volatilidade mensal calculada.\")\n",
    "    return df_vol_mensal\n",
    "\n",
    "def agregar_sentimento_mensal():\n",
    "    # ... (c√≥digo igual, sem altera√ß√µes) ...\n",
    "    logger.info(\"Iniciando Unifica√ß√£o (Passo 2/3): Agrega√ß√£o de Sentimento...\")\n",
    "    try: df_sent_raw = pd.read_parquet(PATH_SENTIMENTO)\n",
    "    except FileNotFoundError: logger.error(f\"Arquivo n√£o encontrado: {PATH_SENTIMENTO}\"); return None\n",
    "    df_sent_raw = df_sent_raw.rename(columns={'ticker_query': 'ticker', 'published_date': 'date'})\n",
    "    df_sent_raw['date'] = pd.to_datetime(df_sent_raw['date'], utc=True).dt.tz_localize(None)\n",
    "    if not df_sent_raw['ticker'].str.contains('.SA').any():\n",
    "        logger.warning(\"Tickers sem sufixo .SA. Adicionando...\")\n",
    "        df_sent_raw['ticker'] = df_sent_raw['ticker'].apply(lambda x: f\"{x}.SA\" if not str(x).endswith(\".SA\") else x)\n",
    "    df_sent_mensal = df_sent_raw.set_index('date').groupby('ticker').resample('M').agg(\n",
    "        SENTIMENT_MEDIO=('numeric_sentiment', 'mean'),\n",
    "        SENTIMENT_STD=('numeric_sentiment', 'std'),\n",
    "        NEWS_COUNT=('ticker', 'count')\n",
    "    ).reset_index()\n",
    "    logger.info(f\"‚úÖ Sentimento mensal agregado.\")\n",
    "    return df_sent_mensal\n",
    "\n",
    "def unificar_dataframe_mestre(df_vol_mensal, df_sent_mensal):\n",
    "    if df_vol_mensal is None or df_sent_mensal is None: return\n",
    "    logger.info(\"Iniciando Unifica√ß√£o (Passo 3/3): Jun√ß√£o do DataFrame Mestre...\")\n",
    "\n",
    "    # --- 1. Carregar Base de Pre√ßos Mensal ---\n",
    "    try:\n",
    "        df_close_wide = pd.read_parquet(PATH_PRECOS_WIDE)\n",
    "        df_base_mensal = df_close_wide.melt(ignore_index=False, var_name='ticker', value_name='Adj Close').reset_index()\n",
    "        df_base_mensal = df_base_mensal.rename(columns={'index': 'date'})\n",
    "        logger.info(f\"Base de pre√ßos (wide) carregada.\")\n",
    "    except FileNotFoundError: logger.error(f\"Arquivo n√£o encontrado: {PATH_PRECOS_WIDE}\"); return\n",
    "\n",
    "    # --- 2. Carregar Fundamentos (Trimestrais) ---\n",
    "    try:\n",
    "        df_fund = pd.read_parquet(PATH_FUNDAMENTOS)\n",
    "        df_fund = df_fund.rename(columns={'DT_FIM_EXERC': 'date'})\n",
    "        df_fund['date'] = pd.to_datetime(df_fund['date'])\n",
    "        logger.info(f\"Fundamentos carregados. Colunas: {df_fund.columns.tolist()}\")\n",
    "    except FileNotFoundError: logger.error(f\"Arquivo n√£o encontrado: {PATH_FUNDAMENTOS}\"); return\n",
    "    except KeyError as e: logger.error(f\"Erro ao renomear 'DT_FIM_EXERC': {e}\"); return\n",
    "\n",
    "    # --- 3. Carregar o Mapeamento (DE-PARA) ---\n",
    "    try:\n",
    "        df_mapping = pd.read_parquet(PATH_DE_PARA)\n",
    "        if 'ticker' not in df_mapping.columns or 'CNPJ_CIA' not in df_mapping.columns:\n",
    "             logger.error(f\"ERRO: Mapeamento {PATH_DE_PARA} sem 'ticker' ou 'CNPJ_CIA'.\")\n",
    "             return\n",
    "    except FileNotFoundError: logger.error(f\"Arquivo n√£o encontrado: {PATH_DE_PARA}\"); return\n",
    "\n",
    "    df_fund_com_ticker = pd.merge(df_fund, df_mapping, on='CNPJ_CIA', how='left')\n",
    "    df_fund_com_ticker = df_fund_com_ticker.dropna(subset=['ticker'])\n",
    "    logger.info(\"Fundamentos mapeados para tickers.\")\n",
    "\n",
    "    # --- 4. Construir o DataFrame Mestre ---\n",
    "    df_master = df_base_mensal.sort_values(by='date')\n",
    "    df_fund_com_ticker = df_fund_com_ticker.sort_values(by='date')\n",
    "\n",
    "    # 4a. Juntar Fundamentos (merge_asof)\n",
    "    df_master = pd.merge_asof(\n",
    "        df_master, df_fund_com_ticker, on='date', by='ticker', direction='backward'\n",
    "    )\n",
    "    logger.info(\"Merge 'as-of' dos fundamentos conclu√≠do.\")\n",
    "    logger.info(f\"Colunas no df_master AP√ìS merge_asof: {df_master.columns.tolist()}\") # DEBUG\n",
    "\n",
    "    # 4b. Juntar Volatilidade (merge)\n",
    "    df_master = pd.merge(df_master, df_vol_mensal, on=['date', 'ticker'], how='left')\n",
    "    logger.info(\"Merge da volatilidade conclu√≠do.\")\n",
    "\n",
    "    # 4c. Juntar Sentimento (merge)\n",
    "    df_master = pd.merge(df_master, df_sent_mensal, on=['date', 'ticker'], how='left')\n",
    "    logger.info(\"Merge do sentimento conclu√≠do.\")\n",
    "\n",
    "    # --- 5. Limpeza Final ---\n",
    "    df_master['SENTIMENT_MEDIO'] = df_master['SENTIMENT_MEDIO'].fillna(0)\n",
    "    df_master['VOLATILIDADE'] = df_master.groupby('ticker')['VOLATILIDADE'].ffill().bfill()\n",
    "\n",
    "    # <<< VERS√ÉO FINAL DO dropna >>>\n",
    "    # Usando ambas as colunas, pois df_master.info() provou que elas existem\n",
    "    fundamental_key_columns = ['ROE', 'ROIC'] # <--- USANDO AMBAS AS COLUNAS\n",
    "\n",
    "    missing_cols = [col for col in fundamental_key_columns if col not in df_master.columns]\n",
    "    if missing_cols:\n",
    "        # Este erro n√£o deve mais acontecer baseado no seu df.info()\n",
    "        logger.error(f\"ERRO CR√çTICO P√ìS-MERGE: Colunas {missing_cols} n√£o encontradas!\")\n",
    "        return\n",
    "    else:\n",
    "        logger.info(f\"Aplicando dropna nas colunas chave: {fundamental_key_columns}\")\n",
    "        df_master = df_master.dropna(subset=fundamental_key_columns)\n",
    "    # <<< FIM DA VERS√ÉO FINAL >>>\n",
    "\n",
    "    logger.info(f\"Limpeza final conclu√≠da. DataFrame Mestre pronto com {len(df_master)} linhas.\")\n",
    "\n",
    "    # --- 6. Salvar o Novo DataFrame Mestre ---\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    df_master.to_parquet(output_path, index=False)\n",
    "    logger.info(f\"‚úÖ‚úÖ‚úÖ DataFrame Mestre salvo em: {output_path} ‚úÖ‚úÖ‚úÖ\")\n",
    "\n",
    "    print(\"\\n--- Informa√ß√µes do DataFrame Mestre Gerado ---\")\n",
    "    df_master.info(verbose=True, show_counts=True) # Mostrar detalhes\n",
    "    print(\"\\n--- Amostra do DataFrame Mestre ---\")\n",
    "    sample_cols = ['date', 'ticker', 'ROE', 'ROIC', 'SENTIMENT_MEDIO', 'VOLATILIDADE']\n",
    "    print(df_master.sample(5)[[col for col in sample_cols if col in df_master.columns]])\n",
    "\n",
    "    return df_master\n",
    "\n",
    "# --- Execu√ß√£o Principal ---\n",
    "if __name__ == \"__main__\":\n",
    "    df_vol = calcular_volatilidade_mensal()\n",
    "    df_sent = agregar_sentimento_mensal()\n",
    "    if df_vol is not None and df_sent is not None:\n",
    "        unificar_dataframe_mestre(df_vol, df_sent)\n",
    "    else:\n",
    "        logger.error(\"Falha ao gerar dados. O DataFrame Mestre n√£o foi criado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809068c7",
   "metadata": {},
   "source": [
    "### **Documenta√ß√£o:** Sistema de Scoring Avan√ßado (AurumScoringSystem)\n",
    "\n",
    "#### 1. Objetivo\n",
    "\n",
    "Este script √© o \"c√©rebro\" do projeto Aurum, onde a tese de investimento quantitativo √© de fato implementada. Ele representa a **evolu√ß√£o** do `AurumQualityScoreCalculator`, aplicando uma metodologia de scoring mais robusta e academicamente embasada.\n",
    "\n",
    "Enquanto o script anterior (`Calculator`) era focado em *calcular ratios* e criar um *score de ranking simples*, este script (`System`) tem como responsabilidades:\n",
    "1.  **Carregar uma Tese:** Define um conjunto expl√≠cito de **m√©tricas e pesos** baseados em teorias financeiras (ex: Fama & French, Graham & Dodd), armazenados na classe `ScoringMetric`.\n",
    "2.  **Normaliza√ß√£o Avan√ßada:** Substitui a normaliza√ß√£o por ranking (percentil) por uma **normaliza√ß√£o sigm√≥ide (baseada em Z-Score)**. Isso cria um score mais suave, robusto a outliers e que recompensa melhor empresas excepcionais.\n",
    "3.  **Incorporar Fatores M√∫ltiplos:** O sistema √© projetado para consumir *n√£o apenas* os ratios financeiros do script anterior, mas tamb√©m m√©tricas de **crescimento**, **sentimento (NLP)** e **volatilidade (pre√ßo)**.\n",
    "4.  **Validar-se:** Gera um relat√≥rio de valida√ß√£o que compara a contribui√ß√£o *te√≥rica* de cada m√©trica (o peso que definimos) com sua contribui√ß√£o *real* no score final.\n",
    "5.  **Gerar o Score Final:** Calcula o `aurum_quality_score` final e as classifica√ß√µes (A, B, C, D, E).\n",
    "\n",
    "#### 2. Configura√ß√£o (Input)\n",
    "\n",
    "Este script √© o **segundo passo** no pipeline de scoring e depende da sa√≠da do script anterior.\n",
    "\n",
    "1.  **Input Principal (Obrigat√≥rio):**\n",
    "    * `data/aurum_scores/aurum_quality_scores_complete.parquet`: Este √© o arquivo de **output** gerado pelo `AurumQualityScoreCalculator`. Ele cont√©m todos os ratios hist√≥ricos (ROE, ROIC, etc.) j√° calculados e tratados.\n",
    "\n",
    "2.  **Input Opcional (Configura√ß√£o):**\n",
    "    * O construtor `AurumScoringSystem(config_path=\"...\")` aceita um caminho para um arquivo `.json`. Isso permite carregar um conjunto personalizado de m√©tricas e pesos sem alterar o c√≥digo, facilitando a experimenta√ß√£o. Se nenhum caminho for fornecido, ele usa os pesos padr√£o definidos em `_initialize_scoring_metrics`.\n",
    "\n",
    "#### 3. Sa√≠da (Output)\n",
    "\n",
    "O script cria um novo diret√≥rio: `data/aurum_final_scores/`.\n",
    "\n",
    "1.  **`aurum_advanced_scores.parquet`**:\n",
    "    * O **hist√≥rico completo** de todas as empresas com os scores finais (0-100), notas (A-E) e quintis. Este √© o arquivo final que ser√° usado para o **backtesting**.\n",
    "\n",
    "2.  **`aurum_latest_advanced_scores.parquet`**:\n",
    "    * Um snapshot contendo apenas o **√∫ltimo score dispon√≠vel** para cada empresa.\n",
    "\n",
    "3.  **`aurum_scoring_config.json`**:\n",
    "    * Um arquivo `.json` que salva a tese exata (m√©tricas e pesos) usada nesta execu√ß√£o, garantindo a reprodutibilidade dos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cea5b758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:‚ùå Arquivo de ratios n√£o encontrado: data/aurum_master_features.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç AN√ÅLISE DE IMPORT√ÇNCIA DAS M√âTRICAS\n",
      "==================================================\n",
      "\n",
      "üìä RENTABILIDADE: 47.0%\n",
      "   ‚Ä¢ ROE                   18.0% - Return on Equity - Efici√™ncia do capital pr√≥prio\n",
      "   ‚Ä¢ ROA                   12.0% - Return on Assets - Efici√™ncia dos ativos\n",
      "   ‚Ä¢ MARGEM_EBIT            8.0% - Margem Operacional - Ebit/Receita\n",
      "   ‚Ä¢ MARGEM_LIQUIDA         5.0% - Margem L√≠quida - Lucro/Receita\n",
      "   ‚Ä¢ MARGEM_BRUTA           4.0% - Margem Bruta - Lucro Bruto/Receita\n",
      "\n",
      "üìä SOLV√äNCIA: 33.0%\n",
      "   ‚Ä¢ ALAVANCAGEM            9.0% - Alavancagem Total - Passivo/Ativo\n",
      "   ‚Ä¢ DIVIDA_PL              9.0% - D√≠vida/Patrim√¥nio L√≠quido\n",
      "   ‚Ä¢ LIQUIDEZ_CORRENTE      8.0% - Liquidez Corrente - Ativo Circulante/Passivo Circulante\n",
      "   ‚Ä¢ GIRO_ATIVO             7.0% - Giro do Ativo - Receita/Ativo Total\n",
      "\n",
      "üìä CRESCIMENTO: 10.0%\n",
      "   ‚Ä¢ CRESC_RECEITA          5.0% - Crescimento da Receita (anual)\n",
      "   ‚Ä¢ CRESC_LUCRO            5.0% - Crescimento do Lucro L√≠quido (anual)\n",
      "\n",
      "üìä EFICI√äNCIA: 10.0%\n",
      "   ‚Ä¢ SENTIMENT_MEDIO        5.0% - Sentimento M√©dio de Not√≠cias\n",
      "   ‚Ä¢ VOLATILIDADE           5.0% - Volatilidade dos Retornos (3 meses)\n",
      "\n",
      "üìà SOMA TOTAL: 100.0%\n",
      "\n",
      "üéØ PR√ìXIMOS PASSOS SUGERIDOS:\n",
      "   1. Analisar correla√ß√£o entre scores e performance futura\n",
      "   2. Ajustar pesos baseado em backtesting hist√≥rico\n",
      "   3. Implementar ajustes setoriais espec√≠ficos\n",
      "   4. Criar dashboard de monitoramento dos scores\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configura√ß√£o de logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class ScoringMetric:\n",
    "    \"\"\"Classe para representar uma m√©trica de scoring\"\"\"\n",
    "    name: str\n",
    "    weight: float\n",
    "    direction: int  # 1 = maior √© melhor, -1 = menor √© melhor\n",
    "    description: str\n",
    "    min_value: float = None\n",
    "    max_value: float = None\n",
    "    ideal_range: Tuple[float, float] = None\n",
    "\n",
    "class AurumScoringSystem:\n",
    "    \"\"\"\n",
    "    Sistema avan√ßado de scoring para o Aurum Quality Score\n",
    "    com pesos baseados em fundamentos financeiros\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: str = None):\n",
    "        self.scoring_metrics = self._initialize_scoring_metrics(config_path)\n",
    "        self.quality_thresholds = {\n",
    "            'A': 80,  # Excelente\n",
    "            'B': 60,  # Bom\n",
    "            'C': 40,  # Regular\n",
    "            'D': 20,  # Ruim\n",
    "            'E': 0    # Muito ruim\n",
    "        }\n",
    "        \n",
    "    def _initialize_scoring_metrics(self, config_path: str = None) -> Dict[str, ScoringMetric]:\n",
    "        \"\"\"\n",
    "        Inicializa as m√©tricas de scoring com pesos baseados em:\n",
    "        - Graham & Dodd: Security Analysis\n",
    "        - Fama & French: Three Factor Model  \n",
    "        - Pr√°ticas do mercado quantitativo\n",
    "        \"\"\"\n",
    "        \n",
    "        if config_path and Path(config_path).exists():\n",
    "            return self._load_custom_config(config_path)\n",
    "        \n",
    "        # Pesos baseados em import√¢ncia relativa para qualidade de empresas\n",
    "        # CORRE√á√ÉO: Soma total = 1.0 (100%)\n",
    "        metrics_config = {\n",
    "            # === RENTABILIDADE (47% do total) ===\n",
    "            'ROE': ScoringMetric(\n",
    "                name='ROE', weight=0.18, direction=1,  # Aumentado de 0.15 para 0.18\n",
    "                description='Return on Equity - Efici√™ncia do capital pr√≥prio',\n",
    "                ideal_range=(0.10, 0.25)\n",
    "            ),\n",
    "            'ROA': ScoringMetric(\n",
    "                name='ROA', weight=0.12, direction=1,\n",
    "                description='Return on Assets - Efici√™ncia dos ativos',\n",
    "                ideal_range=(0.05, 0.15)\n",
    "            ),\n",
    "            'MARGEM_EBIT': ScoringMetric(\n",
    "                name='MARGEM_EBIT', weight=0.08, direction=1,\n",
    "                description='Margem Operacional - Ebit/Receita',\n",
    "                ideal_range=(0.08, 0.20)\n",
    "            ),\n",
    "            'MARGEM_LIQUIDA': ScoringMetric(\n",
    "                name='MARGEM_LIQUIDA', weight=0.05, direction=1,  # Reduzido de 0.06 para 0.05\n",
    "                description='Margem L√≠quida - Lucro/Receita',\n",
    "                ideal_range=(0.06, 0.18)\n",
    "            ),\n",
    "            'MARGEM_BRUTA': ScoringMetric(\n",
    "                name='MARGEM_BRUTA', weight=0.04, direction=1,\n",
    "                description='Margem Bruta - Lucro Bruto/Receita',\n",
    "                ideal_range=(0.20, 0.50)\n",
    "            ),\n",
    "            \n",
    "            # === SOLV√äNCIA E ALAVANCAGEM (33% do total) ===\n",
    "            'ALAVANCAGEM': ScoringMetric(\n",
    "                name='ALAVANCAGEM', weight=0.09, direction=-1,  # Aumentado de 0.08 para 0.09\n",
    "                description='Alavancagem Total - Passivo/Ativo',\n",
    "                ideal_range=(0.30, 0.60)\n",
    "            ),\n",
    "            'DIVIDA_PL': ScoringMetric(\n",
    "                name='DIVIDA_PL', weight=0.09, direction=-1,  # Aumentado de 0.08 para 0.09\n",
    "                description='D√≠vida/Patrim√¥nio L√≠quido',\n",
    "                ideal_range=(0.50, 1.50)\n",
    "            ),\n",
    "            'LIQUIDEZ_CORRENTE': ScoringMetric(\n",
    "                name='LIQUIDEZ_CORRENTE', weight=0.08, direction=1,  # Aumentado de 0.07 para 0.08\n",
    "                description='Liquidez Corrente - Ativo Circulante/Passivo Circulante',\n",
    "                ideal_range=(1.20, 3.00)\n",
    "            ),\n",
    "            'GIRO_ATIVO': ScoringMetric(\n",
    "                name='GIRO_ATIVO', weight=0.07, direction=1,\n",
    "                description='Giro do Ativo - Receita/Ativo Total',\n",
    "                ideal_range=(0.30, 1.00)\n",
    "            ),\n",
    "            \n",
    "            # === CRESCIMENTO (10% do total) ===\n",
    "            'CRESC_RECEITA': ScoringMetric(\n",
    "                name='CRESC_RECEITA', weight=0.05, direction=1,\n",
    "                description='Crescimento da Receita (anual)',\n",
    "                ideal_range=(0.05, 0.30)\n",
    "            ),\n",
    "            'CRESC_LUCRO': ScoringMetric(\n",
    "                name='CRESC_LUCRO', weight=0.05, direction=1,\n",
    "                description='Crescimento do Lucro L√≠quido (anual)',\n",
    "                ideal_range=(0.08, 0.40)\n",
    "            ),\n",
    "            \n",
    "            # === EFICI√äNCIA (10% do total) ===\n",
    "            'SENTIMENT_MEDIO': ScoringMetric(\n",
    "                name='SENTIMENT_MEDIO', weight=0.05, direction=1,\n",
    "                description='Sentimento M√©dio de Not√≠cias',\n",
    "                ideal_range=(0.10, 0.80)\n",
    "            ),\n",
    "            'VOLATILIDADE': ScoringMetric(\n",
    "                name='VOLATILIDADE', weight=0.05, direction=-1,\n",
    "                description='Volatilidade dos Retornos (3 meses)',\n",
    "                ideal_range=(0.10, 0.40)\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Validar que soma dos pesos = 1.0\n",
    "        total_weight = sum(metric.weight for metric in metrics_config.values())\n",
    "        if abs(total_weight - 1.0) > 0.001:\n",
    "            # Ajuste autom√°tico para garantir soma = 1.0\n",
    "            adjustment_factor = 1.0 / total_weight\n",
    "            for metric in metrics_config.values():\n",
    "                metric.weight *= adjustment_factor\n",
    "            \n",
    "            logger.info(f\"üîß Pesos ajustados automaticamente para soma = 1.0\")\n",
    "        \n",
    "        logger.info(f\"‚úÖ Sistema de scoring inicializado com {len(metrics_config)} m√©tricas\")\n",
    "        logger.info(f\"üìä Soma dos pesos: {sum(metric.weight for metric in metrics_config.values()):.3f}\")\n",
    "        \n",
    "        return metrics_config\n",
    "    \n",
    "    def _load_custom_config(self, config_path: str) -> Dict[str, ScoringMetric]:\n",
    "        \"\"\"Carrega configura√ß√£o personalizada de pesos\"\"\"\n",
    "        try:\n",
    "            with open(config_path, 'r', encoding='utf-8') as f:\n",
    "                config_data = json.load(f)\n",
    "            \n",
    "            metrics_config = {}\n",
    "            for metric_name, metric_data in config_data.items():\n",
    "                metrics_config[metric_name] = ScoringMetric(**metric_data)\n",
    "            \n",
    "            logger.info(f\"‚úÖ Configura√ß√£o personalizada carregada: {config_path}\")\n",
    "            return metrics_config\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erro ao carregar configura√ß√£o: {e}\")\n",
    "            return self._initialize_scoring_metrics()  # Fallback para padr√£o\n",
    "    \n",
    "    def calculate_individual_scores(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calcula scores individuais para cada m√©trica usando normaliza√ß√£o avan√ßada\n",
    "        \"\"\"\n",
    "        scores_df = df.copy()\n",
    "        \n",
    "        logger.info(\"üéØ Calculando scores individuais...\")\n",
    "        \n",
    "        for metric_name, metric_config in self.scoring_metrics.items():\n",
    "            if metric_name not in scores_df.columns:\n",
    "                logger.warning(f\"‚ö†Ô∏è M√©trica {metric_name} n√£o encontrada no dataset\")\n",
    "                continue\n",
    "            \n",
    "            # CORRE√á√ÉO: Verifica√ß√£o de tipo de dados corrigida para NumPy 2.0\n",
    "            if scores_df[metric_name].dtype == object or pd.api.types.is_string_dtype(scores_df[metric_name]):\n",
    "                logger.warning(f\"‚ö†Ô∏è M√©trica {metric_name} √© do tipo texto - pulando\")\n",
    "                continue\n",
    "            \n",
    "            # Remover outliers extremos\n",
    "            clean_series = self._remove_outliers(scores_df[metric_name])\n",
    "            \n",
    "            if metric_config.direction == 1:\n",
    "                # MAIOR √© melhor - usar fun√ß√£o sigmoide para suavizar\n",
    "                scores_df[f'score_{metric_name}'] = self._sigmoid_normalization(clean_series)\n",
    "            else:\n",
    "                # MENOR √© melhor - inverter a normaliza√ß√£o\n",
    "                scores_df[f'score_{metric_name}'] = 1 - self._sigmoid_normalization(clean_series)\n",
    "            \n",
    "            # Aplicar pesos\n",
    "            scores_df[f'score_{metric_name}'] *= 100  # Converter para 0-100\n",
    "            scores_df[f'score_{metric_name}'] *= metric_config.weight\n",
    "            \n",
    "            valid_scores = scores_df[f'score_{metric_name}'].notna().sum()\n",
    "            logger.info(f\"  ‚úÖ {metric_name}: {valid_scores} scores calculados (peso: {metric_config.weight*100:.1f}%)\")\n",
    "        \n",
    "        return scores_df\n",
    "    \n",
    "    def _remove_outliers(self, series: pd.Series, n_std: int = 3) -> pd.Series:\n",
    "        \"\"\"Remove outliers usando m√©todo Z-score\"\"\"\n",
    "        # CORRE√á√ÉO: Verifica√ß√£o de tipo de dados atualizada para NumPy 2.0\n",
    "        if series.dtype == object or pd.api.types.is_string_dtype(series) or pd.api.types.is_categorical_dtype(series):\n",
    "            return series\n",
    "        \n",
    "        try:\n",
    "            z_scores = np.abs((series - series.mean()) / series.std())\n",
    "            clean_series = series.copy()\n",
    "            clean_series[z_scores > n_std] = np.nan\n",
    "            \n",
    "            outliers_removed = (z_scores > n_std).sum()\n",
    "            if outliers_removed > 0:\n",
    "                logger.debug(f\"  üéØ {outliers_removed} outliers removidos de {series.name}\")\n",
    "            \n",
    "            return clean_series\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"  ‚ö†Ô∏è Erro ao remover outliers de {series.name}: {e}\")\n",
    "            return series\n",
    "    \n",
    "    def _sigmoid_normalization(self, series: pd.Series) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Normaliza√ß√£o usando fun√ß√£o sigmoide para suavizar valores extremos\n",
    "        Mais robusta que rankeamento simples\n",
    "        \"\"\"\n",
    "        # CORRE√á√ÉO: Verifica√ß√£o de tipo de dados atualizada\n",
    "        if series.dtype == object or pd.api.types.is_string_dtype(series) or pd.api.types.is_categorical_dtype(series):\n",
    "            return series\n",
    "        \n",
    "        try:\n",
    "            # Standardizar para m√©dia 0, std 1\n",
    "            standardized = (series - series.mean()) / series.std()\n",
    "            \n",
    "            # Aplicar sigmoide\n",
    "            sigmoid = 1 / (1 + np.exp(-standardized))\n",
    "            \n",
    "            return sigmoid\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"  ‚ö†Ô∏è Erro na normaliza√ß√£o sigmoide de {series.name}: {e}\")\n",
    "            # Fallback: normaliza√ß√£o linear simples\n",
    "            return (series - series.min()) / (series.max() - series.min())\n",
    "    \n",
    "    def calculate_composite_score(self, scores_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calcula o score composto final com valida√ß√µes\n",
    "        \"\"\"\n",
    "        logger.info(\"‚öñÔ∏è Calculando score composto...\")\n",
    "        \n",
    "        # Identificar colunas de score\n",
    "        score_columns = [col for col in scores_df.columns if col.startswith('score_')]\n",
    "        \n",
    "        if not score_columns:\n",
    "            raise ValueError(\"‚ùå Nenhuma coluna de score encontrada\")\n",
    "        \n",
    "        logger.info(f\"üìã Colunas de score encontradas: {len(score_columns)}\")\n",
    "        \n",
    "        # Calcular score composto\n",
    "        scores_df['aurum_quality_score'] = scores_df[score_columns].sum(axis=1, skipna=True)\n",
    "        \n",
    "        # Normalizar para 0-100 (caso alguns pesos n√£o tenham sido aplicados)\n",
    "        max_possible_score = sum(metric.weight * 100 for metric in self.scoring_metrics.values())\n",
    "        scores_df['aurum_quality_score'] = (scores_df['aurum_quality_score'] / max_possible_score) * 100\n",
    "        \n",
    "        # Garantir que scores estejam entre 0 e 100\n",
    "        scores_df['aurum_quality_score'] = scores_df['aurum_quality_score'].clip(0, 100)\n",
    "        \n",
    "        # Aplicar classifica√ß√µes\n",
    "        scores_df = self._apply_quality_classifications(scores_df)\n",
    "        \n",
    "        valid_scores = scores_df['aurum_quality_score'].notna().sum()\n",
    "        logger.info(f\"‚úÖ Score composto calculado: {valid_scores} empresas\")\n",
    "        \n",
    "        return scores_df\n",
    "    \n",
    "    def _apply_quality_classifications(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Aplica classifica√ß√µes de qualidade (A, B, C, D, E)\"\"\"\n",
    "        \n",
    "        # Classifica√ß√£o por quintis\n",
    "        try:\n",
    "            df['quality_quintile'] = pd.qcut(\n",
    "                df['aurum_quality_score'], \n",
    "                5, \n",
    "                labels=['5¬∫ Quintil', '4¬∫ Quintil', '3¬∫ Quintil', '2¬∫ Quintil', '1¬∫ Quintil']\n",
    "            )\n",
    "        except ValueError as e:\n",
    "            logger.warning(f\"‚ö†Ô∏è Erro no qcut, usando cortes uniformes: {e}\")\n",
    "            # Fallback para cortes uniformes\n",
    "            df['quality_quintile'] = pd.cut(\n",
    "                df['aurum_quality_score'],\n",
    "                bins=5,\n",
    "                labels=['5¬∫ Quintil', '4¬∫ Quintil', '3¬∫ Quintil', '2¬∫ Quintil', '1¬∫ Quintil']\n",
    "            )\n",
    "        \n",
    "        # Classifica√ß√£o por letras baseada em thresholds\n",
    "        conditions = [\n",
    "            df['aurum_quality_score'] >= self.quality_thresholds['A'],\n",
    "            df['aurum_quality_score'] >= self.quality_thresholds['B'],\n",
    "            df['aurum_quality_score'] >= self.quality_thresholds['C'],\n",
    "            df['aurum_quality_score'] >= self.quality_thresholds['D'],\n",
    "            df['aurum_quality_score'] >= self.quality_thresholds['E']\n",
    "        ]\n",
    "        \n",
    "        choices = ['A', 'B', 'C', 'D', 'E']\n",
    "        \n",
    "        df['quality_grade'] = np.select(conditions, choices, default='E')\n",
    "        \n",
    "        # Classifica√ß√£o descritiva\n",
    "        grade_descriptions = {\n",
    "            'A': 'Excelente Qualidade',\n",
    "            'B': 'Boa Qualidade', \n",
    "            'C': 'Qualidade Regular',\n",
    "            'D': 'Qualidade Baixa',\n",
    "            'E': 'Qualidade Muito Baixa'\n",
    "        }\n",
    "        \n",
    "        df['quality_description'] = df['quality_grade'].map(grade_descriptions)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def calculate_sector_adjusted_scores(self, df: pd.DataFrame, sector_column: str = 'setor') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calcula scores ajustados por setor (quando informa√ß√£o de setor dispon√≠vel)\n",
    "        \"\"\"\n",
    "        if sector_column not in df.columns:\n",
    "            logger.warning(\"‚ö†Ô∏è Coluna de setor n√£o encontrada - pulando ajuste setorial\")\n",
    "            return df\n",
    "        \n",
    "        logger.info(\"üè≠ Aplicando ajuste setorial...\")\n",
    "        \n",
    "        df_sector_adjusted = df.copy()\n",
    "        \n",
    "        # Calcular medianas por setor para cada m√©trica\n",
    "        for metric_name in self.scoring_metrics.keys():\n",
    "            if metric_name in df.columns:\n",
    "                sector_medians = df.groupby(sector_column)[metric_name].median()\n",
    "                \n",
    "                # Ajustar scores baseado na mediana do setor\n",
    "                df_sector_adjusted[f'{metric_name}_sector_adj'] = df.apply(\n",
    "                    lambda row: row[metric_name] / sector_medians.get(row[sector_column], 1.0), \n",
    "                    axis=1\n",
    "                )\n",
    "        \n",
    "        # Recalcular scores com ajuste setorial\n",
    "        score_columns_adj = [col for col in df_sector_adjusted.columns if col.endswith('_sector_adj')]\n",
    "        \n",
    "        if score_columns_adj:\n",
    "            logger.info(f\"‚úÖ Ajuste setorial aplicado para {len(score_columns_adj)} m√©tricas\")\n",
    "        \n",
    "        return df_sector_adjusted\n",
    "    \n",
    "    def validate_scoring_system(self, scores_df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Valida o sistema de scoring atrav√©s de an√°lises estat√≠sticas\n",
    "        \"\"\"\n",
    "        logger.info(\"üîç Validando sistema de scoring...\")\n",
    "        \n",
    "        validation_report = {\n",
    "            'basic_stats': {\n",
    "                'total_companies': len(scores_df),\n",
    "                'companies_with_scores': scores_df['aurum_quality_score'].notna().sum(),\n",
    "                'score_mean': scores_df['aurum_quality_score'].mean(),\n",
    "                'score_std': scores_df['aurum_quality_score'].std(),\n",
    "                'score_min': scores_df['aurum_quality_score'].min(),\n",
    "                'score_max': scores_df['aurum_quality_score'].max(),\n",
    "                'score_median': scores_df['aurum_quality_score'].median()\n",
    "            },\n",
    "            'distribution': {\n",
    "                'grade_A': len(scores_df[scores_df['quality_grade'] == 'A']),\n",
    "                'grade_B': len(scores_df[scores_df['quality_grade'] == 'B']),\n",
    "                'grade_C': len(scores_df[scores_df['quality_grade'] == 'C']),\n",
    "                'grade_D': len(scores_df[scores_df['quality_grade'] == 'D']),\n",
    "                'grade_E': len(scores_df[scores_df['quality_grade'] == 'E'])\n",
    "            },\n",
    "            'correlation_analysis': {},\n",
    "            'metric_contribution': {},\n",
    "            'weight_summary': {}\n",
    "        }\n",
    "        \n",
    "        # An√°lise de correla√ß√£o entre scores individuais e score final\n",
    "        score_columns = [col for col in scores_df.columns if col.startswith('score_')]\n",
    "        \n",
    "        for score_col in score_columns:\n",
    "            if score_col in scores_df.columns:\n",
    "                correlation = scores_df[score_col].corr(scores_df['aurum_quality_score'])\n",
    "                validation_report['correlation_analysis'][score_col] = round(correlation, 4) if not pd.isna(correlation) else None\n",
    "        \n",
    "        # Contribui√ß√£o de cada m√©trica\n",
    "        total_actual_weight = 0\n",
    "        for metric_name, metric_config in self.scoring_metrics.items():\n",
    "            score_col = f'score_{metric_name}'\n",
    "            if score_col in scores_df.columns:\n",
    "                actual_contribution = scores_df[score_col].mean() / scores_df['aurum_quality_score'].mean() * 100\n",
    "                if not pd.isna(actual_contribution):\n",
    "                    validation_report['metric_contribution'][metric_name] = {\n",
    "                        'weight': round(metric_config.weight * 100, 2),\n",
    "                        'actual_contribution': round(actual_contribution, 2),\n",
    "                        'description': metric_config.description\n",
    "                    }\n",
    "                    total_actual_weight += actual_contribution\n",
    "        \n",
    "        validation_report['weight_summary']['total_theoretical_weight'] = 100.0\n",
    "        validation_report['weight_summary']['total_actual_weight'] = round(total_actual_weight, 2)\n",
    "        \n",
    "        logger.info(\"‚úÖ Sistema de scoring validado\")\n",
    "        return validation_report\n",
    "    \n",
    "    def save_scoring_configuration(self, output_path: str = \"data/aurum_scoring_config.json\"):\n",
    "        \"\"\"Salva a configura√ß√£o do sistema de scoring\"\"\"\n",
    "        config_data = {}\n",
    "        \n",
    "        for metric_name, metric_config in self.scoring_metrics.items():\n",
    "            config_data[metric_name] = {\n",
    "                'name': metric_config.name,\n",
    "                'weight': metric_config.weight,\n",
    "                'direction': metric_config.direction,\n",
    "                'description': metric_config.description,\n",
    "                'min_value': metric_config.min_value,\n",
    "                'max_value': metric_config.max_value,\n",
    "                'ideal_range': metric_config.ideal_range\n",
    "            }\n",
    "        \n",
    "        output_dir = Path(output_path).parent\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config_data, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        logger.info(f\"üíæ Configura√ß√£o salva em: {output_path}\")\n",
    "        return output_path\n",
    "\n",
    "# ==================== EXECU√á√ÉO PRINCIPAL ====================\n",
    "\n",
    "def run_advanced_scoring_system():\n",
    "    \"\"\"\n",
    "    Executa o sistema avan√ßado de scoring completo\n",
    "    \"\"\"\n",
    "    logger.info(\"üöÄ INICIANDO SISTEMA AVAN√áADO DE SCORING AURUM\")\n",
    "    \n",
    "    try:\n",
    "        # 1. INICIALIZAR SISTEMA DE SCORING\n",
    "        scoring_system = AurumScoringSystem()\n",
    "        \n",
    "        # 2. CARREGAR DADOS DO DATAFRAME MESTRE (Unificado)\n",
    "        ratios_path = \"data/aurum_master_features.parquet\" # <- ESTA √â A MUDAN√áA\n",
    "        \n",
    "        if not Path(ratios_path).exists():\n",
    "            logger.error(f\"‚ùå Arquivo de ratios n√£o encontrado: {ratios_path}\")\n",
    "            logger.info(\"üí° Execute primeiro o c√°lculo dos ratios financeiros\")\n",
    "            return None\n",
    "        \n",
    "        ratios_df = pd.read_parquet(ratios_path)\n",
    "        logger.info(f\"‚úÖ Dados de ratios carregados: {ratios_df.shape}\")\n",
    "        \n",
    "        # 3. CALCULAR SCORES INDIVIDUAIS\n",
    "        individual_scores_df = scoring_system.calculate_individual_scores(ratios_df)\n",
    "        \n",
    "        # 4. CALCULAR SCORE COMPOSTO\n",
    "        final_scores_df = scoring_system.calculate_composite_score(individual_scores_df)\n",
    "        \n",
    "        # 5. VALIDAR SISTEMA\n",
    "        validation_report = scoring_system.validate_scoring_system(final_scores_df)\n",
    "        \n",
    "        # 6. SALVAR RESULTADOS\n",
    "        output_dir = Path(\"data/aurum_final_scores\")\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Salvar scores finais\n",
    "        final_output_path = output_dir / \"aurum_advanced_scores.parquet\"\n",
    "        final_scores_df.to_parquet(final_output_path, index=False)\n",
    "        \n",
    "        # Salvar apenas os mais recentes\n",
    "        latest_scores = final_scores_df.sort_values(['CNPJ_CIA', 'DT_FIM_EXERC']).groupby('CNPJ_CIA').last().reset_index()\n",
    "        latest_output_path = output_dir / \"aurum_latest_advanced_scores.parquet\"\n",
    "        latest_scores.to_parquet(latest_output_path, index=False)\n",
    "        \n",
    "        # Salvar configura√ß√£o\n",
    "        config_path = scoring_system.save_scoring_configuration()\n",
    "        \n",
    "        # 7. RELAT√ìRIO FINAL\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"üéâ SISTEMA DE SCORING AURUM - RESULTADOS FINAIS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        basic_stats = validation_report['basic_stats']\n",
    "        distribution = validation_report['distribution']\n",
    "        \n",
    "        print(f\"\\nüìä ESTAT√çSTICAS GERAIS:\")\n",
    "        print(f\"   ‚Ä¢ Empresas no dataset: {basic_stats['total_companies']:,}\")\n",
    "        print(f\"   ‚Ä¢ Empresas com score:  {basic_stats['companies_with_scores']:,}\")\n",
    "        print(f\"   ‚Ä¢ Score m√©dio: {basic_stats['score_mean']:.2f}\")\n",
    "        print(f\"   ‚Ä¢ Score mediano: {basic_stats['score_median']:.2f}\")\n",
    "        print(f\"   ‚Ä¢ Melhor score: {basic_stats['score_max']:.2f}\")\n",
    "        print(f\"   ‚Ä¢ Pior score: {basic_stats['score_min']:.2f}\")\n",
    "        \n",
    "        print(f\"\\nüìà DISTRIBUI√á√ÉO DAS NOTAS:\")\n",
    "        print(f\"   ‚Ä¢ Nota A (Excelente): {distribution['grade_A']:3d} empresas\")\n",
    "        print(f\"   ‚Ä¢ Nota B (Boa):       {distribution['grade_B']:3d} empresas\") \n",
    "        print(f\"   ‚Ä¢ Nota C (Regular):   {distribution['grade_C']:3d} empresas\")\n",
    "        print(f\"   ‚Ä¢ Nota D (Baixa):     {distribution['grade_D']:3d} empresas\")\n",
    "        print(f\"   ‚Ä¢ Nota E (Muito Baixa): {distribution['grade_E']:3d} empresas\")\n",
    "        \n",
    "        print(f\"\\nü•á TOP 10 EMPRESAS POR QUALIDADE:\")\n",
    "        top_10 = latest_scores.nlargest(10, 'aurum_quality_score')[\n",
    "            ['DENOM_CIA', 'aurum_quality_score', 'quality_grade', 'quality_description']\n",
    "        ]\n",
    "        \n",
    "        for i, (_, row) in enumerate(top_10.iterrows(), 1):\n",
    "            print(f\"   {i:2d}. {row['DENOM_CIA'][:35]:35} {row['aurum_quality_score']:6.2f} ({row['quality_grade']})\")\n",
    "        \n",
    "        print(f\"\\nüìã CONTRIBUI√á√ÉO DAS M√âTRICAS:\")\n",
    "        metric_contrib = validation_report['metric_contribution']\n",
    "        for metric_name, contrib_info in list(metric_contrib.items())[:6]:  # Mostrar top 6\n",
    "            diff = contrib_info['actual_contribution'] - contrib_info['weight']\n",
    "            diff_symbol = \"+\" if diff > 0 else \"\"\n",
    "            print(f\"   ‚Ä¢ {metric_name:15}: {contrib_info['actual_contribution']:5.1f}% (peso: {contrib_info['weight']:.1f}%) {diff_symbol}{diff:+.1f}%\")\n",
    "        \n",
    "        weight_summary = validation_report['weight_summary']\n",
    "        print(f\"\\n‚öñÔ∏è  RESUMO DE PESOS:\")\n",
    "        print(f\"   ‚Ä¢ Peso te√≥rico total: {weight_summary['total_theoretical_weight']}%\")\n",
    "        print(f\"   ‚Ä¢ Peso real total:    {weight_summary['total_actual_weight']}%\")\n",
    "        \n",
    "        print(f\"\\nüíæ ARQUIVOS GERADOS:\")\n",
    "        print(f\"   ‚Ä¢ Scores completos: {final_output_path}\")\n",
    "        print(f\"   ‚Ä¢ Scores recentes:  {latest_output_path}\")\n",
    "        print(f\"   ‚Ä¢ Configura√ß√£o:     {config_path}\")\n",
    "        \n",
    "        return {\n",
    "            'scoring_system': scoring_system,\n",
    "            'final_scores': final_scores_df,\n",
    "            'latest_scores': latest_scores,\n",
    "            'validation_report': validation_report\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Erro no sistema de scoring: {e}\")\n",
    "        import traceback\n",
    "        logger.error(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "# Fun√ß√£o para an√°lise r√°pida de m√©tricas\n",
    "def analyze_metric_importance():\n",
    "    \"\"\"Analisa a import√¢ncia de cada m√©trica no sistema de scoring\"\"\"\n",
    "    scoring_system = AurumScoringSystem()\n",
    "    \n",
    "    print(\"\\nüîç AN√ÅLISE DE IMPORT√ÇNCIA DAS M√âTRICAS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    metrics_by_category = {\n",
    "        'RENTABILIDADE': ['ROE', 'ROA', 'MARGEM_EBIT', 'MARGEM_LIQUIDA', 'MARGEM_BRUTA'],\n",
    "        'SOLV√äNCIA': ['ALAVANCAGEM', 'DIVIDA_PL', 'LIQUIDEZ_CORRENTE', 'GIRO_ATIVO'],\n",
    "        'CRESCIMENTO': ['CRESC_RECEITA', 'CRESC_LUCRO'],\n",
    "        'EFICI√äNCIA': ['SENTIMENT_MEDIO', 'VOLATILIDADE']\n",
    "    }\n",
    "    \n",
    "    total_weight = 0\n",
    "    for category, metrics in metrics_by_category.items():\n",
    "        category_weight = sum(\n",
    "            scoring_system.scoring_metrics[metric].weight \n",
    "            for metric in metrics \n",
    "            if metric in scoring_system.scoring_metrics\n",
    "        )\n",
    "        total_weight += category_weight\n",
    "        \n",
    "        print(f\"\\nüìä {category}: {category_weight*100:.1f}%\")\n",
    "        for metric in metrics:\n",
    "            if metric in scoring_system.scoring_metrics:\n",
    "                metric_config = scoring_system.scoring_metrics[metric]\n",
    "                print(f\"   ‚Ä¢ {metric:20} {metric_config.weight*100:5.1f}% - {metric_config.description}\")\n",
    "    \n",
    "    print(f\"\\nüìà SOMA TOTAL: {total_weight*100:.1f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Executar sistema completo\n",
    "    results = run_advanced_scoring_system()\n",
    "    \n",
    "    # Mostrar an√°lise de import√¢ncia\n",
    "    analyze_metric_importance()\n",
    "    \n",
    "    print(\"\\nüéØ PR√ìXIMOS PASSOS SUGERIDOS:\")\n",
    "    print(\"   1. Analisar correla√ß√£o entre scores e performance futura\")\n",
    "    print(\"   2. Ajustar pesos baseado em backtesting hist√≥rico\") \n",
    "    print(\"   3. Implementar ajustes setoriais espec√≠ficos\")\n",
    "    print(\"   4. Criar dashboard de monitoramento dos scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f22f038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05a7a643",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Documenta√ß√£o**: Script de Coleta de Not√≠cias (Google News RSS)\n",
    "\n",
    "#### **1. Objetivo**\n",
    "\n",
    "Este script implementa parte do Pilar 2 (Qualidade da Comunica√ß√£o) do projeto Aurum. Sua responsabilidade √© automatizar a coleta de not√≠cias financeiras recentes para cada empresa (ticker) listada no √≠ndice IBRX-100.\n",
    "\n",
    "O script utiliza o *feed RSS do Google News* como fonte de dados, buscando men√ß√µes a cada ticker nos √∫ltimos 30 dias. Os dados brutos coletados s√£o a base para a futura an√°lise de sentimento (NLP).\n",
    "\n",
    "#### **2. Configura√ß√£o (Input)**\n",
    "\n",
    "O script depende de um √∫nico arquivo de entrada:\n",
    "\n",
    "* `tickers_ibrx100_full.csv`: Um arquivo CSV que deve conter a lista completa de tickers do IBRX-100.\n",
    "    * Formato esperado: O script l√™ a **primeira coluna** deste arquivo. Os tickers podem estar no formato `PETR4.SA` ou `PETR4`. A fun√ß√£o `load_tickers_from_csv` remove automaticamente o sufixo `.SA` para otimizar a busca no Google News.\n",
    "\n",
    "#### **3. Sa√≠da (Output)**\n",
    "\n",
    "O script gera dois arquivos id√™nticos em conte√∫do, localizados em `data/news/`:\n",
    "\n",
    "1.  `raw_news_data.parquet`\n",
    "2.  `raw_news_data.csv`\n",
    "\n",
    "O schema (colunas) do DataFrame salvo √©:\n",
    "\n",
    "| Coluna | Tipo | Descri√ß√£o |\n",
    "| :--- | :--- | :--- |\n",
    "| `ticker_query` | string | O ticker usado na busca (ex: `PETR4`). |\n",
    "| `title` | string | O t√≠tulo da not√≠cia. |\n",
    "| `link` | string | O link original da not√≠cia. |\n",
    "| `published_date` | datetime | A data e hora da publica√ß√£o (j√° convertida). |\n",
    "| `source` | string | O nome do ve√≠culo de m√≠dia (ex: \"InfoMoney\"). |\n",
    "| `summary` | string | Um pequeno resumo ou *snippet* da not√≠cia (HTML). |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c597f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diret√≥rio para salvar os dados\n",
    "DATA_DIR = \"data\"\n",
    "NEWS_DIR = os.path.join(DATA_DIR, \"news\")\n",
    "os.makedirs(NEWS_DIR, exist_ok=True)\n",
    "\n",
    "def load_tickers_from_csv(file_path: str) -> list:\n",
    "    \"\"\"Carrega a lista de tickers a partir de um arquivo CSV.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    tickers = df.iloc[:, 0].dropna().astype(str).tolist()\n",
    "    # Remove o sufixo '.SA' para usar na busca de not√≠cias\n",
    "    return [t.replace('.SA', '') for t in tickers]\n",
    "\n",
    "def fetch_news_for_ticker(ticker: str):\n",
    "    \"\"\"Busca not√≠cias para um ticker espec√≠fico usando o RSS do Google News.\"\"\"\n",
    "    raw_query = f'\"{ticker}\" when:30d'\n",
    "    search_query = urllib.parse.quote(raw_query)\n",
    "    url = f\"https://news.google.com/rss/search?q={search_query}&hl=pt-BR&gl=BR&ceid=BR:pt-419\"\n",
    "    \n",
    "    feed = feedparser.parse(url)\n",
    "    \n",
    "    news_items = []\n",
    "    for entry in feed.entries:\n",
    "        news_items.append({\n",
    "            'ticker_query': ticker,\n",
    "            'title': entry.title,\n",
    "            'link': entry.link,\n",
    "            'published_date': entry.published,\n",
    "            'source': entry.source.title,\n",
    "            'summary': entry.summary\n",
    "        })\n",
    "    return news_items\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tickers_csv_path = \"tickers_ibrx100_full.csv\"\n",
    "    tickers = load_tickers_from_csv(tickers_csv_path)\n",
    "    \n",
    "    all_news = []\n",
    "    \n",
    "    print(\"Iniciando a coleta de not√≠cias via Google News RSS...\")\n",
    "    for ticker in tqdm(tickers, desc=\"Buscando not√≠cias\"):\n",
    "        try:\n",
    "            news = fetch_news_for_ticker(ticker)\n",
    "            if news:\n",
    "                all_news.extend(news)\n",
    "            time.sleep(0.5)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao buscar not√≠cias para {ticker}: {e}\")\n",
    "\n",
    "    if not all_news:\n",
    "        print(\"\\nNenhuma not√≠cia foi coletada. Verifique a conex√£o ou a consulta de busca. Encerrando.\")\n",
    "    else:\n",
    "        df_news = pd.DataFrame(all_news)\n",
    "        df_news.drop_duplicates(subset=['title', 'link'], inplace=True)\n",
    "        df_news['published_date'] = pd.to_datetime(df_news['published_date'])\n",
    "        \n",
    "        # --- SALVANDO ARQUIVOS ---\n",
    "        output_path_parquet = os.path.join(NEWS_DIR, \"raw_news_data.parquet\")\n",
    "        output_path_csv = os.path.join(NEWS_DIR, \"raw_news_data.csv\")\n",
    "\n",
    "        # Salva em Parquet\n",
    "        df_news.to_parquet(output_path_parquet, index=False)\n",
    "        \n",
    "        # Salva em CSV\n",
    "        df_news.to_csv(output_path_csv, index=False)\n",
    "        \n",
    "        print(f\"\\nColeta conclu√≠da. {len(df_news)} not√≠cias √∫nicas salvas.\")\n",
    "        print(f\"-> {output_path_parquet}\")\n",
    "        print(f\"-> {output_path_csv}\")\n",
    "        print(\"\\nAmostra das not√≠cias coletadas:\")\n",
    "        print(df_news.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946e0066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1aa5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6728029b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93080c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8791114",
   "metadata": {},
   "source": [
    "### **Documenta√ß√£o:** Script de Amostragem e An√°lise (AurumDataSampler)\n",
    "\n",
    "#### **1. Objetivo**\n",
    "\n",
    "Este √© um **script utilit√°rio** de An√°lise Explorat√≥ria de Dados (EDA). Seu principal objetivo √© ajudar o desenvolvedor a entender o ecossistema de dados do projeto Aurum, que est√° em constante evolu√ß√£o.\n",
    "\n",
    "Ele **n√£o coleta** novos dados. Em vez disso, ele **varre** o diret√≥rio `data/` em busca de todos os arquivos de dados `.parquet` j√° processados e, para cada um, executa as seguintes a√ß√µes:\n",
    "1.  **Descobre:** Lista todos os datasets `.parquet` dispon√≠veis.\n",
    "2.  **Amostra:** Carrega uma amostra pequena e inteligente dos dados (usando amostragem estratificada por data, se poss√≠vel).\n",
    "3.  **Analisa:** Gera um relat√≥rio b√°sico de qualidade dos dados (contagem de linhas, colunas, tipos de dados, valores nulos, duplicatas e estat√≠sticas num√©ricas).\n",
    "4.  **Reporta:** Salva as amostras em arquivos `.csv` f√°ceis de visualizar e cria um relat√≥rio consolidado em `.json`.\n",
    "\n",
    "Este script √© fundamental para validar rapidamente a sa√≠da de cada etapa do pipeline (coleta de not√≠cias, processamento de fundamentos, etc.).\n",
    "\n",
    "#### **2. Configura√ß√£o (Input)**\n",
    "\n",
    "O script foi projetado para funcionar sem configura√ß√£o manual. Ele assume a seguinte estrutura de diret√≥rio:\n",
    "\n",
    "* **data/ (Diret√≥rio Raiz):** O script inicia sua busca aqui.\n",
    "* **.parquet** (Arquivos Alvo):** Ele procurar√° recursivamente em **todos** os subdiret√≥rios dentro de data/ por qualquer arquivo que termine com a extens√£o .parquet.\n",
    "\n",
    "## 3. Sa√≠da (Output)\n",
    "\n",
    "Ao executar main(), o script cria um novo diret√≥rio: data/samples/. Este diret√≥rio conter√°:\n",
    "\n",
    "1.  **Amostras em CSV (`*.sample.csv`):**\n",
    "    * `raw_news_data_sample.csv`\n",
    "    * `fundamentals_wide_sample.csv`\n",
    "    * `aurum_quality_scores_complete_sample.csv`\n",
    "    * *(...e um `.csv` para cada `.parquet` encontrado)*\n",
    "\n",
    "2.  **Relat√≥rio Consolidado (`aurum_sampling_report.json`):**\n",
    "    * Um √∫nico arquivo JSON que armazena os metadados e a an√°lise de qualidade de todos os datasets processados. Este arquivo √© ideal para monitoramento program√°tico do \"data health\" (sa√∫de dos dados) do projeto.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4742156d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéØ RELAT√ìRIO DE AMOSTRAGEM - AURUM DATA SAMPLES\n",
      "============================================================\n",
      "\n",
      "üìÅ DATASET: aurum_master_features.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 19,581\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 57\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.03 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date   ticker  Adj Close           CNPJ_CIA                                DENOM_CIA  Custo dos Bens e/ou Servi√ßos Vendidos         EBIT          EBT   Lucro Bruto  Lucro L√≠quido Consolidado  Receita L√≠quida  Ativo Circulante  Ativo N√£o Circulante  Ativo Total  Caixa e Equivalentes  D√≠vida Curto Prazo  D√≠vida Longo Prazo  Passivo Circulante  Passivo N√£o Circulante  Passivo Total  Patrim√¥nio L√≠quido Consolidado  Receita L√≠quida_ttm  Custo dos Bens e/ou Servi√ßos Vendidos_ttm  Lucro Bruto_ttm     EBIT_ttm      EBT_ttm  Lucro L√≠quido Consolidado_ttm  D√≠vida Bruta  Capital Investido  D√≠vida L√≠quida       ROE       ROA      ROIC  MARGEM_EBIT  MARGEM_LIQUIDA  MARGEM_BRUTA  ALAVANCAGEM  DIVIDA_PL  DIVIDA_LIQ_EBIT  LIQUIDEZ_CORRENTE  GIRO_ATIVO  score_ROIC  score_ROE  score_MARGEM_EBIT  score_MARGEM_LIQUIDA  score_DIVIDA_LIQ_EBIT  score_LIQUIDEZ_CORRENTE  score_GIRO_ATIVO  aurum_quality_score  final_rank quality_quintile quality_grade ticker_simple  VOLATILIDADE  SENTIMENT_MEDIO  SENTIMENT_STD  NEWS_COUNT\n",
      "2017-03-01 CSMG3.SA   7.746188 17.281.106/0001-03 CIA SANEAMENTO DE MINAS GERAIS-COPASA MG                          -1.900376e+09  559831000.0  411041000.0  1.128673e+09                302409000.0     3.029049e+09      1.701561e+09          9.275757e+09 1.097732e+10           648223000.0         471679000.0        3004353000.0        1.070867e+09            4041019000.0   1.097732e+10                    5.865432e+09          199840000.0                               8.554800e+07      285388000.0  246836000.0  390985000.0                    273233000.0  3476032000.0       9.341464e+09    2.827809e+09  0.046584  0.024891  0.026424     1.235168        1.367259      1.428082          1.0   0.592630        11.456226           1.588956    0.018205   68.444444  74.222222          88.516746             90.430622              25.892857                56.012658         55.111111            65.561324    0.109943       1¬∫ Quintil             A         CSMG3      0.018682              0.0            NaN         NaN\n",
      "2016-02-01 PRIO3.SA   0.145766 10.629.105/0001-68                                PRIO S.A.                          -2.770870e+08  -76244000.0  -75064000.0 -4.748500e+07                -69753000.0     2.296020e+08      8.911470e+08          3.426580e+08 1.233805e+09           167194000.0           4466000.0          87192000.0        1.857030e+08             369238000.0   1.233805e+09                    6.788640e+08         -175520000.0                               2.353200e+07     -151988000.0  -94528000.0 -105460000.0                    -90731000.0    91658000.0       7.705220e+08   -7.553600e+07 -0.133651 -0.073538 -0.122680     0.538560        0.516927      0.865930          1.0   0.135017         0.799086           4.798775   -0.142259    5.785124  13.223140          74.008811             75.770925              49.173554                94.117647          6.584362            39.554400    0.830492       5¬∫ Quintil             E         PRIO3      0.041649              0.0            NaN         NaN\n",
      "2025-01-31 WEGE3.SA  54.057568 84.429.695/0001-11                                 WEG S.A.                          -2.517310e+10 7690528000.0 7908508000.0  1.281384e+10               6318763000.0     3.798694e+10      2.722136e+10          1.426834e+10 4.148970e+10          7347599000.0        2850956000.0         744281000.0        1.545426e+10            2910219000.0   4.148970e+10                    2.312522e+10         5483340000.0                              -3.470359e+09     2012981000.0 1228403000.0 1317711000.0                    451148000.0  3595237000.0       2.672045e+10   -3.752362e+09  0.019509  0.010874  0.045972     0.224025        0.082276      0.367109          1.0   0.155468        -3.054667           1.761414    0.132161   73.316062  51.295337          53.333333             50.000000              72.588832                63.950617         77.979275            64.104630    0.126136       1¬∫ Quintil             A         WEGE3      0.018430              0.0            NaN         NaN\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\aurum_master_features_sample.csv\n",
      "\n",
      "üìÅ DATASET: ticker_cnpj_map.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 97\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 3\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.01 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "  ticker ticker_simple           CNPJ_CIA\n",
      "NATU3.SA         NATU3 32.785.497/0001-97\n",
      "GGPS3.SA         GGPS3 09.229.201/0001-30\n",
      "VBBR3.SA         VBBR3 34.274.233/0001-02\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\ticker_cnpj_map_sample.csv\n",
      "\n",
      "üìÅ DATASET: aurum_scores\\aurum_quality_scores_complete.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 21,120\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 50\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.03 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "          CNPJ_CIA       DENOM_CIA DT_FIM_EXERC  Custo dos Bens e/ou Servi√ßos Vendidos        EBIT          EBT  Lucro Bruto  Lucro L√≠quido Consolidado  Receita L√≠quida  Ativo Circulante  Ativo N√£o Circulante  Ativo Total  Caixa e Equivalentes  D√≠vida Curto Prazo  D√≠vida Longo Prazo  Passivo Circulante  Passivo N√£o Circulante  Passivo Total  Patrim√¥nio L√≠quido Consolidado  Receita L√≠quida_ttm  Custo dos Bens e/ou Servi√ßos Vendidos_ttm  Lucro Bruto_ttm    EBIT_ttm     EBT_ttm  Lucro L√≠quido Consolidado_ttm  D√≠vida Bruta  Capital Investido  D√≠vida L√≠quida      ROE      ROA     ROIC  MARGEM_EBIT  MARGEM_LIQUIDA  MARGEM_BRUTA  ALAVANCAGEM  DIVIDA_PL  DIVIDA_LIQ_EBIT  LIQUIDEZ_CORRENTE  GIRO_ATIVO  score_ROIC  score_ROE  score_MARGEM_EBIT  score_MARGEM_LIQUIDA  score_DIVIDA_LIQ_EBIT  score_LIQUIDEZ_CORRENTE  score_GIRO_ATIVO  aurum_quality_score  final_rank quality_quintile quality_grade\n",
      "08.935.054/0001-50    MULTINER S/A   2016-06-30                          -1.453800e+07 -70676000.0 -119566000.0   12767000.0               -124731000.0       27305000.0       187686000.0          9.143210e+08 1.102007e+09             1125000.0         560184000.0        0.000000e+00         881650000.0            1.479990e+08   1.102007e+09                      72358000.0                  NaN                                        NaN              NaN         NaN         NaN                            NaN  5.601840e+08       6.325420e+08    5.590590e+08      NaN      NaN      NaN          NaN             NaN           NaN          1.0   7.741839              NaN           0.212880         NaN         NaN        NaN                NaN                   NaN                    NaN                 7.278481               NaN            45.727848    0.700000       4¬∫ Quintil             D\n",
      "01.083.200/0001-18 NEOENERGIA S.A.   2017-03-31                          -2.833255e+09 570185000.0  244386000.0  905420000.0                160231000.0     3738675000.0      5710019000.0          2.236694e+10 2.807696e+10            72071000.0        4175336000.0        8.203506e+09        8072768000.0            1.067918e+10   2.807696e+10                    9325016000.0                  NaN                                        NaN              NaN         NaN         NaN                            NaN  1.237884e+10       2.170386e+10    1.230677e+10      NaN      NaN      NaN          NaN             NaN           NaN          1.0   1.327487              NaN           0.707319         NaN         NaN        NaN                NaN                   NaN                    NaN                20.245399               NaN            47.024540    0.640152       4¬∫ Quintil             D\n",
      "02.919.555/0001-67    ARTERIS S.A.   2023-12-31                          -4.437713e+09 446061000.0 -456676000.0 1555036000.0               -442173000.0     5992749000.0      1675336000.0          1.728154e+10 1.895687e+10          1246350000.0        1499813000.0        1.082672e+10        2598648000.0            1.195649e+10   1.895687e+10                    4401735000.0         1153854000.0                               -642739000.0      511115000.0 869575000.0 791169000.0                   1178305000.0  1.232654e+10       1.672827e+10    1.108019e+10 0.267691 0.062157 0.051982     0.753627        1.021191      0.442963          1.0   2.800381        12.742072           0.644695    0.060867        75.0  90.666667          79.666667                  84.0               21.24183                17.455621         64.569536            64.088790    0.126231       1¬∫ Quintil             A\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\aurum_quality_scores_complete_sample.csv\n",
      "\n",
      "üìÅ DATASET: aurum_scores\\aurum_quality_scores_latest.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 727\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 50\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.03 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "          CNPJ_CIA                              DENOM_CIA DT_FIM_EXERC  Custo dos Bens e/ou Servi√ßos Vendidos         EBIT          EBT  Lucro Bruto  Lucro L√≠quido Consolidado  Receita L√≠quida  Ativo Circulante  Ativo N√£o Circulante  Ativo Total  Caixa e Equivalentes  D√≠vida Curto Prazo  D√≠vida Longo Prazo  Passivo Circulante  Passivo N√£o Circulante  Passivo Total  Patrim√¥nio L√≠quido Consolidado  Receita L√≠quida_ttm  Custo dos Bens e/ou Servi√ßos Vendidos_ttm  Lucro Bruto_ttm     EBIT_ttm       EBT_ttm  Lucro L√≠quido Consolidado_ttm  D√≠vida Bruta  Capital Investido  D√≠vida L√≠quida       ROE       ROA      ROIC  MARGEM_EBIT  MARGEM_LIQUIDA  MARGEM_BRUTA  ALAVANCAGEM  DIVIDA_PL  DIVIDA_LIQ_EBIT  LIQUIDEZ_CORRENTE  GIRO_ATIVO  score_ROIC  score_ROE  score_MARGEM_EBIT  score_MARGEM_LIQUIDA  score_DIVIDA_LIQ_EBIT  score_LIQUIDEZ_CORRENTE  score_GIRO_ATIVO  aurum_quality_score  final_rank quality_quintile quality_grade\n",
      "36.542.025/0001-64       BRQ SOLUCOES EM INFORMATICA S.A.   2025-06-30                          -1.810170e+08   39501000.0   44187000.0   96271000.0                 32362000.0     2.772880e+08      2.290510e+08          1.318970e+08 3.609480e+08            20618000.0           1508000.0        2.933000e+06        9.343100e+07            3.224200e+07   3.609480e+08                    2.352750e+08          -10637000.0                               6.670000e+06       -3967000.0   -6030000.0 -1.107000e+06                      2402000.0  4.441000e+06       2.397160e+08   -1.617700e+07  0.010209  0.006655 -0.025155     0.566889       -0.225816      0.372943          1.0   0.018876         2.682753           2.451552   -0.029470   13.053613  57.109557          78.959811             23.640662              54.418605                75.694444         10.023310            42.772441    0.772822       4¬∫ Quintil             D\n",
      "88.613.658/0001-10        PETTENATI S.A. INDUSTRIA TEXTIL   2025-06-30                          -7.162344e+08   54900157.0   49544388.0  137888957.0                 57768709.0     8.541234e+08      5.697737e+08          4.551457e+08 1.024919e+09           236167073.0          89624208.0        2.770688e+08        2.297122e+08            2.850721e+08   1.024919e+09                    5.101351e+08          107673113.0                              -8.362598e+07       24047130.0   15378451.0  1.201287e+07                     23797073.0  3.666930e+08       8.768280e+08    1.305259e+08  0.046649  0.023218  0.017539     0.142825        0.221012      0.223335          1.0   0.718815         8.487584           2.480380    0.105055   71.328671  79.020979          47.517730             63.593381              41.395349                76.851852         87.878788            65.854679    0.107055       1¬∫ Quintil             A\n",
      "17.155.730/0001-64 CIA ENERGETICA DE MINAS GERAIS - CEMIG   2025-06-30                          -1.663973e+10 3104447000.0 2591856000.0 3990799000.0               2227021000.0     2.063053e+10      1.361220e+10          4.979903e+10 6.341123e+10          1757787000.0        2747228000.0        1.251671e+10        1.367308e+10            2.125929e+10   6.341123e+10                    2.847886e+10         2136668000.0                              -2.675016e+09     -538348000.0 -611049000.0 -1.060773e+09                   -614456000.0  1.526394e+10       4.374280e+10    1.350615e+10 -0.021576 -0.009690 -0.013969    -0.285982       -0.287577     -0.251957          1.0   0.535974       -22.103219           0.995547    0.033695   18.881119  26.573427          18.439716             21.276596              81.860465                23.379630         62.470862            34.464030    0.915199       5¬∫ Quintil             E\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\aurum_quality_scores_latest_sample.csv\n",
      "\n",
      "üìÅ DATASET: cvm\\final\\fundamentals_wide.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 21,120\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 19\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.01 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "          CNPJ_CIA       DENOM_CIA DT_FIM_EXERC  Custo dos Bens e/ou Servi√ßos Vendidos        EBIT          EBT  Lucro Bruto  Lucro L√≠quido Consolidado  Receita L√≠quida  Ativo Circulante  Ativo N√£o Circulante  Ativo Total  Caixa e Equivalentes  D√≠vida Curto Prazo  D√≠vida Longo Prazo  Passivo Circulante  Passivo N√£o Circulante  Passivo Total  Patrim√¥nio L√≠quido Consolidado\n",
      "08.935.054/0001-50    MULTINER S/A   2016-06-30                          -1.453800e+07 -70676000.0 -119566000.0   12767000.0               -124731000.0       27305000.0       187686000.0          9.143210e+08 1.102007e+09             1125000.0         560184000.0        0.000000e+00         881650000.0            1.479990e+08   1.102007e+09                      72358000.0\n",
      "01.083.200/0001-18 NEOENERGIA S.A.   2017-03-31                          -2.833255e+09 570185000.0  244386000.0  905420000.0                160231000.0     3738675000.0      5710019000.0          2.236694e+10 2.807696e+10            72071000.0        4175336000.0        8.203506e+09        8072768000.0            1.067918e+10   2.807696e+10                    9325016000.0\n",
      "02.919.555/0001-67    ARTERIS S.A.   2023-12-31                          -4.437713e+09 446061000.0 -456676000.0 1555036000.0               -442173000.0     5992749000.0      1675336000.0          1.728154e+10 1.895687e+10          1246350000.0        1499813000.0        1.082672e+10        2598648000.0            1.195649e+10   1.895687e+10                    4401735000.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\fundamentals_wide_sample.csv\n",
      "\n",
      "üìÅ DATASET: cvm\\processed\\raw_bpa.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 1,495,848\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 14\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.02 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "          CNPJ_CIA   DT_REFER  VERSAO                              DENOM_CIA  CD_CVM                                  GRUPO_DFP MOEDA ESCALA_MOEDA ORDEM_EXERC DT_FIM_EXERC   CD_CONTA                     DS_CONTA    VL_CONTA ST_CONTA_FIXA\n",
      "02.365.069/0001-44 2013-06-30       1                    PADTEC HOLDING S.A.   18414 DF Consolidado - Balan√ßo Patrimonial Ativo  REAL          MIL      √öLTIMO   2013-06-30 1.01.08.01 Ativos N√£o-Correntes a Venda         0.0             S\n",
      "22.677.520/0001-76 2018-03-31       1   CIA TECIDOS NORTE DE MINAS COTEMINAS    3158 DF Consolidado - Balan√ßo Patrimonial Ativo  REAL          MIL      √öLTIMO   2018-03-31 1.01.03.01                     Clientes 593942000.0             S\n",
      "17.155.730/0001-64 2018-03-31       1 CIA ENERGETICA DE MINAS GERAIS - CEMIG    2453 DF Consolidado - Balan√ßo Patrimonial Ativo  REAL          MIL      √öLTIMO   2018-03-31 1.02.03.03     Imobilizado em Andamento  98447000.0             S\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\raw_bpa_sample.csv\n",
      "\n",
      "üìÅ DATASET: cvm\\processed\\raw_bpp.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 2,536,167\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 14\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.02 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "          CNPJ_CIA   DT_REFER  VERSAO                        DENOM_CIA  CD_CVM                                    GRUPO_DFP MOEDA ESCALA_MOEDA ORDEM_EXERC DT_FIM_EXERC      CD_CONTA                                    DS_CONTA  VL_CONTA ST_CONTA_FIXA\n",
      "24.396.489/0001-20 2023-03-31       2 BRK AMBIENTAL PARTICIPA√á√ïES S.A.   24830 DF Consolidado - Balan√ßo Patrimonial Passivo  REAL          MIL      √öLTIMO   2023-03-31 2.01.06.01.02    Provis√µes Previdenci√°rias e Trabalhistas       0.0             S\n",
      "18.593.815/0001-97 2023-12-31       2 PRINER SERVI√áOS INDUSTRIAIS S.A.   24236 DF Consolidado - Balan√ßo Patrimonial Passivo  REAL          MIL      √öLTIMO   2023-12-31 2.01.05.02.01                    Dividendos e JCP a Pagar       0.0             S\n",
      "35.727.157/0001-06 2023-12-31       1        EZ INC INCORPORA√á√ïES S.A.   25380 DF Consolidado - Balan√ßo Patrimonial Passivo  REAL          MIL      √öLTIMO   2023-12-31 2.02.02.02.01 Obriga√ß√µes por Pagamentos Baseados em A√ß√µes       0.0             S\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\raw_bpp_sample.csv\n",
      "\n",
      "üìÅ DATASET: cvm\\processed\\raw_dre.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 880,935\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 15\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.03 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "          CNPJ_CIA   DT_REFER  VERSAO                     DENOM_CIA  CD_CVM                                  GRUPO_DFP MOEDA ESCALA_MOEDA ORDEM_EXERC DT_INI_EXERC DT_FIM_EXERC CD_CONTA                                                        DS_CONTA  VL_CONTA ST_CONTA_FIXA\n",
      "84.683.671/0001-94 2019-12-31       1                   WETZEL S.A.   11991 DF Consolidado - Demonstra√ß√£o do Resultado  REAL          MIL      √öLTIMO   2019-01-01   2019-12-31  3.10.02 Ganhos/Perdas L√≠quidas sobre Ativos de Opera√ß√µes Descontinuadas    1000.0             S\n",
      "38.482.780/0001-26 2023-06-30       1      ANEMUS WIND HOLDING S.A.   26832 DF Consolidado - Demonstra√ß√£o do Resultado  REAL          MIL      √öLTIMO   2023-01-01   2023-06-30  3.04.05                                    Outras Despesas Operacionais       0.0             S\n",
      "02.998.301/0001-81 2021-06-30       1 RIO PARANAPANEMA ENERGIA S.A.   18368 DF Consolidado - Demonstra√ß√£o do Resultado  REAL          MIL      √öLTIMO   2021-01-01   2021-06-30  3.11.02                            Atribu√≠do a S√≥cios N√£o Controladores       0.0             S\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\raw_dre_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\ABEV3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low  Close  Adj Close   Volume  Dividends  Stock Splits\n",
      "2019-02-26 18.420000 18.580000 18.299999  18.49  13.835294 12685600        0.0           0.0\n",
      "2021-06-08 19.469999 19.860001 19.370001  19.59  15.514716 23209600        0.0           0.0\n",
      "2022-11-21 15.780000 15.800000 15.520000  15.67  12.894517 15656100        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\ABEV3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\all_histories.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 357,154\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 10\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.01 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "  ticker       date  Open  High       Low     Close  Adj Close     Volume  Dividends  Stock Splits\n",
      "USIM5.SA 2012-08-10  8.19  8.25  7.790000  8.020000   6.249056 12935700.0        0.0           0.0\n",
      "IRBR3.SA 2012-08-16   NaN   NaN       NaN       NaN        NaN        NaN        NaN           NaN\n",
      "BRKM5.SA 2013-10-25 19.76 19.83 19.389999 19.549999  11.867913   982700.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\all_histories_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\all_histories_cleaned.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 277,190\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 10\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.01 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "  ticker       date      Open  High       Low     Close  Adj Close     Volume  Dividends  Stock Splits\n",
      "PETR3.SA 2016-02-24  6.830000  7.15  6.740000  7.030000   2.064449 14172300.0        0.0           0.0\n",
      "VALE3.SA 2017-07-10 28.900000 29.75 28.620001 29.709999  16.067547  5053400.0        0.0           0.0\n",
      "SUZB3.SA 2021-12-06 57.259998 59.18 57.160000 57.869999  51.631351  6317600.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\all_histories_cleaned_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\ALOS3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High   Low     Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26       NaN       NaN   NaN       NaN        NaN       NaN        NaN           NaN\n",
      "2021-06-08 31.959999 32.080002 31.15 31.709999  27.563427 1907900.0        0.0           0.0\n",
      "2022-11-21 17.500000 18.490000 17.42 18.330000  15.933070 3554700.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\ALOS3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\ANIM3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26  6.223333  6.316666  6.136666  6.263333   5.404160  527400.0        0.0           0.0\n",
      "2021-06-08 12.750000 12.850000 12.280000 12.580000  10.858155 3451500.0        0.0           0.0\n",
      "2022-11-21  5.100000  5.270000  4.980000  5.230000   4.514162 3765600.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\ANIM3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\ASAI3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open   High       Low     Close  Adj Close     Volume  Dividends  Stock Splits\n",
      "2019-02-26       NaN    NaN       NaN       NaN        NaN        NaN        NaN           NaN\n",
      "2021-06-08 17.414000 17.424 16.709999 16.870001  16.297590  9246000.0        0.0           0.0\n",
      "2022-11-21 19.309999 19.750 19.290001 19.709999  19.242868 10461100.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\ASAI3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\AURE3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date  Open  High  Low  Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26   NaN   NaN  NaN    NaN        NaN       NaN        NaN           NaN\n",
      "2021-06-08   NaN   NaN  NaN    NaN        NaN       NaN        NaN           NaN\n",
      "2022-11-21 13.84 14.46 13.8   14.4  11.279315 4939800.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\AURE3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\AZZA3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26 54.000000 54.439999 53.259998 54.299999  47.791710  318200.0        0.0           0.0\n",
      "2021-06-08 85.339996 89.290001 85.339996 87.599998  79.884254  994000.0        0.0           0.0\n",
      "2022-11-21 88.910004 93.730003 88.449997 93.440002  87.417679 2289000.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\AZZA3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\BBAS3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High    Low  Close  Adj Close   Volume  Dividends  Stock Splits\n",
      "2019-02-26 26.150000 26.459999 26.000  26.25  16.319176 14574600        0.0           0.0\n",
      "2021-06-08 18.200001 18.334999 17.885  18.15  12.630210 41678000        0.0           0.0\n",
      "2022-11-21 17.264999 17.709999 17.105  17.60  13.846167 42765800        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\BBAS3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\BBDC3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 24.780867 25.169044 24.705734 25.050087  16.465574 2358265        0.0           0.0\n",
      "2021-06-08 22.072727 22.290909 21.845453 22.100000  15.851297 6423120        0.0           0.0\n",
      "2022-11-21 13.400000 13.460000 13.040000 13.150000  10.057689 6589300        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\BBDC3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\BBDC4.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close   Volume  Dividends  Stock Splits\n",
      "2019-02-26 27.648384 28.143000 27.529427 28.136740   18.67873 26716683        0.0           0.0\n",
      "2021-06-08 25.772726 26.045454 25.481817 25.672728   18.61459 34704120        0.0           0.0\n",
      "2022-11-21 15.710000 15.830000 15.230000 15.500000   11.92675 46728300        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\BBDC4.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\BBSE3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26 27.709999 27.740000 27.280001 27.469999  15.767140 5499000.0        0.0           0.0\n",
      "2021-06-08 24.620001 24.639999 24.250000 24.309999  16.405121 4723500.0        0.0           0.0\n",
      "2022-11-21 30.459999 30.570000 29.910000 30.459999  22.674213 7547100.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\BBSE3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\BEEF3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26  6.302919  6.312752  6.116093  6.145592   5.000320 1812376        0.0           0.0\n",
      "2021-06-08 10.090000 10.120000  9.850000  9.890000   8.783260 6892600        0.0           0.0\n",
      "2022-11-21 13.350000 13.610000 12.920000 13.000000  12.343428 7404200        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\BEEF3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\BPAC11.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low  Close  Adj Close     Volume  Dividends  Stock Splits\n",
      "2019-02-26  8.162500  8.205000  8.030000  8.135   7.033854  2338400.0        0.0           0.0\n",
      "2021-06-08 30.610001 30.905001 30.139999 30.875  27.589521 18486800.0        0.0           0.0\n",
      "2022-11-21 25.360001 25.650000 24.459999 25.270  23.258596  7717200.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\BPAC11.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\BRAP4.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 13.336407 13.442434 13.239599 13.345626   5.154035 2003302        0.0           0.0\n",
      "2021-06-08 32.324535 32.476662 31.716030 31.794399  15.927111 4098363        0.0           0.0\n",
      "2022-11-21 26.760000 26.990000 26.280001 26.559999  19.707737 3983300        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\BRAP4.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\BRAV3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26       NaN       NaN       NaN       NaN        NaN       NaN        NaN           NaN\n",
      "2021-06-08 39.980000 40.419998 39.240002 40.009998  40.009998  942200.0        0.0           0.0\n",
      "2022-11-21 37.700001 38.980000 36.529999 37.650002  37.650002 4593000.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\BRAV3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\BRKM5.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 55.669998 57.200001 55.669998 56.029999  46.358631 1248800        0.0           0.0\n",
      "2021-06-08 60.740002 61.529999 56.110001 56.369999  47.953728 4419800        0.0           0.0\n",
      "2022-11-21 29.620001 29.670000 28.500000 29.469999  29.469999 1563100        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\BRKM5.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\CEAB3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date  Open  High   Low  Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26   NaN   NaN   NaN    NaN        NaN       NaN        NaN           NaN\n",
      "2021-06-08 14.48 15.06 14.48  14.96  14.278788 1502800.0        0.0           0.0\n",
      "2022-11-21  2.33  2.40  2.27   2.37   2.262081 3423000.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\CEAB3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\CMIG4.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date     Open     High      Low    Close  Adj Close   Volume  Dividends  Stock Splits\n",
      "2019-02-26 7.217858 7.355487 7.192371 7.314708   3.549256 17112002        0.0           0.0\n",
      "2021-06-08 8.242603 8.260355 8.147928 8.236686   4.626351 11888136        0.0           0.0\n",
      "2022-11-21 8.007692 8.730769 8.007692 8.553846   5.760123 36965500        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\CMIG4.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\CMIN3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date  Open  High  Low  Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26   NaN   NaN  NaN    NaN        NaN       NaN        NaN           NaN\n",
      "2021-06-08  9.37  9.46 9.23   9.31   5.268619 3924200.0        0.0           0.0\n",
      "2022-11-21  3.64  3.66 3.52   3.56   2.607551 6842700.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\CMIN3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\COGN3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date  Open  High   Low  Close  Adj Close     Volume  Dividends  Stock Splits\n",
      "2019-02-26 11.18 11.21 10.96  11.02  10.603634  5338500.0        0.0           0.0\n",
      "2021-06-08  4.62  4.70  4.55   4.56   4.439746 33168000.0        0.0           0.0\n",
      "2022-11-21  2.27  2.34  2.22   2.31   2.249082 31846900.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\COGN3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\CPFE3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High   Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 31.740000 31.799999 31.26 31.440001  21.373053  379000        0.0           0.0\n",
      "2021-06-08 29.959999 29.990000 29.58 29.750000  22.913847  877300        0.0           0.0\n",
      "2022-11-21 34.150002 34.459999 33.32 34.020000  31.153282 1439300        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\CPFE3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\CPLE3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date  Open  High   Low  Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 3.173 3.185 3.136   3.18   2.136068 1131000        0.0           0.0\n",
      "2021-06-08 5.770 5.780 5.680   5.72   4.273674 1264200        0.0           0.0\n",
      "2022-11-21 7.800 8.360 7.680   7.97   7.045704 8152300        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\CPLE3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\CPLE6.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date     Open     High      Low    Close  Adj Close     Volume  Dividends  Stock Splits\n",
      "2019-02-26 3.439034 3.483021 3.415040 3.479022   2.163306  5940668.0        0.0           0.0\n",
      "2021-06-08 6.378208 6.398202 6.258242 6.278236   4.693832  5960173.0        0.0           0.0\n",
      "2022-11-21 8.707554 9.117439 8.457624 8.737545   7.717256 72383924.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\CPLE6.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\CSAN3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High     Low     Close  Adj Close   Volume  Dividends  Stock Splits\n",
      "2019-02-26 10.955000 11.312500 10.9075 11.287500   9.663434  7775600        0.0           0.0\n",
      "2021-06-08 24.700001 25.160000 24.2600 24.940001  22.543461 10796400        0.0           0.0\n",
      "2022-11-21 16.950001 17.389999 16.3400 16.770000  15.771297  9535900        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\CSAN3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\CSMG3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 20.673332 20.836666 20.453333 20.763332  10.983916  982800        0.0           0.0\n",
      "2021-06-08 17.799999 17.809999 17.450001 17.590000  11.717224 1336100        0.0           0.0\n",
      "2022-11-21 14.600000 15.640000 14.560000 15.530000  11.065383 2280300        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\CSMG3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\CSNA3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High   Low     Close  Adj Close   Volume  Dividends  Stock Splits\n",
      "2019-02-26 13.200000 13.320000 12.71 13.190000   7.933288 25779600        0.0           0.0\n",
      "2021-06-08 44.470001 44.740002 43.75 43.799999  28.633474  8439100        0.0           0.0\n",
      "2022-11-21 14.150000 14.180000 13.36 13.930000   9.750651 10084900        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\CSNA3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\CURY3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date  Open  High   Low  Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26   NaN   NaN   NaN    NaN        NaN       NaN        NaN           NaN\n",
      "2021-06-08  9.70  9.79  9.57   9.67   7.018286  534100.0        0.0           0.0\n",
      "2022-11-21 10.87 11.48 10.85  11.19   8.958047 1218100.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\CURY3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\CVCB3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close     Volume  Dividends  Stock Splits\n",
      "2019-02-26 49.822262 49.822262 48.973347 49.426651  48.929474  1729699.0        0.0           0.0\n",
      "2021-06-08 24.097122 26.251934 24.097122 24.508814  24.508814 21735381.0        0.0           0.0\n",
      "2022-11-21  5.240244  5.323130  5.065262  5.267872   5.267872 15044245.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\CVCB3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\CXSE3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date  Open  High   Low  Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26   NaN   NaN   NaN    NaN        NaN       NaN        NaN           NaN\n",
      "2021-06-08 11.76 11.77 11.01  11.10   7.940366 4315100.0        0.0           0.0\n",
      "2022-11-21  7.95  8.09  7.92   8.01   6.352328  664400.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\CXSE3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\CYRE3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low  Close  Adj Close   Volume  Dividends  Stock Splits\n",
      "2019-02-26 17.070000 17.150000 16.809999  17.01  12.192803  5978900        0.0           0.0\n",
      "2021-06-08 25.950001 26.540001 25.809999  26.18  22.324690  7496100        0.0           0.0\n",
      "2022-11-21 13.900000 14.780000 13.840000  14.70  13.028086 11414000        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\CYRE3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\DIRR3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date     Open     High      Low    Close  Adj Close   Volume  Dividends  Stock Splits\n",
      "2019-02-26 2.884728 2.931202 2.834934 2.907965   1.818404  2274373        0.0           0.0\n",
      "2021-06-08 4.793495 4.893083 4.737062 4.846609   3.650257  5605502        0.0           0.0\n",
      "2022-11-21 4.537886 4.800134 4.537886 4.727103   3.710055 16008879        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\DIRR3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\ECOR3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 10.824980 11.091653 10.745965 10.864487   9.980733 2506984        0.0           0.0\n",
      "2021-06-08 13.126275 13.205290 12.899109 13.057138  11.995027 3037520        0.0           0.0\n",
      "2022-11-21  4.500000  4.630000  4.430000  4.580000   4.207448 4148700        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\ECOR3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\EGIE3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 42.110001 42.110001 41.279999 41.480000  28.733232 1193600        0.0           0.0\n",
      "2021-06-08 41.200001 41.439999 40.900002 40.950001  31.112019 1311700        0.0           0.0\n",
      "2022-11-21 40.200001 40.790001 40.110001 40.180000  33.058163 2278300        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\EGIE3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\ELET3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close   Volume  Dividends  Stock Splits\n",
      "2019-02-26 37.285831 37.645695 36.556107 37.195866  28.559843  2698625        0.0           0.0\n",
      "2021-06-08 46.070000 46.360001 44.799999 45.230000  40.002071  5287300        0.0           0.0\n",
      "2022-11-21 44.980000 46.000000 44.189999 45.619999  41.140373 11063500        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\ELET3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\ELET6.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 39.230000 39.700001 38.529999 39.320000  26.879248 1674700        0.0           0.0\n",
      "2021-06-08 45.950001 46.160000 44.389999 44.740002  36.090542 2635300        0.0           0.0\n",
      "2022-11-21 48.330002 49.299999 47.490002 48.959999  41.130291 4037200        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\ELET6.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\EMBR3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High   Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 19.620001 20.290001 19.60 19.700001  19.679752 5073200        0.0           0.0\n",
      "2021-06-08 17.570000 18.100000 17.40 17.549999  17.531961 8078300        0.0           0.0\n",
      "2022-11-21 14.340000 14.500000 14.08 14.360000  14.345241 5846200        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\EMBR3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\ENEV3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date    Open      High     Low   Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26  4.6875  4.687500  4.6175  4.6575     4.6575 1386000        0.0           0.0\n",
      "2021-06-08 17.6000 18.120001 17.3300 17.4000    17.4000 7656300        0.0           0.0\n",
      "2022-11-21 13.0100 13.060000 12.6600 12.8500    12.8500 6714400        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\ENEV3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\ENGI11.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 41.709999 41.740002 41.150002 41.540001  30.859852  276800        0.0           0.0\n",
      "2021-06-08 48.509998 49.820000 48.509998 49.099998  38.782677 2274300        0.0           0.0\n",
      "2022-11-21 45.959999 45.959999 44.980000 45.599998  39.481125  933200        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\ENGI11.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\EQTL3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 16.810110 16.810110 16.622334 16.702238  14.679322 3025575        0.0           0.0\n",
      "2021-06-08 24.750717 24.810646 24.391142 24.630859  22.835663 6300845        0.0           0.0\n",
      "2022-11-21 27.737185 27.946936 27.177845 27.697231  26.331347 6931891        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\EQTL3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\EZTC3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 21.326483 21.532734 21.087229 21.367731  17.620197  778658        0.0           0.0\n",
      "2021-06-08 34.119999 34.840000 33.740002 34.330002  29.120731 2665900        0.0           0.0\n",
      "2022-11-21 15.130000 15.900000 15.120000 15.850000  14.042307 3162100        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\EZTC3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\FLRY3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 19.208658 19.255808 18.906902 19.048349  14.041732 1948381        0.0           0.0\n",
      "2021-06-08 25.630404 26.290495 25.583254 25.790712  20.924410 2079454        0.0           0.0\n",
      "2022-11-21 16.371428 16.857141 16.342857 16.704762  14.301907 4994850        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\FLRY3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\GGBR4.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close   Volume  Dividends  Stock Splits\n",
      "2019-02-26 12.119047 12.333333 12.087301 12.285714   8.173514 11517030        0.0           0.0\n",
      "2021-06-08 25.412699 25.666666 24.873014 24.873014  17.401888 14707224        0.0           0.0\n",
      "2022-11-21 23.880953 24.007936 23.341269 23.888887  19.535353 16460640        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\GGBR4.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\GGPS3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date  Open  High       Low  Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26   NaN   NaN       NaN    NaN        NaN       NaN        NaN           NaN\n",
      "2021-06-08 17.75 19.00 17.379999  17.90  16.627340 1157800.0        0.0           0.0\n",
      "2022-11-21 12.57 13.15 12.500000  12.95  12.166279 2338900.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\GGPS3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\GMAT3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date  Open  High  Low  Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26   NaN   NaN  NaN    NaN        NaN       NaN        NaN           NaN\n",
      "2021-06-08  7.95  8.01 7.85   7.93   7.519714 3643000.0        0.0           0.0\n",
      "2022-11-21  6.25  6.39 6.13   6.33   6.002496 5212600.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\GMAT3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\GOAU4.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date  Open  High   Low  Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26  7.27  7.37  7.23   7.31   4.214604 5934200        0.0           0.0\n",
      "2021-06-08 14.19 14.41 14.07  14.10   8.634726 9778600        0.0           0.0\n",
      "2022-11-21 12.90 12.98 12.67  12.97  10.192628 8923800        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\GOAU4.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\HAPV3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date       Open  High        Low      Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26  97.290001  99.0  95.040001  96.839996  95.135124  332133.0        0.0           0.0\n",
      "2021-06-08 235.350006 237.0 234.449997 237.000000 236.407562  523580.0        0.0           0.0\n",
      "2022-11-21  87.299995  88.5  82.349998  84.000000  84.000000 3522826.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\HAPV3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\HYPE3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 27.250000 27.700001 26.650000 26.799999  20.822184 4237300        0.0           0.0\n",
      "2021-06-08 36.950001 37.000000 35.810001 35.810001  30.168341 2567200        0.0           0.0\n",
      "2022-11-21 45.459999 45.790001 45.040001 45.540001  40.415058 2801400        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\HYPE3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\IGTI11.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date  Open  High       Low  Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26   NaN   NaN       NaN    NaN        NaN       NaN        NaN           NaN\n",
      "2021-06-08   NaN   NaN       NaN    NaN        NaN       NaN        NaN           NaN\n",
      "2022-11-21 17.93 18.92 17.860001  18.91  17.363768 2685400.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\IGTI11.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\INTB3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date  Open  High       Low  Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26   NaN   NaN       NaN    NaN        NaN       NaN        NaN           NaN\n",
      "2021-06-08 30.32 30.34 29.320000  30.01  26.969774 2177400.0        0.0           0.0\n",
      "2022-11-21 31.09 31.84 30.120001  31.49  28.819441  981100.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\INTB3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\IRBR3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date       Open       High        Low      Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26 718.606812 725.507629 703.731689 718.836792 693.370850  234558.0        0.0           0.0\n",
      "2021-06-08 156.817688 157.066605 151.092606 151.590439 151.590439 1284990.0        0.0           0.0\n",
      "2022-11-21  22.799999  23.700001  22.200001  23.700001  23.700001 2204199.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\IRBR3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\ISAE4.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26 19.410000 19.490000 19.172501 19.282499  11.741533  798800.0        0.0           0.0\n",
      "2021-06-08 27.320000 27.320000 26.670000 26.770000  19.823059 1334800.0        0.0           0.0\n",
      "2022-11-21 22.879999 23.610001 22.840000 23.510000  18.879108 2638600.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\ISAE4.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\ITSA4.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close   Volume  Dividends  Stock Splits\n",
      "2019-02-26 10.040874 10.087794 10.009594 10.025234   6.628523 23744131   0.000000           0.0\n",
      "2021-06-08  9.368354  9.399634  9.211954  9.337074   6.817702 46337111   0.000000           0.0\n",
      "2022-11-21  8.146949  8.155981  7.939211  8.011468   6.253462 49390425   0.046348           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\ITSA4.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\ITUB4.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close   Volume  Dividends  Stock Splits\n",
      "2019-02-26 32.900002 33.109089 32.809090 32.936363  23.648113  8858410        0.0           0.0\n",
      "2021-06-08 29.836363 30.136362 29.327272 29.827272  23.325546 79384030        0.0           0.0\n",
      "2022-11-21 24.545454 24.672728 23.963636 24.290909  19.740046 42705520        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\ITUB4.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\KLBN11.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26 16.636362 17.200001 16.627272 17.063637  12.738850 2607770.0        0.0           0.0\n",
      "2021-06-08 23.400000 23.627272 22.872726 22.963636  18.043665 7543580.0        0.0           0.0\n",
      "2022-11-21 20.163635 20.336363 19.400000 19.436363  16.619034 6349200.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\KLBN11.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\LREN3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close   Volume  Dividends  Stock Splits\n",
      "2019-02-26 32.878323 33.288277 32.811241 33.094482  27.375139  2239421        0.0           0.0\n",
      "2021-06-08 40.165291 40.619835 39.859505 40.314049  34.134346 12069750        0.0           0.0\n",
      "2022-11-21 22.427273 23.045454 22.000000 22.827272  19.921400 14692370        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\LREN3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\LWSA3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date  Open      High   Low     Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26   NaN       NaN   NaN       NaN        NaN       NaN        NaN           NaN\n",
      "2021-06-08 23.93 24.360001 23.75 23.950001  23.039543 5550300.0        0.0           0.0\n",
      "2022-11-21  7.68  7.840000  7.52  7.770000   7.474624 6545400.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\LWSA3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\MBRF3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close   Volume  Dividends  Stock Splits\n",
      "2019-02-26  5.478759  5.518532  5.399213  5.419099   3.335195  4047548        0.0           0.0\n",
      "2021-06-08 18.226072 18.226072 17.599646 18.057037  11.238177 16332903        0.0           0.0\n",
      "2022-11-21 10.032792 10.032792  9.475967  9.744436   7.384258 15325894        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\MBRF3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\MGLU3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date       Open       High        Low      Close  Adj Close     Volume  Dividends  Stock Splits\n",
      "2019-02-26  50.568840  52.573940  50.039799  52.173504  50.254650  3611499.0        0.0           0.0\n",
      "2021-06-08 197.259689 200.439789 195.014908 196.885559 190.997330  3750328.0        0.0           0.0\n",
      "2022-11-21  30.304476  32.549252  29.930346  31.613928  30.690695 26862197.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\MGLU3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\MOTV3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date  Open  High  Low  Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26   NaN   NaN  NaN    NaN        NaN     NaN        NaN           NaN\n",
      "2021-06-08   NaN   NaN  NaN    NaN        NaN     NaN        NaN           NaN\n",
      "2022-11-21   NaN   NaN  NaN    NaN        NaN     NaN        NaN           NaN\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\MOTV3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\MOVI3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26 11.120477 11.575585 11.110583 11.526117   9.305655 1631347.0        0.0           0.0\n",
      "2021-06-08 19.500000 19.580000 19.120001 19.260000  16.022169 1732100.0        0.0           0.0\n",
      "2022-11-21  8.980000  9.480000  8.980000  9.430000   8.709580 4159900.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\MOVI3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\MRVE3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High   Low  Close  Adj Close   Volume  Dividends  Stock Splits\n",
      "2019-02-26 13.790000 13.900000 13.31  13.36  11.364754  7273900        0.0           0.0\n",
      "2021-06-08 17.950001 18.219999 17.82  18.00  17.057289  3118500        0.0           0.0\n",
      "2022-11-21  8.440000  8.700000  8.33   8.62   8.620000 11330100        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\MRVE3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\MULT3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date  Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 25.40 25.490000 25.190001 25.250000  20.552223 2094500        0.0           0.0\n",
      "2021-06-08 27.59 27.700001 26.760000 27.180000  22.910461 8274600        0.0           0.0\n",
      "2022-11-21 22.73 23.740000 22.600000 23.549999  20.756254 8513100        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\MULT3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\NATU3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open  High       Low     Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26 49.970001  50.5 49.099998 49.099998  48.835907 1629400.0        0.0           0.0\n",
      "2021-06-08       NaN   NaN       NaN       NaN        NaN       NaN        NaN           NaN\n",
      "2022-11-21       NaN   NaN       NaN       NaN        NaN       NaN        NaN           NaN\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\NATU3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\PCAR3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 12.380301 12.380301 12.380301 12.380301  10.276521       0        0.0           0.0\n",
      "2021-06-08 40.360001 40.630001 39.060001 39.250000  38.669945 3772100        0.0           0.0\n",
      "2022-11-21 18.219999 18.490000 17.629999 18.400000  18.400000 2349200        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\PCAR3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\PETR3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close   Volume  Dividends  Stock Splits\n",
      "2019-02-26 30.549999 31.090000 30.350000 30.510000   9.047254  7813200        0.0           0.0\n",
      "2021-06-08 28.490000 29.559999 28.440001 29.410000   9.263550 51187500        0.0           0.0\n",
      "2022-11-21 30.940001 31.290001 29.530001 30.280001  16.395691 33579600        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\PETR3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\PETR4.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26 26.760000 27.139999 26.570000 26.580000   7.620859  46504200        0.0           0.0\n",
      "2021-06-08 28.170000 28.799999 27.959999 28.660000   8.776088  88398600        0.0           0.0\n",
      "2022-11-21 27.120001 27.570000 26.049999 26.780001  14.613328 164898100        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\PETR4.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\PETZ3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close     Volume  Dividends  Stock Splits\n",
      "2019-02-26       NaN       NaN       NaN       NaN        NaN        NaN        NaN           NaN\n",
      "2021-06-08 22.932577 24.119089 22.932577 23.740204  22.163120  3695031.0        0.0           0.0\n",
      "2022-11-21  7.430000  7.930000  7.390000  7.810000   7.309255 10345200.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\PETZ3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\POMO4.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date  Open  High  Low  Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26  3.97  4.02 3.92   3.97   2.931461 3047400        0.0           0.0\n",
      "2021-06-08  2.97  3.00 2.92   2.99   2.292338 5140600        0.0           0.0\n",
      "2022-11-21  2.40  2.44 2.38   2.42   1.905555 3397300        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\POMO4.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\prices_close_wide.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 356\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 97\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.04 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      " ABEV3.SA  ALOS3.SA  ANIM3.SA  ASAI3.SA  AURE3.SA  AZZA3.SA  BBAS3.SA  BBDC3.SA  BBDC4.SA  BBSE3.SA  BEEF3.SA  BPAC11.SA  BRAP4.SA  BRAV3.SA  BRKM5.SA  CEAB3.SA  CMIG4.SA  CMIN3.SA  COGN3.SA  CPFE3.SA  CPLE3.SA  CPLE6.SA  CSAN3.SA  CSMG3.SA  CSNA3.SA  CURY3.SA  CVCB3.SA  CXSE3.SA  CYRE3.SA  DIRR3.SA  ECOR3.SA  EGIE3.SA  ELET3.SA  ELET6.SA  EMBR3.SA   ENEV3.SA  ENGI11.SA  EQTL3.SA  EZTC3.SA  FLRY3.SA  GGBR4.SA  GGPS3.SA  GMAT3.SA  GOAU4.SA   HAPV3.SA  HYPE3.SA  IGTI11.SA  INTB3.SA   IRBR3.SA  ISAE4.SA  ITSA4.SA  ITUB4.SA  KLBN11.SA  LREN3.SA  LWSA3.SA  MBRF3.SA   MGLU3.SA  MOTV3.SA  MOVI3.SA  MRVE3.SA  MULT3.SA  NATU3.SA  PCAR3.SA  PETR3.SA  PETR4.SA  PETZ3.SA  POMO4.SA  PRIO3.SA  PSSA3.SA  RADL3.SA  RAIL3.SA  RAIZ4.SA  RAPT4.SA  RDOR3.SA  RECV3.SA  RENT3.SA  SANB11.SA  SAPR11.SA  SBSP3.SA  SLCE3.SA  SMFT3.SA  SMTO3.SA  SRNA3.SA  SUZB3.SA  TAEE11.SA  TEND3.SA  TIMS3.SA  TOTS3.SA  UGPA3.SA  USIM5.SA  VALE3.SA  VAMO3.SA  VBBR3.SA  VIVA3.SA  VIVT3.SA  WEGE3.SA  YDUQ3.SA\n",
      "10.858255 24.608170  7.069021 14.210446 12.438301 43.293140 10.553456 10.950438 12.090376 17.549564 10.699409  17.012960  7.016386 23.950001 19.753159  9.840529  2.870757  5.028724  6.435684 21.183920  4.233972  3.957074 15.841058 10.933984  6.889184  7.529579 14.950799  7.160637 17.563118  3.534586 11.768191 30.205849 24.437536 22.984213  8.081685  11.245000  38.089371 20.620546 33.408070 18.283411  8.638920 11.889941  7.396440  4.321508 185.262161 27.293575  17.034452 17.075163 252.020721 13.734422  5.325525 17.664330  14.772094 28.832888 10.368715  7.716847 162.176559 13.260954 10.859082 16.304918 16.955715 36.860001  8.805048  6.736579  6.383921 13.283948  2.232821  7.066650 19.731157 19.962296 21.358828  6.261024  7.538247 62.282734 11.834115 35.488174  19.001083  22.680399 51.764095  7.694532 25.971813 16.788580     12.65 32.823872  16.441277 28.787485  9.581703 21.548677  7.146783  5.872503 32.646278  7.560617 15.368533 19.308979 17.230247 23.268408 30.652241\n",
      " 7.307890 32.400036  5.146394 14.210446 12.438301 26.546923  5.040816  4.858829  5.978752  6.693225  8.777396   3.418429  2.508829 23.950001  8.662666 15.770802  2.181996  5.028724  1.578388 12.700722  1.208982  1.424330  5.800923  6.226844  5.522790  7.529579 10.886316  7.160637 10.458101  1.699417 10.951047 11.233257  6.714444  8.215002 12.689289 150.418442   5.798319  2.838789 10.524903  6.580122  9.204766 11.889941  7.396440 12.949300  80.980492 10.385984  17.034452 17.075163 202.923904  1.160548  1.903793  6.496866   7.068090  6.848051  5.112352  7.221158   3.202985 13.260954  6.853962  6.846700 14.367400 50.124546  6.951085  6.598839  5.534229 13.283948  3.783113  4.442864  4.903344  3.909372 12.346144  6.261024  6.333354 62.282734 11.834115  7.667813   5.873339  12.423540 19.958494  2.688236 25.971813  5.592539     12.65 15.378759   6.531593  6.127604  4.389012 11.193801  7.351946  7.885342 16.086060  7.560617 14.121199 22.094751  7.880558  2.742952  7.837525\n",
      "13.542709 21.817789  8.363714 16.239624 12.438301 81.805214 10.763491 13.319953 15.606302 13.178593  7.424474  25.550770 15.237814 38.439999 56.137421  8.485189  4.490087  4.179429  3.125347 22.366997  4.445517  5.112391 19.931168  9.381406 23.424456  5.704625 19.398508  7.060487 17.242369  3.175223  9.048768 29.270876 33.280529 30.468029 23.395929  15.670000  35.579956 23.465357 22.733343 18.203981 16.009951 16.655205  6.761105  8.079940 220.508560 30.230061  17.034452 26.115528 132.921661 18.582689  6.622112 22.092615  18.793695 26.691090 23.559013 12.680078 165.620804 13.260954 15.523862 12.973016 17.785530 36.860001 27.911327  9.304611  8.821848 24.660591  2.166147 19.139259 23.776445 23.171721 17.778444  6.261024  9.581620 63.080696 12.951375 48.415108  30.474867  14.740292 32.717281 13.894975 25.508875 26.785912     12.65 54.423927  25.379274 19.185459  8.790601 37.288403 12.634786 15.026068 63.960358 13.332988 21.681366 30.618193 16.908766 33.124191 23.682281\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\prices_close_wide_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\prices_open_wide.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 356\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 97\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.04 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      " ABEV3.SA  ALOS3.SA  ANIM3.SA  ASAI3.SA  AURE3.SA  AZZA3.SA  BBAS3.SA  BBDC3.SA  BBDC4.SA  BBSE3.SA  BEEF3.SA  BPAC11.SA  BRAP4.SA  BRAV3.SA  BRKM5.SA  CEAB3.SA  CMIG4.SA  CMIN3.SA  COGN3.SA  CPFE3.SA  CPLE3.SA  CPLE6.SA  CSAN3.SA  CSMG3.SA  CSNA3.SA  CURY3.SA  CVCB3.SA  CXSE3.SA  CYRE3.SA  DIRR3.SA  ECOR3.SA  EGIE3.SA  ELET3.SA  ELET6.SA  EMBR3.SA   ENEV3.SA  ENGI11.SA  EQTL3.SA  EZTC3.SA  FLRY3.SA  GGBR4.SA  GGPS3.SA  GMAT3.SA  GOAU4.SA   HAPV3.SA  HYPE3.SA  IGTI11.SA  INTB3.SA   IRBR3.SA  ISAE4.SA  ITSA4.SA  ITUB4.SA  KLBN11.SA  LREN3.SA  LWSA3.SA  MBRF3.SA   MGLU3.SA  MOTV3.SA  MOVI3.SA  MRVE3.SA  MULT3.SA  NATU3.SA  PCAR3.SA  PETR3.SA  PETR4.SA  PETZ3.SA  POMO4.SA  PRIO3.SA  PSSA3.SA  RADL3.SA  RAIL3.SA  RAIZ4.SA  RAPT4.SA  RDOR3.SA  RECV3.SA  RENT3.SA  SANB11.SA  SAPR11.SA  SBSP3.SA  SLCE3.SA  SMFT3.SA  SMTO3.SA  SRNA3.SA  SUZB3.SA  TAEE11.SA  TEND3.SA  TIMS3.SA  TOTS3.SA  UGPA3.SA  USIM5.SA  VALE3.SA  VAMO3.SA  VBBR3.SA  VIVA3.SA  VIVT3.SA  WEGE3.SA  YDUQ3.SA\n",
      "12.320000 25.480000      7.30    14.402 16.799999 42.099998    15.385 14.619834 15.611570 25.379999 13.310000  12.210000 16.300566 20.650000 27.650000      9.45  5.418491      8.92  5.290000  32.00000     5.920  6.327222 16.552500 19.233334     10.55      8.91 11.670524     10.19 16.950001  3.220007 12.849725 41.830002 28.629999 30.660000      7.29   9.550000  47.520000 19.776602 32.900002 22.018761 10.912698 12.890000      9.30      6.25 162.990005 31.799999     20.598 20.200001 191.613144     20.85  6.912875 20.754545  17.909090 31.402489  6.575000 12.926293 149.651733     12.88 11.180000 15.210000 20.700001 33.259998  8.234555 20.670000     20.15 15.753684  2.650000     6.342 23.780001 21.094231     22.48      7.48  8.880000 66.334412 14.139331 38.220009  24.471754  27.490000 53.709999 10.347107     29.02 19.450001      12.5 38.310001  28.610001 24.614019     13.38 20.040758  7.772438  6.220000 52.820000  8.334033 21.639999 19.250000 23.580000 20.945000 28.030001\n",
      "12.900684 34.639999      6.50    14.402 16.799999 36.689999    12.355  9.423943 11.578130 16.950001 11.081732   4.550000 12.926126 20.650000 14.240000     18.00  8.213164      8.92  2.065002  21.16613     2.779  3.311070  8.196422 15.057821     11.50      8.91 12.791422     10.19 17.480000  3.737864 17.432564 25.768000 12.155380 18.250000     13.61 150.279175   9.945211  3.561881 18.465221 11.452584 15.261904 12.890000      9.30     24.42  87.000000 14.800000     20.598 20.200001 216.686172      9.37  4.277173 12.741305  10.676363  9.145027  4.950000 11.713210   3.548382     12.88  8.468975 11.081949 19.995525 55.000000  9.734757 23.290001     22.33 15.753684  5.874154     4.500 10.190000  4.461538     13.80      7.48  9.088143 66.334412 14.139331 10.151237  14.516003  19.026667 27.656666  4.545454     29.02  7.960000      12.5 21.120001  24.400000  7.631912      7.88 13.995622 10.479200 10.170000 36.570000  8.334033 23.299999 24.299999 19.905001  3.566568 11.330000\n",
      "17.160000 25.160000      9.76    17.100 16.799999 89.849998    15.260 18.172728 21.163635 19.100000  8.310000  28.360001 28.553648 38.599998 66.480003      8.95  8.088757      7.03  3.230000  27.52000     5.960  6.838079 21.879999 14.010000     34.00      7.91 19.498735      9.86 20.410000  4.222525  9.950000 37.740002 37.650002 37.779999     23.60  15.740000  44.630001 25.869392 26.879999 22.348806 22.222221 17.959999      7.19     12.88 223.500000 35.299999     20.598 29.110001 133.170578     24.66  9.032094 28.236362  24.000000 31.239670 24.549999 20.433487 171.725357     12.88 18.540001 13.700000 21.180000 33.259998 28.450001 28.100000     27.32 26.462200  2.790000    19.230 28.575001 24.625000     18.74      7.10 11.580000 68.980003 16.986767 56.245125  40.571846  19.420000 35.930000 17.983471     27.24 32.209999      12.5 61.299999  38.049999 19.924236     12.50 39.872032 14.480000 17.120001 97.360001 15.380596 27.110001 33.529999 21.855000 35.689999 25.799999\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\prices_open_wide_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\PRIO3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close   Volume  Dividends  Stock Splits\n",
      "2019-02-26  2.858000  2.860000  2.782000  2.836000   2.831452  3795000        0.0           0.0\n",
      "2021-06-08 19.709999 19.750000 19.049999 19.320000  19.289019 10850600        0.0           0.0\n",
      "2022-11-21 34.330002 34.860001 32.860001 34.490002  34.434692 16010300        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\PRIO3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\PSSA3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 29.495001 29.620001 28.209999 28.775000  20.467226 2278200        0.0           0.0\n",
      "2021-06-08 27.370001 27.615000 26.930000 27.344999  22.442741 1580800        0.0           0.0\n",
      "2022-11-21 21.910000 22.510000 21.760000 22.379999  19.880239 1462900        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\PSSA3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\RADL3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 11.346153 11.965384 11.273076 11.719230  10.874806 9806160        0.0           0.0\n",
      "2021-06-08 27.288462 27.692307 26.865383 26.884615  25.317165 4939376        0.0           0.0\n",
      "2022-11-21 22.759615 22.759615 22.038462 22.615383  21.666302 8256248        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\RADL3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\RAIL3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High   Low     Close  Adj Close     Volume  Dividends  Stock Splits\n",
      "2019-02-26 19.209999 19.459999 19.10 19.309999  18.338768  6968600.0        0.0           0.0\n",
      "2021-06-08 21.170000 21.250000 21.01 21.219999  20.152702  4903800.0        0.0           0.0\n",
      "2022-11-21 19.639999 20.070000 19.25 19.830000  18.853218 15762300.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\RAIL3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\RAIZ4.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date  Open  High  Low  Close  Adj Close     Volume  Dividends  Stock Splits\n",
      "2019-02-26   NaN   NaN  NaN    NaN        NaN        NaN        NaN           NaN\n",
      "2021-06-08   NaN   NaN  NaN    NaN        NaN        NaN        NaN           NaN\n",
      "2022-11-21   4.1  4.16 3.95   4.03   3.616806 11314600.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\RAIZ4.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\RAPT4.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date  Open  High   Low  Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26   9.8  9.83  9.60   9.71   7.363028 1555100        0.0           0.0\n",
      "2021-06-08  15.2 15.44 14.81  15.05  12.328028 3418400        0.0           0.0\n",
      "2022-11-21   9.4  9.55  9.35   9.53   8.446183 1353800        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\RAPT4.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\RDOR3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low  Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26       NaN       NaN       NaN    NaN        NaN       NaN        NaN           NaN\n",
      "2021-06-08 68.510002 69.000000 67.410004  69.00  63.172188 1637000.0        0.0           0.0\n",
      "2022-11-21 28.510000 29.370001 28.420000  28.82  27.179855 3172500.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\RDOR3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\RECV3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26       NaN       NaN       NaN       NaN        NaN       NaN        NaN           NaN\n",
      "2021-06-08 16.213751 16.223536 15.763641 15.812566  11.878218  201124.0        0.0           0.0\n",
      "2022-11-21 31.770000 32.049999 29.959999 31.700001  23.976059 2623400.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\RECV3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\RENT3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 32.959072 33.470947 32.797924 33.271881  28.575903 3525730        0.0           0.0\n",
      "2021-06-08 66.267921 67.183609 65.690643 66.407265  58.158146 2937368        0.0           0.0\n",
      "2022-11-21 60.874935 62.428429 59.789486 61.362892  54.962959 7981301        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\RENT3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\SANB11.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 45.861736 46.140156 45.458515 45.679329  30.007027  574030        0.0           0.0\n",
      "2021-06-08 43.797623 43.980034 43.010380 43.778423  32.355476 3184511        0.0           0.0\n",
      "2022-11-21 28.129999 28.270000 27.610001 27.840000  23.134451  915700        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\SANB11.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\SAPR11.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26 26.233334 26.576666 25.996666 26.290001  17.924135  642000.0        0.0           0.0\n",
      "2021-06-08 20.510000 20.590000 20.280001 20.320000  15.078633 1459500.0        0.0           0.0\n",
      "2022-11-21 17.080000 19.650000 17.080000 19.420000  15.727230 5176600.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\SAPR11.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\SBSP3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 39.889999 40.259998 38.450001 40.169998  34.009460 3971100        0.0           0.0\n",
      "2021-06-08 38.700001 38.919998 37.959999 38.389999  34.763809 4191800        0.0           0.0\n",
      "2022-11-21 56.349998 59.430000 56.270000 59.250000  54.768566 7666500        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\SBSP3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\SLCE3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26  8.799586  8.799586  8.588842  8.626033   6.492414 1474264        0.0           0.0\n",
      "2021-06-08 20.847107 21.442148 20.702478 20.826447  17.189661 2640704        0.0           0.0\n",
      "2022-11-21 19.750000 20.177273 19.522726 19.836363  17.159296 2786080        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\SLCE3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\SMFT3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date  Open      High   Low  Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26   NaN       NaN   NaN    NaN        NaN       NaN        NaN           NaN\n",
      "2021-06-08   NaN       NaN   NaN    NaN        NaN       NaN        NaN           NaN\n",
      "2022-11-21 15.97 16.459999 15.65  16.26  15.362013 1081900.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\SMFT3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\SMTO3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 18.530001 18.799999 18.440001 18.650000  14.305385  676900        0.0           0.0\n",
      "2021-06-08 35.680000 36.500000 35.360001 35.799999  29.030426  936600        0.0           0.0\n",
      "2022-11-21 28.090000 28.389999 27.600000 28.000000  24.698017 2219300        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\SMTO3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\SRNA3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date  Open  High  Low  Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26   NaN   NaN  NaN    NaN        NaN       NaN        NaN           NaN\n",
      "2021-06-08   NaN   NaN  NaN    NaN        NaN       NaN        NaN           NaN\n",
      "2022-11-21   9.8  10.4  9.8  10.37      10.37 2682900.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\SRNA3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\SUZB3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close   Volume  Dividends  Stock Splits\n",
      "2019-02-26 48.599998 50.500000 48.570000 50.299999  44.391037  8764800        0.0           0.0\n",
      "2021-06-08 59.689999 59.959999 57.689999 58.080002  51.818714  8116400        0.0           0.0\n",
      "2022-11-21 57.830002 57.830002 55.349998 55.919998  51.069592 11439600        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\SUZB3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\TAEE11.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 25.049999 25.200001 24.950001 25.059999  13.086203 1305100        0.0           0.0\n",
      "2021-06-08 40.130001 40.490002 40.020000 40.020000  26.791838  892900        0.0           0.0\n",
      "2022-11-21 37.360001 37.820000 36.970001 37.099998  29.000177 1898600        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\TAEE11.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\TEND3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 16.791185 16.928257 16.399555 16.458300  15.195994  597909        0.0           0.0\n",
      "2021-06-08 25.299374 25.691006 25.162304 25.583305  24.817625 1102465        0.0           0.0\n",
      "2022-11-21  4.670201  4.807272  4.493967  4.582084   4.444948 3543742        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\TEND3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\TIMS3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date  Open  High   Low  Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 12.22 12.44 12.13  12.31   8.041409 1885800        0.0           0.0\n",
      "2021-06-08 12.60 12.60 12.37  12.41   8.655364 4729600        0.0           0.0\n",
      "2022-11-21 13.03 13.24 12.92  12.94   9.639189 6678200        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\TIMS3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\TOTS3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 11.416870 11.738799 11.416870 11.645871  10.768495 3415633        0.0           0.0\n",
      "2021-06-08 36.061398 36.091324 35.462868 35.752155  33.665665 4006519        0.0           0.0\n",
      "2022-11-21 30.230000 30.549999 29.700001 30.379999  28.913898 3981800        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\TOTS3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\UGPA3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 12.547857 12.658431 12.356656 12.444194  10.115456 5743984        0.0           0.0\n",
      "2021-06-08  9.721307  9.864132  9.693663  9.748950   8.419188 7183236        0.0           0.0\n",
      "2022-11-21 13.200000 13.440000 12.730000 13.220000  12.071607 6094400        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\UGPA3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\USIM5.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close   Volume  Dividends  Stock Splits\n",
      "2019-02-26 10.160000 10.300000 10.040000 10.060000   7.978939 10908800        0.0           0.0\n",
      "2021-06-08 18.879999 19.040001 18.309999 18.379999  14.936674 14247700        0.0           0.0\n",
      "2022-11-21  7.600000  7.650000  7.250000  7.400000   6.822979 16699300        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\USIM5.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\VALE3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date       Open       High        Low      Close  Adj Close   Volume  Dividends  Stock Splits\n",
      "2019-02-26  46.790001  47.290001  46.599998  47.200001  26.844185 10362900        0.0           0.0\n",
      "2021-06-08 112.480003 112.769997 109.699997 109.919998  69.865959 22676500        0.0           0.0\n",
      "2022-11-21  79.099998  80.190002  78.279999  79.919998  62.457920 27745900        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\VALE3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\VAMO3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26       NaN       NaN       NaN       NaN        NaN       NaN        NaN           NaN\n",
      "2021-06-08 13.064627 13.121355 12.845116 12.946239  11.503099 1135248.0        0.0           0.0\n",
      "2022-11-21 12.708266 13.104471 12.500259 13.015325  11.747440 4991964.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\VAMO3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\VBBR3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close     Volume  Dividends  Stock Splits\n",
      "2019-02-26 25.110001 25.219999 24.620001 24.650000  15.673520  6673900.0        0.0           0.0\n",
      "2021-06-08 27.530001 28.090000 27.490000 27.950001  22.287392  7163800.0        0.0           0.0\n",
      "2022-11-21 17.270000 17.520000 17.020000 17.270000  14.373416 11469400.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\VBBR3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\VIVA3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close    Volume  Dividends  Stock Splits\n",
      "2019-02-26       NaN       NaN       NaN       NaN        NaN       NaN        NaN           NaN\n",
      "2021-06-08 31.200001 31.440001 30.910000 31.360001  28.921280 1212600.0        0.0           0.0\n",
      "2022-11-21 22.620001 23.459999 22.559999 23.250000  21.729342 1863500.0        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\VIVA3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\VIVT3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 21.395000 21.990000 21.395000 21.740000  13.897585   33600        0.0           0.0\n",
      "2021-06-08 23.049999 23.170000 22.745001 22.745001  17.432705 3716200        0.0           0.0\n",
      "2022-11-21 19.910000 20.004999 19.385000 19.480000  16.261332 7443000        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\VIVT3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\WEGE3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open   High       Low     Close  Adj Close   Volume  Dividends  Stock Splits\n",
      "2019-02-26  9.245000  9.335  9.065000  9.165000   8.291451  6948000        0.0           0.0\n",
      "2021-06-08 34.000000 34.910 33.599998 34.020000  31.530003 10491800        0.0           0.0\n",
      "2022-11-21 39.389999 39.840 38.860001 39.490002  37.439907  4928900        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\WEGE3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: historical\\YDUQ3.SA.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 3,682\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 9\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.00 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "      date      Open      High       Low     Close  Adj Close  Volume  Dividends  Stock Splits\n",
      "2019-02-26 28.200001 28.309999 27.530001 28.049999  24.620615 2560400        0.0           0.0\n",
      "2021-06-08 34.480000 34.799999 33.759998 34.380001  31.816990 1899600        0.0           0.0\n",
      "2022-11-21 11.490000 11.720000 11.190000 11.540000  10.760610 4101000        0.0           0.0\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\YDUQ3.SA_sample.csv\n",
      "\n",
      "üìÅ DATASET: news\\news_with_sentiment.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 2,671\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 10\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.07 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "ticker_query                                                                                                          title                                                                                                                                                                                                                                                                                                                                                              link      published_date           source                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              summary sentiment_label  sentiment_score  numeric_sentiment  sentiment_weighted\n",
      "       RAIZ4                  Ra√≠zen (RAIZ4): Press√£o nos bonds revela resili√™ncia no mercado de cr√©dito - Acionista.com.br                                                                                              https://news.google.com/rss/articles/CBMingFBVV95cUxObDRoWmFidmJ0S3Y2b0lBY05mNWd2M3FJLVBvUE5ZZzNWaFE1UjRMQS1XRTRub3VwRG8wczhka1hEOHR1ampKTmJIcTFwTFNFa0JUaVhSTEpKb0s2bFptN1gtcHVMS29vcXpTUWoxVU1qTEt2QWcxS2RoOU5lOVpKTnlNR0piaTdZd0xHZkFsZFozbWk1RThfUlBVdlJoUQ?oc=5 2025-10-13 20:04:53 Acionista.com.br                                                                                                               <a href=\"https://news.google.com/rss/articles/CBMingFBVV95cUxObDRoWmFidmJ0S3Y2b0lBY05mNWd2M3FJLVBvUE5ZZzNWaFE1UjRMQS1XRTRub3VwRG8wczhka1hEOHR1ampKTmJIcTFwTFNFa0JUaVhSTEpKb0s2bFptN1gtcHVMS29vcXpTUWoxVU1qTEt2QWcxS2RoOU5lOVpKTnlNR0piaTdZd0xHZkFsZFozbWk1RThfUlBVdlJoUQ?oc=5\" target=\"_blank\">Ra√≠zen (RAIZ4): Press√£o nos bonds revela resili√™ncia no mercado de cr√©dito</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Acionista.com.br</font>         neutral         0.715279                  0            0.000000\n",
      "       GGBR4 Bons ventos para Marcopolo (POMO4) e Gerdau (GGBR4): Empiricus v√™ curto prazo favor√°vel para a√ß√µes - Empiricus https://news.google.com/rss/articles/CBMi5AFBVV95cUxQdnllVTRCOXdIcVNpdGo3NTZNSFhmSXA5dDBMbVZ0Q0FTaV9YYU9mcHJSZEdaWHpWaGZQS05MWkRDOXM0ZVRmcldIQ05TUEhobURWSE9Bakw5REU1ZjRJRVNNczZmOUVrdEFnZV9mUTdzek52LUoxLWJjN2ZQalJqWWd6bFJ5MHFNVHc0ZzJpclNlVXJfdGNLOC1Lb1dhbFhIbWFLNHJZY3BHcXlqTlBkeFRtdDFuVnJ0NFh5SVRCQTJwbTdnMDBPOGtwRlhrWkQ3QXE3QjRtWEJOV1NnbFR4Z0lxbmo?oc=5 2025-10-02 07:00:00        Empiricus <a href=\"https://news.google.com/rss/articles/CBMi5AFBVV95cUxQdnllVTRCOXdIcVNpdGo3NTZNSFhmSXA5dDBMbVZ0Q0FTaV9YYU9mcHJSZEdaWHpWaGZQS05MWkRDOXM0ZVRmcldIQ05TUEhobURWSE9Bakw5REU1ZjRJRVNNczZmOUVrdEFnZV9mUTdzek52LUoxLWJjN2ZQalJqWWd6bFJ5MHFNVHc0ZzJpclNlVXJfdGNLOC1Lb1dhbFhIbWFLNHJZY3BHcXlqTlBkeFRtdDFuVnJ0NFh5SVRCQTJwbTdnMDBPOGtwRlhrWkQ3QXE3QjRtWEJOV1NnbFR4Z0lxbmo?oc=5\" target=\"_blank\">Bons ventos para Marcopolo (POMO4) e Gerdau (GGBR4): Empiricus v√™ curto prazo favor√°vel para a√ß√µes</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Empiricus</font>        positive         0.767323                  1            0.767323\n",
      "       LREN3                                                      Varejo na Berlinda: LREN3 e o cen√°rio de inflex√£o - ADVFN                                                                                                                      https://news.google.com/rss/articles/CBMijAFBVV95cUxPQlNqQ1B3eUNEX3liMjFDSU5nSnFaVzhtX2hjVXJkS2hvNlJMRVBEMF9fcDRNbjdSc0x1c1VITFpqdDJON0JfMzY5c0FjX0Y2cjBRWVBJYVZVNHVYQkF4ZnU5WWNZTkdiT0laVU5YcGlXbkliOEVQbGNEZnBxUXhJZjlvT3ZkbUpreTAwTg?oc=5 2025-10-10 14:54:51            ADVFN                                                                                                                                                                           <a href=\"https://news.google.com/rss/articles/CBMijAFBVV95cUxPQlNqQ1B3eUNEX3liMjFDSU5nSnFaVzhtX2hjVXJkS2hvNlJMRVBEMF9fcDRNbjdSc0x1c1VITFpqdDJON0JfMzY5c0FjX0Y2cjBRWVBJYVZVNHVYQkF4ZnU5WWNZTkdiT0laVU5YcGlXbkliOEVQbGNEZnBxUXhJZjlvT3ZkbUpreTAwTg?oc=5\" target=\"_blank\">Varejo na Berlinda: LREN3 e o cen√°rio de inflex√£o</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">ADVFN</font>         neutral         0.911613                  0            0.000000\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\news_with_sentiment_sample.csv\n",
      "\n",
      "üìÅ DATASET: news\\raw_news_data.parquet\n",
      "----------------------------------------\n",
      "üìà Estat√≠sticas:\n",
      "   ‚Ä¢ Linhas totais: 2,671\n",
      "   ‚Ä¢ Amostra: 50 linhas\n",
      "   ‚Ä¢ Colunas: 6\n",
      "   ‚Ä¢ Uso de mem√≥ria: 0.07 MB\n",
      "\n",
      "üëÄ Primeiras 3 linhas da amostra:\n",
      "ticker_query                                                                                                          title                                                                                                                                                                                                                                                                                                                                                              link      published_date           source                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              summary\n",
      "       RAIZ4                  Ra√≠zen (RAIZ4): Press√£o nos bonds revela resili√™ncia no mercado de cr√©dito - Acionista.com.br                                                                                              https://news.google.com/rss/articles/CBMingFBVV95cUxObDRoWmFidmJ0S3Y2b0lBY05mNWd2M3FJLVBvUE5ZZzNWaFE1UjRMQS1XRTRub3VwRG8wczhka1hEOHR1ampKTmJIcTFwTFNFa0JUaVhSTEpKb0s2bFptN1gtcHVMS29vcXpTUWoxVU1qTEt2QWcxS2RoOU5lOVpKTnlNR0piaTdZd0xHZkFsZFozbWk1RThfUlBVdlJoUQ?oc=5 2025-10-13 20:04:53 Acionista.com.br                                                                                                               <a href=\"https://news.google.com/rss/articles/CBMingFBVV95cUxObDRoWmFidmJ0S3Y2b0lBY05mNWd2M3FJLVBvUE5ZZzNWaFE1UjRMQS1XRTRub3VwRG8wczhka1hEOHR1ampKTmJIcTFwTFNFa0JUaVhSTEpKb0s2bFptN1gtcHVMS29vcXpTUWoxVU1qTEt2QWcxS2RoOU5lOVpKTnlNR0piaTdZd0xHZkFsZFozbWk1RThfUlBVdlJoUQ?oc=5\" target=\"_blank\">Ra√≠zen (RAIZ4): Press√£o nos bonds revela resili√™ncia no mercado de cr√©dito</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Acionista.com.br</font>\n",
      "       GGBR4 Bons ventos para Marcopolo (POMO4) e Gerdau (GGBR4): Empiricus v√™ curto prazo favor√°vel para a√ß√µes - Empiricus https://news.google.com/rss/articles/CBMi5AFBVV95cUxQdnllVTRCOXdIcVNpdGo3NTZNSFhmSXA5dDBMbVZ0Q0FTaV9YYU9mcHJSZEdaWHpWaGZQS05MWkRDOXM0ZVRmcldIQ05TUEhobURWSE9Bakw5REU1ZjRJRVNNczZmOUVrdEFnZV9mUTdzek52LUoxLWJjN2ZQalJqWWd6bFJ5MHFNVHc0ZzJpclNlVXJfdGNLOC1Lb1dhbFhIbWFLNHJZY3BHcXlqTlBkeFRtdDFuVnJ0NFh5SVRCQTJwbTdnMDBPOGtwRlhrWkQ3QXE3QjRtWEJOV1NnbFR4Z0lxbmo?oc=5 2025-10-02 07:00:00        Empiricus <a href=\"https://news.google.com/rss/articles/CBMi5AFBVV95cUxQdnllVTRCOXdIcVNpdGo3NTZNSFhmSXA5dDBMbVZ0Q0FTaV9YYU9mcHJSZEdaWHpWaGZQS05MWkRDOXM0ZVRmcldIQ05TUEhobURWSE9Bakw5REU1ZjRJRVNNczZmOUVrdEFnZV9mUTdzek52LUoxLWJjN2ZQalJqWWd6bFJ5MHFNVHc0ZzJpclNlVXJfdGNLOC1Lb1dhbFhIbWFLNHJZY3BHcXlqTlBkeFRtdDFuVnJ0NFh5SVRCQTJwbTdnMDBPOGtwRlhrWkQ3QXE3QjRtWEJOV1NnbFR4Z0lxbmo?oc=5\" target=\"_blank\">Bons ventos para Marcopolo (POMO4) e Gerdau (GGBR4): Empiricus v√™ curto prazo favor√°vel para a√ß√µes</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Empiricus</font>\n",
      "       LREN3                                                      Varejo na Berlinda: LREN3 e o cen√°rio de inflex√£o - ADVFN                                                                                                                      https://news.google.com/rss/articles/CBMijAFBVV95cUxPQlNqQ1B3eUNEX3liMjFDSU5nSnFaVzhtX2hjVXJkS2hvNlJMRVBEMF9fcDRNbjdSc0x1c1VITFpqdDJON0JfMzY5c0FjX0Y2cjBRWVBJYVZVNHVYQkF4ZnU5WWNZTkdiT0laVU5YcGlXbkliOEVQbGNEZnBxUXhJZjlvT3ZkbUpreTAwTg?oc=5 2025-10-10 14:54:51            ADVFN                                                                                                                                                                           <a href=\"https://news.google.com/rss/articles/CBMijAFBVV95cUxPQlNqQ1B3eUNEX3liMjFDSU5nSnFaVzhtX2hjVXJkS2hvNlJMRVBEMF9fcDRNbjdSc0x1c1VITFpqdDJON0JfMzY5c0FjX0Y2cjBRWVBJYVZVNHVYQkF4ZnU5WWNZTkdiT0laVU5YcGlXbkliOEVQbGNEZnBxUXhJZjlvT3ZkbUpreTAwTg?oc=5\" target=\"_blank\">Varejo na Berlinda: LREN3 e o cen√°rio de inflex√£o</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">ADVFN</font>\n",
      "\n",
      "üíæ Amostra salva em: data\\samples\\raw_news_data_sample.csv\n",
      "\n",
      "============================================================\n",
      "‚úÖ AMOSTRAGEM CONCLU√çDA!\n",
      "============================================================\n",
      "üìä Resumo Final:\n",
      "   ‚Ä¢ Datasets processados: 111\n",
      "   ‚Ä¢ Amostras geradas com sucesso: 111\n",
      "   ‚Ä¢ Local das amostras: data/samples/\n",
      "\n",
      "üéØ COMANDOS R√ÅPIDOS:\n",
      "   ‚Ä¢ list_datasets() - Lista todos os datasets\n",
      "   ‚Ä¢ quick_sample('quality') - Amostra de datasets de qualidade\n",
      "   ‚Ä¢ quick_sample('news') - Amostra de datasets de not√≠cias\n",
      "   ‚Ä¢ quick_sample('fundamentals') - Amostra de dados fundamentalistas\n",
      "\n",
      "============================================================\n",
      "üöÄ EXEMPLOS DE USO R√ÅPIDO - EXECUTE AP√ìS O SCRIPT:\n",
      "============================================================\n",
      "\n",
      "# Listar todos os datasets\n",
      "list_datasets()\n",
      "\n",
      "# Amostra r√°pida de datasets de qualidade\n",
      "quick_sample('quality', 10)\n",
      "\n",
      "# Amostra de dados fundamentalistas  \n",
      "quick_sample('fundamental', 15)\n",
      "\n",
      "# Amostra de not√≠cias\n",
      "quick_sample('news', 8)\n",
      "\n",
      "# Visualiza√ß√£o r√°pida de tudo\n",
      "sampler = AurumDataSampler()\n",
      "sampler.quick_preview()\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List, Optional\n",
    "import json\n",
    "\n",
    "# Configura√ß√£o de logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class AurumDataSampler:\n",
    "    \"\"\"\n",
    "    Classe para amostragem e an√°lise explorat√≥ria dos dados do Aurum\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str = \"data\"):\n",
    "        self.base_path = Path(base_path)\n",
    "        self.sample_size = 100  # Tamanho padr√£o da amostra\n",
    "        \n",
    "    def list_available_datasets(self) -> Dict[str, Path]:\n",
    "        \"\"\"\n",
    "        Lista todos os datasets Parquet dispon√≠veis\n",
    "        \"\"\"\n",
    "        datasets = {}\n",
    "        \n",
    "        # Procurar recursivamente por arquivos .parquet\n",
    "        for parquet_file in self.base_path.rglob(\"*.parquet\"):\n",
    "            relative_path = parquet_file.relative_to(self.base_path)\n",
    "            datasets[str(relative_path)] = parquet_file\n",
    "            \n",
    "        return datasets\n",
    "    \n",
    "    def load_dataset_sample(self, dataset_path: Path, sample_size: int = None, \n",
    "                          random_state: int = 42) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Carrega uma amostra representativa do dataset\n",
    "        \"\"\"\n",
    "        if sample_size is None:\n",
    "            sample_size = self.sample_size\n",
    "            \n",
    "        logger.info(f\"üìä Carregando amostra de {dataset_path}...\")\n",
    "        \n",
    "        try:\n",
    "            # Ler o dataset completo para obter metadados\n",
    "            full_df = pd.read_parquet(dataset_path)\n",
    "            total_rows = len(full_df)\n",
    "            \n",
    "            logger.info(f\"   üìà Dataset completo: {total_rows} linhas, {len(full_df.columns)} colunas\")\n",
    "            \n",
    "            # Estrat√©gias de amostragem baseadas no tamanho do dataset\n",
    "            if total_rows <= sample_size:\n",
    "                # Dataset pequeno - retornar tudo\n",
    "                sample_df = full_df.copy()\n",
    "                logger.info(\"   üîç Dataset pequeno - retornando dados completos\")\n",
    "            else:\n",
    "                # Dataset grande - amostragem estratificada quando poss√≠vel\n",
    "                sample_df = self._stratified_sampling(full_df, sample_size, random_state)\n",
    "                \n",
    "            return sample_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erro ao carregar {dataset_path}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _stratified_sampling(self, df: pd.DataFrame, sample_size: int, random_state: int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Amostragem estratificada tentando manter propor√ß√µes de categorias importantes\n",
    "        \"\"\"\n",
    "        sample_df = df.copy()\n",
    "        \n",
    "        # Tentar estratificar por ano se a coluna existir\n",
    "        year_columns = [col for col in sample_df.columns if 'ANO' in col or 'YEAR' in col or 'DATA' in col]\n",
    "        \n",
    "        if year_columns:\n",
    "            try:\n",
    "                year_col = year_columns[0]\n",
    "                # Amostragem estratificada por ano\n",
    "                stratified_sample = sample_df.groupby(year_col, group_keys=False).apply(\n",
    "                    lambda x: x.sample(n=min(len(x), max(1, sample_size // sample_df[year_col].nunique())), \n",
    "                                      random_state=random_state)\n",
    "                )\n",
    "                \n",
    "                # Se a amostra estratificada for muito pequena, completar com amostra aleat√≥ria\n",
    "                if len(stratified_sample) < sample_size:\n",
    "                    remaining = sample_size - len(stratified_sample)\n",
    "                    additional_sample = sample_df.drop(stratified_sample.index).sample(\n",
    "                        n=remaining, random_state=random_state)\n",
    "                    stratified_sample = pd.concat([stratified_sample, additional_sample])\n",
    "                \n",
    "                logger.info(f\"   üéØ Amostragem estratificada por {year_col}: {len(stratified_sample)} linhas\")\n",
    "                return stratified_sample\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"   ‚ö†Ô∏è Amostragem estratificada falhou: {e}\")\n",
    "        \n",
    "        # Fallback: amostra aleat√≥ria simples\n",
    "        simple_sample = sample_df.sample(n=min(sample_size, len(sample_df)), random_state=random_state)\n",
    "        logger.info(f\"   üé≤ Amostra aleat√≥ria simples: {len(simple_sample)} linhas\")\n",
    "        return simple_sample\n",
    "    \n",
    "    def generate_basic_report(self, dataset_path: Path, sample_size: int = 50) -> Dict:\n",
    "        \"\"\"\n",
    "        Gera relat√≥rio b√°sico do dataset (sem dados complexos para JSON)\n",
    "        \"\"\"\n",
    "        # Carregar amostra\n",
    "        sample_df = self.load_dataset_sample(dataset_path, sample_size)\n",
    "        \n",
    "        if sample_df.empty:\n",
    "            return {\n",
    "                'dataset_name': dataset_path.name,\n",
    "                'dataset_path': str(dataset_path),\n",
    "                'error': 'Falha ao carregar dataset'\n",
    "            }\n",
    "        \n",
    "        # Estat√≠sticas b√°sicas\n",
    "        basic_stats = {\n",
    "            'total_rows_original': len(pd.read_parquet(dataset_path)),\n",
    "            'sample_size': len(sample_df),\n",
    "            'columns_count': len(sample_df.columns),\n",
    "            'memory_usage_mb': round(sample_df.memory_usage(deep=True).sum() / 1024**2, 2),\n",
    "            'columns_list': list(sample_df.columns),\n",
    "            'dtypes': {col: str(dtype) for col, dtype in sample_df.dtypes.items()}\n",
    "        }\n",
    "        \n",
    "        # Informa√ß√µes de data\n",
    "        date_info = self._get_date_info(sample_df)\n",
    "        \n",
    "        # Qualidade b√°sica dos dados\n",
    "        quality_info = self._get_basic_quality_info(sample_df)\n",
    "        \n",
    "        report = {\n",
    "            'dataset_name': dataset_path.name,\n",
    "            'dataset_path': str(dataset_path),\n",
    "            'basic_stats': basic_stats,\n",
    "            'date_info': date_info,\n",
    "            'quality_info': quality_info\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def _get_date_info(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Extrai informa√ß√µes de data de forma segura\"\"\"\n",
    "        date_info = {}\n",
    "        date_columns = df.select_dtypes(include=['datetime64']).columns\n",
    "        \n",
    "        for col in date_columns:\n",
    "            try:\n",
    "                if col in df.columns and not df[col].isna().all():\n",
    "                    date_info[col] = {\n",
    "                        'min': str(df[col].min()),\n",
    "                        'max': str(df[col].max()),\n",
    "                        'null_count': int(df[col].isna().sum()),\n",
    "                        'null_percentage': round((df[col].isna().sum() / len(df)) * 100, 2)\n",
    "                    }\n",
    "            except Exception as e:\n",
    "                date_info[col] = {'error': f'Erro ao processar: {str(e)}'}\n",
    "        \n",
    "        return date_info\n",
    "    \n",
    "    def _get_basic_quality_info(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Analisa qualidade b√°sica dos dados\"\"\"\n",
    "        quality_info = {\n",
    "            'total_rows': len(df),\n",
    "            'complete_cases': len(df.dropna()),\n",
    "            'duplicate_rows': int(df.duplicated().sum()),\n",
    "            'columns_quality': {}\n",
    "        }\n",
    "        \n",
    "        for column in df.columns:\n",
    "            try:\n",
    "                col_data = df[column]\n",
    "                col_info = {\n",
    "                    'dtype': str(col_data.dtype),\n",
    "                    'null_count': int(col_data.isna().sum()),\n",
    "                    'null_percentage': round((col_data.isna().sum() / len(df)) * 100, 2),\n",
    "                    'unique_count': int(col_data.nunique())\n",
    "                }\n",
    "                \n",
    "                # Estat√≠sticas para colunas num√©ricas\n",
    "                if pd.api.types.is_numeric_dtype(col_data):\n",
    "                    col_info.update({\n",
    "                        'mean': float(col_data.mean()) if not col_data.isna().all() else None,\n",
    "                        'std': float(col_data.std()) if not col_data.isna().all() else None,\n",
    "                        'min': float(col_data.min()) if not col_data.isna().all() else None,\n",
    "                        'max': float(col_data.max()) if not col_data.isna().all() else None\n",
    "                    })\n",
    "                \n",
    "                quality_info['columns_quality'][column] = col_info\n",
    "                \n",
    "            except Exception as e:\n",
    "                quality_info['columns_quality'][column] = {'error': f'Erro na an√°lise: {str(e)}'}\n",
    "        \n",
    "        return quality_info\n",
    "    \n",
    "    def create_simple_sampling_report(self, output_dir: str = \"data/samples\") -> Dict:\n",
    "        \"\"\"\n",
    "        Cria relat√≥rio simplificado de amostragem para todos os datasets\n",
    "        \"\"\"\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        datasets = self.list_available_datasets()\n",
    "        reports = {}\n",
    "        \n",
    "        logger.info(\"üîç INICIANDO AMOSTRAGEM DOS DATASETS AURUM\")\n",
    "        logger.info(f\"üìÅ Encontrados {len(datasets)} datasets:\")\n",
    "        \n",
    "        for name, path in datasets.items():\n",
    "            logger.info(f\"   üìä {name}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üéØ RELAT√ìRIO DE AMOSTRAGEM - AURUM DATA SAMPLES\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for dataset_name, dataset_path in datasets.items():\n",
    "            print(f\"\\nüìÅ DATASET: {dataset_name}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Gerar relat√≥rio b√°sico\n",
    "            report = self.generate_basic_report(dataset_path)\n",
    "            reports[dataset_name] = report\n",
    "            \n",
    "            # Carregar amostra para salvar como CSV\n",
    "            sample_df = self.load_dataset_sample(dataset_path, 50)\n",
    "            \n",
    "            if not sample_df.empty:\n",
    "                # Salvar amostra como CSV\n",
    "                sample_csv_path = output_path / f\"{dataset_path.stem}_sample.csv\"\n",
    "                sample_df.to_csv(sample_csv_path, index=False, encoding='utf-8')\n",
    "                \n",
    "                # Mostrar resumo no console\n",
    "                basic_stats = report['basic_stats']\n",
    "                print(f\"üìà Estat√≠sticas:\")\n",
    "                print(f\"   ‚Ä¢ Linhas totais: {basic_stats['total_rows_original']:,}\")\n",
    "                print(f\"   ‚Ä¢ Amostra: {basic_stats['sample_size']} linhas\")\n",
    "                print(f\"   ‚Ä¢ Colunas: {basic_stats['columns_count']}\")\n",
    "                print(f\"   ‚Ä¢ Uso de mem√≥ria: {basic_stats['memory_usage_mb']:.2f} MB\")\n",
    "                \n",
    "                # Mostrar primeiras linhas\n",
    "                print(f\"\\nüëÄ Primeiras 3 linhas da amostra:\")\n",
    "                print(sample_df.head(3).to_string(index=False))\n",
    "                \n",
    "                print(f\"\\nüíæ Amostra salva em: {sample_csv_path}\")\n",
    "            else:\n",
    "                print(\"‚ùå N√£o foi poss√≠vel carregar amostra deste dataset\")\n",
    "        \n",
    "        # Salvar relat√≥rio consolidado (agora seguro para JSON)\n",
    "        report_path = output_path / \"aurum_sampling_report.json\"\n",
    "        \n",
    "        try:\n",
    "            with open(report_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(reports, f, indent=2, ensure_ascii=False, default=str)\n",
    "            logger.info(f\"üìã Relat√≥rio consolidado salvo em: {report_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Erro ao salvar relat√≥rio JSON: {e}\")\n",
    "        \n",
    "        return reports\n",
    "\n",
    "    def quick_preview(self, dataset_pattern: str = None, sample_size: int = 10):\n",
    "        \"\"\"\n",
    "        Visualiza√ß√£o r√°pida dos datasets\n",
    "        \"\"\"\n",
    "        datasets = self.list_available_datasets()\n",
    "        \n",
    "        print(\"üöÄ VISUALIZA√á√ÉO R√ÅPIDA DOS DATASETS AURUM\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for name, path in datasets.items():\n",
    "            if dataset_pattern and dataset_pattern.lower() not in name.lower():\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\nüìä {name}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            sample_df = self.load_dataset_sample(path, sample_size)\n",
    "            if not sample_df.empty:\n",
    "                print(f\"üìã Shape: {sample_df.shape}\")\n",
    "                print(f\"üéØ Amostra de {len(sample_df)} linhas:\")\n",
    "                print(sample_df.to_string(index=False))\n",
    "            else:\n",
    "                print(\"‚ùå N√£o foi poss√≠vel carregar amostra\")\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 50)\n",
    "\n",
    "# Fun√ß√µes de uso r√°pido\n",
    "def quick_sample(dataset_pattern: str, sample_size: int = 15) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fun√ß√£o r√°pida para obter amostra de datasets que correspondam ao padr√£o\n",
    "    \"\"\"\n",
    "    sampler = AurumDataSampler()\n",
    "    datasets = sampler.list_available_datasets()\n",
    "    \n",
    "    matching_datasets = []\n",
    "    for name, path in datasets.items():\n",
    "        if dataset_pattern.lower() in name.lower():\n",
    "            matching_datasets.append((name, path))\n",
    "    \n",
    "    if not matching_datasets:\n",
    "        available = \"\\n\".join(datasets.keys())\n",
    "        print(f\"‚ùå Nenhum dataset encontrado com '{dataset_pattern}'.\")\n",
    "        print(f\"üìÅ Datasets dispon√≠veis:\\n{available}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if len(matching_datasets) == 1:\n",
    "        name, path = matching_datasets[0]\n",
    "        sample = sampler.load_dataset_sample(path, sample_size)\n",
    "        print(f\"üéØ Amostra de {name} ({len(sample)} linhas):\")\n",
    "        print(sample.to_string(index=False))\n",
    "        return sample\n",
    "    else:\n",
    "        print(f\"üîç M√∫ltiplos datasets encontrados com '{dataset_pattern}':\")\n",
    "        for i, (name, path) in enumerate(matching_datasets, 1):\n",
    "            print(f\"   {i}. {name}\")\n",
    "        \n",
    "        choice = input(\"üëâ Escolha o n√∫mero do dataset (ou Enter para o primeiro): \").strip()\n",
    "        try:\n",
    "            choice_idx = int(choice) - 1 if choice else 0\n",
    "            name, path = matching_datasets[choice_idx]\n",
    "            sample = sampler.load_dataset_sample(path, sample_size)\n",
    "            print(f\"üéØ Amostra de {name} ({len(sample)} linhas):\")\n",
    "            print(sample.to_string(index=False))\n",
    "            return sample\n",
    "        except (ValueError, IndexError):\n",
    "            print(\"‚ùå Escolha inv√°lida.\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "def list_datasets():\n",
    "    \"\"\"Lista todos os datasets dispon√≠veis\"\"\"\n",
    "    sampler = AurumDataSampler()\n",
    "    datasets = sampler.list_available_datasets()\n",
    "    \n",
    "    print(\"üìÅ DATASETS DISPON√çVEIS NO AURUM:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, (name, path) in enumerate(datasets.items(), 1):\n",
    "        try:\n",
    "            # Tentar obter informa√ß√µes b√°sicas\n",
    "            df_sample = pd.read_parquet(path, nrows=1)\n",
    "            print(f\"{i:2d}. {name}\")\n",
    "            print(f\"    üìä Colunas: {len(df_sample.columns)}, Estilo: {df_sample.shape[1]}xN\")\n",
    "            print(f\"    üìç {path}\")\n",
    "        except:\n",
    "            print(f\"{i:2d}. {name} (‚ùå erro ao carregar)\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "# Execu√ß√£o principal\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Executa amostragem completa de todos os datasets\n",
    "    \"\"\"\n",
    "    sampler = AurumDataSampler()\n",
    "    reports = sampler.create_simple_sampling_report()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ AMOSTRAGEM CONCLU√çDA!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Resumo final\n",
    "    total_datasets = len(reports)\n",
    "    successful_samples = sum(1 for report in reports.values() if 'error' not in report)\n",
    "    \n",
    "    print(f\"üìä Resumo Final:\")\n",
    "    print(f\"   ‚Ä¢ Datasets processados: {total_datasets}\")\n",
    "    print(f\"   ‚Ä¢ Amostras geradas com sucesso: {successful_samples}\")\n",
    "    print(f\"   ‚Ä¢ Local das amostras: data/samples/\")\n",
    "    \n",
    "    print(f\"\\nüéØ COMANDOS R√ÅPIDOS:\")\n",
    "    print(f\"   ‚Ä¢ list_datasets() - Lista todos os datasets\")\n",
    "    print(f\"   ‚Ä¢ quick_sample('quality') - Amostra de datasets de qualidade\")\n",
    "    print(f\"   ‚Ä¢ quick_sample('news') - Amostra de datasets de not√≠cias\")\n",
    "    print(f\"   ‚Ä¢ quick_sample('fundamentals') - Amostra de dados fundamentalistas\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# Exemplos de uso ap√≥s execu√ß√£o\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ EXEMPLOS DE USO R√ÅPIDO - EXECUTE AP√ìS O SCRIPT:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "# Listar todos os datasets\n",
    "list_datasets()\n",
    "\n",
    "# Amostra r√°pida de datasets de qualidade\n",
    "quick_sample('quality', 10)\n",
    "\n",
    "# Amostra de dados fundamentalistas  \n",
    "quick_sample('fundamental', 15)\n",
    "\n",
    "# Amostra de not√≠cias\n",
    "quick_sample('news', 8)\n",
    "\n",
    "# Visualiza√ß√£o r√°pida de tudo\n",
    "sampler = AurumDataSampler()\n",
    "sampler.quick_preview()\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c611dee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6208335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b104e84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_aurum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
